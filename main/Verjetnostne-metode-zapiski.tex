\documentclass[a4paper, 12pt]{book}

\usepackage{fancyhdr}

\newcommand{\ttitle}{Verjetnostne metode v računalništvu - zapiski s predavanj prof. Marca}
\newcommand{\ttitleshort}{Verjetnostne metode v računalništvu}
\newcommand{\tauthor}{Tomaž Poljanšek}
\newcommand{\tdate}{študijsko leto 2023/24}

\usepackage{color}
\usepackage{soul}
\usepackage[numbers]{natbib}

\usepackage{physics}

\usepackage[parfill]{parskip}
\usepackage[hyphens]{url}

\usepackage[usestackEOL]{stackengine}[2013-10-15] % formatting Pascal
\usepackage[dvipsnames]{xcolor}

\usepackage{cancel}
\usepackage[export]{adjustbox}

% Related to math
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{mathtools}
\usepackage{youngtab}
\usepackage{tikz}

% encoding and language
\usepackage{lmodern}
\usepackage[slovene, english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% multiline comments
\usepackage{comment}
\usepackage{verbatim}

% random text - for texting
\usepackage{lipsum}
\usepackage{blindtext}

\usepackage{hyperref}

% images
\usepackage{graphicx}
\graphicspath{ {../images/} }

% no blank page
\usepackage{atbegshi}
\renewcommand{\cleardoublepage}{\clearpage}
%\renewcommand{\clearpage}{}

\usepackage{listings}
\usepackage{verbatim}
%\usepackage{fancyvrb}
%\usepackage{bera}

\newcommand*\Eval[3]{\left.#1\right\rvert_{#2}^{#3}}

\lstset{basicstyle=\ttfamily,
escapeinside={||},
mathescape=true}

% theorems
\theoremstyle{definition}
\newtheorem{counter}{Counter}[section]
\newtheorem{defn}[counter]{Definicija}
\newtheorem{lemma}[counter]{Lema}
\newtheorem{conseq}[counter]{Posledica}
\newtheorem{claim}[counter]{Trditev}
\newtheorem{theorem}[counter]{Izrek}
\newtheorem{pro}[counter]{Dokaz}
%%
\theoremstyle{remark}
\newtheorem*{ex}{Primer}
\newtheorem*{exmp}{Zgled}
\newtheorem*{rem}{Opomba}

% QED
\renewcommand\qedsymbol{$\blacksquare$}

\hypersetup{pdftitle={\ttitle}}

\addtolength{\marginparwidth}{-20pt}
\addtolength{\oddsidemargin}{40pt}
\addtolength{\evensidemargin}{-40pt}

\renewcommand{\baselinestretch}{1.3}
\setlength{\headheight}{15pt}
\renewcommand{\chaptermark}[1]
{\markboth{\MakeUppercase{\thechapter.\ #1}}{}} \renewcommand{\sectionmark}[1]
{\markright{\MakeUppercase{\thesection.\ #1}}} \renewcommand{\headrulewidth}{0.5pt} \renewcommand{\footrulewidth}{0pt}

% header
\fancyhf{}
\fancyhead[LE,RO]{\sl \thepage} 
\fancyhead[RE]{\sc \tauthor}
\fancyhead[LO]{\sc \ttitleshort}


\newcommand{\autfont}{\Large}
\newcommand{\titfont}{\LARGE\bf}
\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{empty}\cleardoublepage}}
\setcounter{tocdepth}{1}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\ch}{\operatorname{char}}

\makeatletter
\newcommand{\longeq}[1]{\mathrel{\mathpalette\longeq@{#1}}}
\newcommand{\longeq@}[2]{%
  \begingroup
  \sbox\z@{$\m@th#1=$}%
  \ifdim#2<\wd\z@
    \resizebox{#2}{\height}{\box\z@}%
  \else
    \ifdim#2<3\wd\z@
      \hbox to #2{$\m@th#1=\hss=\hss=\hss=$}%
    \else
      \hbox to #2{$\m@th#1=\cleaders\hbox to 0.2\wd\z@{\hss$#1=$\hss}\hfil=$}%
    \fi
  \fi
  \endgroup
}
\makeatother


\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{float}
\usepackage{multirow}
\usepackage{icomma}
\usepackage{tabularx}
\usepackage{hhline}

\usepackage{enumitem}
\usepackage{ulem}
\newcommand{\msout}[1]{\text{\sout{\ensuremath{#1}}}} % cross text in math mode

\usepackage{alltt}

\title{\ttitle}
\author{\tauthor}
\date{\tdate}

\newcommand\mymaketitle{
  \begin{titlepage}
    \begin{center}
        \vspace*{4cm}
        \Huge
        \textbf{\ttitle}
                        
        \vspace{1.5cm}
        \huge
        \tauthor
            
        \vspace{3cm}
        \Large
        \tdate
    \end{center}
  \end{titlepage}
}


% inductive hypothesis - IH

\begin{document}

\selectlanguage{slovene}
%\setcounter{page}{1}
\renewcommand{\thepage}{}
\newcommand{\sn}[1]{"`#1"'}

\mymaketitle

\clearpage
%\AtBeginShipoutNext{\AtBeginShipoutDiscard}

\frontmatter

% kazalo
\pagestyle{empty}
\def\thepage{}
\tableofcontents{}

%%
\def\x{\hspace{3ex}}    %BETWEEN TWO 1-DIGIT NUMBERS
\def\y{\hspace{2.45ex}}  %BETWEEN 1 AND 2 DIGIT NUMBERS
\def\z{\hspace{1.9ex}}    %BETWEEN TWO 2-DIGIT NUMBERS
\stackMath

%\clearpage
%\phantomsection
%\addcontentsline{toc}{chapter}{Povzetek}
%\chapter*{Povzetek}

%Predloga.

%\newpage

\pagenumbering{arabic}

\mainmatter
\setcounter{page}{1}
\pagestyle{fancy}


% 1. predavanje: 6.10.


\chapter{Introduction}


\section{Probability}

$(\Omega, F, P_r)$:
\begin{itemize}[label=$\circ$]
  \item $\emptyset \in F$,
  \item $A \in F \implies A^c \in F$,
  \item $A_1, A_2 \dots \in F \implies \cup_{i=1}^{\infty} A_i \in F$.
\end{itemize}
$P_r(A) \geq 0$, \\
$P_r\left(\cup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P_r(A_i)$ if $A_i$ disjoint, \\
$P_r\left(\cup_{i=1}^{\infty} A_i\right) \leq \sum_{i=1}^{\infty} P_r(A_i)$, \\
$\Omega = \{\omega_1, \omega_2 \dots\}$ - countable case. \\
$\begin{pmatrix}
  \omega_1 & \omega_2 & \dots \\
  p_1 & p_2 & \dots
\end{pmatrix}$
\begin{ex} \text{}
  \begin{lstlisting}
    Alg():
      while True:
        B = sample as random from $\{0,1\}$  # 1 with probability p
        if B = 1:
          return
  \end{lstlisting}
  $\Omega = \{1, 01, 001, 0001 \dots\}$ \\
  $\begin{pmatrix}
    1 & 01 & 001 & 0001 & \dots \\
    p & (1-p)p & (1-p)^2 p & (1-p)^3p & \dots
  \end{pmatrix}$.
\end{ex}


\section{Random variables}

$X: \Omega \to \Z$. \\
$E[X] = \sum_{c \in \Z} c \cdot P_r(X = c)$: expected value of $X$. \\
Properties:
\begin{itemize}[label=$\circ$]
  \item $E[f(X)] = \sum_{c \in \Z} f(c) \cdot P_r(X = c)$,
  \item $E[aX + bY] = aE[X] + bE[Y]$,
  \item $E[X \cdot Y] = E[X] \cdot E[Y]$ if $X, Y$ independent,
  \item $P_r(X \geq a) \leq \frac{E[X]}{a}; \; \forall a > 0 \; \forall X \geq 0$ Markov inequality.
\end{itemize}
\begin{ex}
  (Continuing from before). \\
  $X =$ number of trials before return. \\
  $X: \Omega \to \Z$. \\
  $X: 1 \mapsto 1, 01 \mapsto 2, 003 \mapsto 3 \dots$ \\
  $\begin{pmatrix}
    1 & 2 & 3 & 4 & \dots \\
    p & (1-p)p & (1-p)^2 p & (1-p)^3p & \dots
  \end{pmatrix}$ - geometric distribution.
\end{ex}
\begin{claim}
  $E[X] = \frac{1}{p}$.
\end{claim}
\begin{pro}
  $X = \sum_{i=1}^{\infty} X_i$. \\
  $X_i = \begin{cases}
    1: \text{ if trial $i$ is executed} \\
    0: \text{ else}
  \end{cases}$ \\
  \begin{align*}
    E[X] &= E\left[\sum_{i=1}^{\infty} X_i\right] = \sum_{i=1}^{\infty} E[X_i] = \\
    &= \sum_{i=1}^{\infty} (1-p)^{i-1} = \sum_{i=0}^{\infty} (1-p)^i = \frac{1}{1-(1-p)} = \frac{1}{p}.
  \end{align*}
  \qed
\end{pro}
$E[X] = \frac{1}{p}$. \\
$P_r(X \geq 100 \cdot \frac{1}{p}) \stackrel{\text{Markov}}{\leq} \frac{E[X]}{\frac{1}{p}} = \frac{1}{100}$.
\begin{defn}
  $H_n = 1 + \frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{n} = \sum_{i=1}^{\infty} \frac{1}{i}$.
\end{defn}
\begin{theorem}
  $H_n \leq 1 + \ln(n)$.
\end{theorem}
\begin{pro}
  \begin{equation*}
    H_n = 1 + \sum_{i=2}^{n} \frac{1}{i} \stackrel{\text{integral}}{\leq}
    1 + \int_{1}^{n} \frac{dx}{x} = 1 + \Eval{\ln(x)}{1}{n} = 1 + \ln(n).
  \end{equation*}
  % skica
  \qed
\end{pro}



\chapter{Quicksort, min-cut}


\section{Quicksort}

\begin{lstlisting}
  Input: set (no equal element) (unordered list) $S \in \R$
      (or whatever you can compare linearly)
  Output: ordered list
  Code:
    def Quicksort(S):
      if $|S| = 0$ or $|S| = 1$:
        return S
      else:
        a = uniformly at random from S
        $S^{-} = \{b \in S \mid b < a\}$
        $S^{+} = \{b \in S \mid a < b\}$
        return Quicksort($S^{-}$), a, Quicksort($S^{+}$)
\end{lstlisting}
% skica
$C(n)$ - random variable, the number of comparisons in evaluation of Quicksort with $|S| = n$.
\begin{theorem}
  $E[C(n)] = O\left(N \log(n)\right)$.
\end{theorem}
\begin{pro}
  $C(0) = C(1) = 0$. \\
  \begin{align*}
    E[C(n)] &= n - 1 + \sum_{i=1}^{n} \left(E[C(i-1)] + E[C(n-i)]\right) \cdot P_r(a \text{ is $i$-it element}) \leq \\
    &\leq n + \frac{2}{n} \sum_{i=1}^{n-1} E[C(i)].
  \end{align*}
  Induction: \\
  $n = 1: \checkmark$ \\
  $n-1 \to n$:
  \begin{align*}
    E[C(n)] &\leq n + \frac{2}{n} \sum_{i=1}^{n} E[C(i)] \leq \\
    &\leq n + \frac{2}{n} \sum_{i=1}^{n} 5i \log i \leq \\
    &\leq n + \frac{2}{n} \sum_{i=1}^{\lfloor\frac{n}{2}\rfloor} 5i \log i +
      \frac{2}{n} \sum_{i=1+\lfloor\frac{n}{2}\rfloor}^{n-1} 5i \log i \leq \\
    &\leq n + \frac{2}{n} \sum_{i=1}^{\lfloor\frac{n}{2}\rfloor} 5i \log \frac{n}{2} +
      \frac{2}{n} \sum_{i=1+\lfloor\frac{n}{2}\rfloor}^{n-1} 5i \log n \leq \\
    &\stackrel{\refeq{eq:logn2}}{\leq} n + \frac{2}{n} \left(\sum_{i=1}^n 5i \log n - \sum_{i=1}^{\frac{n}{2}} 5i\right) = \\
    &= n + \frac{10}{n} \left(\frac{n(n-1)}{2} \log n - \frac{\frac{n}{2} (\frac{n}{2} + 1)}{2}\right) \leq \\
    &\leq n + 5(n-1) \log n - n < \\
    &< 5n \log n.
  \end{align*}
  \begin{equation}
    \label{eq:logn2}
    \log \frac{n}{2} = \log n - 1
  \end{equation}
  \qed
\end{pro}
$P\left(C(n) \geq b \cdot 5n \log n\right) \stackrel{\text{Markov}}{\leq} \frac{1}{b}$.
\begin{pro} \text{} \\
  2: \\
  Let $S_1, S_2 \dots S_n$ sorted elements of $S$. \\
  Define random variable
  $X_{ij} = \begin{cases}
    1: \text{ if $S_i$ and $S_j$ are compared} \\
    0: \text{ else}
  \end{cases}$ \\
  $C(n) = \sum_{1 \leq i < j \leq n} E[X_{ij}]$. \\
  $E[X_{ij}] = P(S_i$ and $X_j$ compared$)$. \\
  % skica
  $S_{ij}$ - the last set including $S_i$ and $S_j$. \\
  $E[X_{ij}] = \frac{2}{|S_{ij}|} \leq \frac{2}{j-i+1}$. \\
  $|S_{ij}| \geq j - i + 1$. \\
  $S_{ij}$ has everything in between. \\
  \begin{align*}
    \implies E[C(n)] &\leq \sum_{1 \leq i < j \leq n} \frac{2}{j-i+1} = \\
    &\stackrel{k=j-i+1}{\longeq{35pt}} \sum_{i=1}^{n-1} \sum_{k=2}^{n-i+1} \frac{2}{k} \leq \\
    &\leq 2 \cdot n \cdot H_n \leq \\
    &\leq 2 n (1 + \log n).
  \end{align*}
  \qed
\end{pro}


% 2. predavanje: 13.10.

\section{Min-cut}

$G$ multigraph. \\
Cut: $U \subset V(G), \; U \neq \emptyset, V(g)$. \\
$(U, V(G) \setminus U) = \{uv \in E(G) \mid u \in U, v \in V(G) \setminus U\}$. \\
% skica
Problem min-cut: \\
Input: $G$. \\
Output: $\min |(U, V(G) \setminus U)|$ - cut size.
\begin{lstlisting}
  Algorithm 1:
    $x \in V(G)$
    Call maxFlow(G, x, y) $\forall y \in V(G)$
    Take min
\end{lstlisting}
$maxFlow$ is Edmonds-Karp algorithm $O\left(|V| |E|^2\right)$.
\begin{lstlisting}
  Algorithm 2 (Stoer Wagner)
\end{lstlisting}
Is $O\left(|E| |V| + |V| log |V|\right)$.
\begin{lstlisting}
  Algorithm randMinCut:
    $G_0 = G$
    i = 0
    while $|V(G_i)| > 2$:
      $e_i$ = uniformly at random from $G_{i}$
      $G_{i+1} = G_{i}/_{e_{i}}$
      $i = i + 1$
    $u, v = V(G_{n-2})$ # $n = |V(G)|$
    $U = \{w \in V(G) \mid w \text{ is merged into } u\}$
    return $(U, V(G) \setminus U)$
\end{lstlisting}
\begin{theorem}
  Algorithm $randMinCut$ gives you a minimal cut with probability greater or equal to $\frac{2}{n(n-1)}$.
\end{theorem}
\begin{pro} \text{} \\
  Fact 1: $minCut(G_i) \leq minCut(G_i)$;
  \begin{itemize}[label={}]
    %\item $\leq$: %example
    \item $\ngtr$: $minCut$ remains.
  \end{itemize}
  Fact 2: $minCut(G) \leq \delta(G)$. \\
  $k := minCut(G)$. \\
  Let $(A,B)$ be an optimal cut. \\
  $\varepsilon_i = e_i$ not in $(A,B)$.
  \begin{align}
    &P_r(\text{Algorithm returning } (A,B)) \nonumber \\
    &= P_r(\varepsilon_0 \cap \dots \cap \varepsilon_{n-3}) \quad i = 0 \dots n-3 \nonumber  \\
    &= P_r(\varepsilon_0 \cap \dots \cap \varepsilon_{n-4}) \cdot
      P_r(\varepsilon_{n-3} \mid \varepsilon_0 \cap \dots \cap \varepsilon_{n-4}) \nonumber  \\
    &= P_r(\varepsilon_{n-3} \mid \cap_{i=0}^{n-4} \varepsilon_i) \cdot
      P_r(\varepsilon_{n-3} \mid \cap_{i=0}^{n-4} \varepsilon_i) \nonumber \\
    &\text{ } \dots P_r(\varepsilon_1 \mid \varepsilon_0) \cdot P_r(\varepsilon_0) \nonumber \\
    &\stackrel{\refeq{eq:ei-geq}}{\geq} \frac{n-2}{n} \cdot \frac{n-3}{n-1} \dots \frac{1}{3} = \frac{2}{n(n-1)}. \nonumber
  \end{align}
  \begin{equation*}
    P_r(\overline{\varepsilon_i} \mid \varepsilon_{i-1} \cap \dots \cap \varepsilon_0) =
      %\frac{k}{|E(G_i)|} \stackrel{\text{\refeq{E-gi}}}{\leq} \frac{k}{\frac{(n-i)k}{2}} = \frac{2}{n-i}
      \frac{k}{|E(G_i)|} \stackrel{\refeq{eq:E-gi}}{\leq} \frac{k}{\frac{(n-i)k}{2}} = \frac{2}{n-i}
  \end{equation*}
  \begin{equation}
    \label{eq:E-gi}
    |E(G_i)| \geq \frac{(n-i) \delta(G)}{2} \geq \frac{(n-i)k}{2}.
  \end{equation}
  \begin{equation}
    \label{eq:ei-geq}
    P_r(\varepsilon_i \mid \varepsilon_{i-1} \cap \dots \cap \varepsilon_0) \geq 1 - \frac{2}{n-i} = \frac{n-2-i}{n-i}.
  \end{equation}
  \qed
\end{pro}
\begin{theorem}
  Running $randMinCut \; n(n-1)$ times and taking best output gives correct solution with probability $\geq 0.86$.
\end{theorem}
\begin{pro}
  $A_i$ - event that $i$-th run gives sub-optimal solution.
  \begin{align*}
    P_r(\text{solution not correct}) &= P_r(A_1 \cap \dots \cap A_{n(n-1)}) \\
    &= \prod_{i=1}^{n(n-1)} P_r(A_i) \leq \left(1 - \frac{2}{n(n-1)}\right)^{n(n-1)} \\
    &\stackrel{\refeq{eq:1-x-leq-e}}{\leq} e^{-\frac{2}{n(n-1)} \cdot n(n-1)} = e^{-2} \leq 0.14.
  \end{align*}
  \begin{equation}
    \label{eq:1-x-leq-e}
    1 - x \leq e^x \; \forall x \in \R.
  \end{equation}
  \qed
\end{pro}
If we run $n(n-1) log(n)$ times $\to O\left(\frac{1}{n}\right)$. \\
$O\left(n^2 \log n \cdot n\right)$. \\
Improved: $O\left(n^2 \log^3 n\right)$.



\chapter{Complexity classes}


Decision problem - yes/no question on a set of inputs = asking $w \in \Pi$. \\
Randomized algorithms:
\begin{itemize}
  \item Las Vegas algorithms: always gives correct solution, example: $Quicksort$.
  \item Monte Carlo algorithms: it can give wrong answers.
    Monte Carlo algorithms subtypes:
    \begin{itemize}
      \item type(1): $\begin{cases}
          \omega \in \Pi \implies \text{ alg. returns \sn{$\omega \in \Pi$} with probab. } \geq \frac{1}{2} \\ 
          \omega \notin \Pi \implies \text{ alg. returns \sn{$\omega \in \Pi$} with probab. } = 0 
        \end{cases}$
      \item type(2): $\begin{cases}
          \omega \in \Pi \implies \text{ alg. returns \sn{$\omega \in \Pi$} with probab. } = 1 \\ 
          \omega \notin \Pi \implies \text{ alg. returns \sn{$\omega \in \Pi$} with probab. } \leq \frac{1}{2} 
        \end{cases}$
      \item type(3): $\begin{cases}
          \omega \in \Pi \implies \text{ alg. returns \sn{$\omega \in \Pi$} with probab. } \geq \frac{3}{4} \\ 
          \omega \notin \Pi \implies \text{ alg. returns \sn{$\omega \in \Pi$} with probab. } \leq \frac{1}{2}
        \end{cases}$
    \end{itemize}
    type(1) and type(2): one-sided error, type(3): 2-sided error. \\
    $\frac{1}{2}, \frac{3}{4}$ and $\frac{1}{4}$ arbitrary numbers, can be something different (for type(3) better than coin flip).
\end{itemize}
\begin{ex}
  Decision problem: does a graph $G$ have $minCut \leq k$? \\
  Run $randMinCut(G) \; n(n-1)$ times.
  \begin{lstlisting}
    Algorithm randMinCut:
      if one of runs gives $|(A,B)| \leq k$:
        return true
      else:
        return false
  \end{lstlisting}
\end{ex}
Complexity classes:
\begin{itemize}
  \item RP (randomized polynomial time): decisional problems for which there exists Monte Carlo algorithm of type(1)
    with polynomial time complexity (worst case).
  \item co-RP: decisional problems for which there exists Monte Carlo algorithm of type(2) with polynomial time complexity
    (worst case).
  \item BRP (bounded-error probabilistic polynomial time): decisional problems for which there exists Monte Carlo algorithm of type(3)
    with polynomial time complexity (worst case).
  \item ZPP (zero-error probabilistic polynomial time): decisional problems for which there exists Las Vegas algorithm
    with expected polynomial time complexity (worst case).
\end{itemize}
% skica
ZPP = RP $\cap$ co-RP.



\chapter{Chernoff bounds}


\begin{theorem}
  Let $X_1, X_2 \dots X_n$ independent random variables with image $\{0, 1\}$. Let \\
  $p_i = P_r(X_i = x_i), \\
  X = \sum_{i=1}^{n} X_i$ and \\
  $\mu = E(X) = p_1 + \dots + p_n$. \\
  For every $\delta \in (0,1)$:
  \begin{align*}
    &P_r(X - \mu \geq \delta \mu) \leq e^{-\frac{\delta^2 \mu}{3}} \\
    &P_r(\mu - X \leq \delta \mu) \leq e^{-\frac{\delta^2 \mu}{2}} \\
    \implies &P_r(|X - \mu| \geq \delta \mu) \leq e^{-\frac{\delta^2 \mu}{3}}. \\
  \end{align*}
\end{theorem}
% skica
Probability falls extremely quickly after $E(X)$.


% 3. predavanje: 20.10.

\begin{pro}
  \begin{align*}
    P_r(X - \mu \geq \delta \mu) &= P_r(X \geq \mu(1+\delta)) \\
    &\stackrel{t>0}{=} P_r(tX \geq t\mu(1+\delta)) \\
    &\stackrel{e^y>0}{\longeq{10pt}} P_r(e^{tX} \geq e^{t\mu(1+\delta)}) \\
    &\stackrel{\text{Markov}}{\leq} \frac{E\left(e^{tX}\right)}{e^{t\mu(1+\delta)}} \\
    &\stackrel{\refeq{eq:et_x}}{\leq} \frac{e^{(e^t-1)\mu}}{e^{t\mu(1+\delta)}} \\
    &\stackrel{\refeq{eq:leq_in_exp}}{\leq} e^{-\mu \frac{\delta^2}{3}}.
  \end{align*}
  \begin{align}
    E(e^{tX}) &= E(e^{tX_1 + \dots + tX_n}) \nonumber \\
    &= E(e^{tX_1} \dots e^{tX_n}) \nonumber \\
    &\stackrel{\text{independent}}{\longeq{25pt}} \prod_{i=1}^{n} E(e^{tX_i}) \nonumber \\
    &\stackrel{\refeq{eq:et_xi}}{\leq} \prod_{i=1}^{n} e^{p_i(e^t-1)} \nonumber \\
    &= e^{(e^t-1) \sum_{i=1}^{n}p_i} \nonumber \\
    &= e^{(e^t-1)\mu} \label{eq:et_x}.
  \end{align}
  \begin{equation}
    E(e^{tX_i}) = p_i \cdot e^t + (1-p_i) \cdot e^0 = 1+p_i(e^t-1) \stackrel{\refeq{eq:1-x-leq-e}}{\leq} e^{p_i(e^t-1)}.
    \label{eq:et_xi}
  \end{equation}
  Want:
  \begin{equation}
    e^t - 1 - t(1+\delta) \leq -\frac{\delta^2}{3} \; \forall \delta \in (0,1) \label{eq:leq_in_exp}
  \end{equation}
  \begin{align*}
    &t = \ln(1+\delta) \\
    &f(\delta) = 1 + \delta - 1 - (1+\delta) \ln(1+\delta) + \frac{\delta^2}{3} \stackrel{?}{\leq} 0 \\
    &f(0) = 0 \\
    &f^{'}(\delta) = 1 - \ln(1+\delta) - 1 + \frac{2}{3} \delta = \frac{2}{3} \delta - \ln(1+\delta) \stackrel{?}{\leq} 0 \\
    &\frac{2}{3} \delta \leq \ln(1+\delta) \\
    % skica
    &\delta=1: \; \frac{2}{3} \stackrel{?}{\leq} \ln(2) \approx 0.69 \checkmark
  \end{align*}
  \begin{align*}
    P_r(\mu - X \leq \delta \mu) &= P_r(X \geq \mu(1-\delta)) \\
    &\stackrel{t>0}{=} P_r(tX \geq t\mu(1-\delta)) \\
    &\stackrel{e^y>0}{\longeq{15pt}} P_r(e^{tX} \geq e^{t\mu(1-\delta)}) \\
    &\leq \dots \leq \frac{e^{(e^t-1)\mu}}{e^{t\mu(1-\delta)}}.
  \end{align*}
  Want: $e^t - 1 - t(1-\delta) \leq -\frac{\delta^2}{2} \; \forall \delta \in (0,1)$:
  \begin{align*}
    &t = \ln(1-\delta) \\
    &f(\delta) = 1 - \delta - 1 - (1-\delta) \ln(1-\delta) + \frac{\delta^2}{2} \stackrel{?}{\leq} 0 \\
    &f(0) = 0 \\
    &f^{'}(\delta) = - 1 + 1 - \ln(1-\delta) + \delta \stackrel{?}{\leq} 0 \\
    &\frac{2}{3} \delta \leq \ln(1+\delta) \\
    &\ln(1-\delta) \stackrel{?}{\leq} -\delta \checkmark
    % skica
  \end{align*}
  \qed
\end{pro}
$X_i \sim \begin{pmatrix}0 & 1 \\ \frac{1}{2} & \frac{1}{2}\end{pmatrix}$ \\
$X = \sum_{i=1}^{n} X_i$ \\
$\mu = \frac{n}{2}$
\begin{align*}
  P_r\left(|X-\mu| \geq \sqrt{\frac{3}{2}n \ln(n)}\right)
  &= P_r\left(|X-\mu| \geq \frac{n}{2} \sqrt{\frac{6}{n} \ln(n)}\right) \\
  &\stackrel{\text{Chernoff}}{\leq} 2 e^{-\frac{\frac{n}{2} \frac{6}{n} \ln(n)}{3}} = \frac{2}{n};
\end{align*}
For \sn{big} $n$ is $\delta \in (0,1)$, \\
$\mu = \frac{n}{2}, \delta = \sqrt{\frac{6}{n} \ln(n)}$. \\
$d = \sqrt{\frac{3}{2}n \ln(n)}$
% skica
\begin{equation*}
  \implies P_r\left(X \in \left(\mu - \sqrt{\frac{3}{2}n \ln(n)}, \mu + \sqrt{\frac{3}{2}n \ln(n)}\right)\right)
  \geq 1 - \frac{2}{n}.
\end{equation*}
% skica
\begin{claim} \text{} \\
  Let $X_1, X_2 \dots$ independent random variables with image $\{0,1\}$. \\
  $P_r(X_i = 1) = \frac{1}{2} \; \forall i$. \\
  Let $X = \sum_{i=1}^{cm} X_i$ where $c \geq 4$. \\
  Then $P_r(X \leq m) \leq e^{-\frac{cm}{16}}$.
\end{claim}
\begin{pro}
  \begin{align*}
    P_r(X \leq m) &= P_r\left(\frac{cm}{2} - X \geq \frac{cm}{2} - m\right) \\
    &= P_r\left(\frac{cm}{2} - X \geq \frac{cm}{2} \left(1 - \frac{2}{c}\right)\right) \\
    &\stackrel{\text{Chernoff}}{\leq} e^{-\frac{\frac{cm}{2} \left(1-\frac{2}{c}\right)^2}{2}} \\
    &\stackrel{\refeq{eq:1-2c}}{\leq} e^{-\frac{\frac{cm}{2} \frac{1}{4}}{2}} = e^{-\frac{cm}{16}}.
  \end{align*}
  \begin{equation}
    \label{eq:1-2c}
    \quad 1-\frac{2}{c} \geq \frac{1}{2} \text{ if } c \geq 4
  \end{equation}
  \qed
\end{pro}
Back to Quicksort.
\begin{theorem} \text{} \\
  With probability $\geq 1 - \frac{1}{n}$ Quicksort uses at most $48n\ln(n)$ comparisons.
\end{theorem}
\begin{pro} \text{} \\
  % skica (i)
  For $s \in S$ define $S_1^S \dots S_{t_s}^S \neq \emptyset$ sets that include $s$,
  $t_s$ - number of comparisons with $s$ where $s$ is not a pivot $+1$. \\
  Define: iteration $i$ is successful if $|S_{i+1}| \leq \frac{3}{4} |S_i|$ ($\frac{1}{2}$ is too strict).
  \begin{equation*}
    X_i = \begin{cases}
      1: \text{ if iteration } i \text{ is successful} \\
      0: \text{ else}
    \end{cases}
  \end{equation*}
  % skica (i)
  $P_r(X_i = 1) \geq \frac{1}{2}$ \\
  $S_i: n \to \frac{3}{4} n \to (\frac{3}{4})^2 n \to \dots \to 1$. \\
  Notice: max number of iterations is $\log_{\frac{4}{3}}(n) = \frac{\ln(n)}{\ln(4)-\ln(3)}$. \\
  Probability that we haven't succeeded in $\log_{\frac{4}{3}}(n)$ steps:
  \begin{align}
    P_r\left(\sum_{i=1}^{c \log_{\frac{4}{3}}(n)} X_i < \log_{\frac{4}{3}}(n)\right) &\leq
      P_r\left(\sum_{i=1}^{c \log_{\frac{4}{3}}(n)} Y_i < \log_{\frac{4}{3}}(n)\right) \label{eq:X_to_Y} \\
    &\stackrel{\text{Chernoff}}{<} e^{-\frac{c \log_{\frac{4}{3}}(n)}{24}} \nonumber \\
    &= e^{-\frac{c \ln(n) \log_{\frac{4}{3}}(e)}{24}} \nonumber\\
    &= \frac{1}{n} \frac{c \log_{\frac{4}{3}}(e)}{24} \nonumber \\
    &\stackrel{\refeq{eq:log-c}}{\leq} \left(\frac{1}{n}\right)^2. \nonumber
  \end{align}
  \refeq{eq:X_to_Y} because $X_i$ not independent,
  $Y_i \sim \begin{pmatrix}0 & 1 \\ \frac{1}{2} & \frac{1}{2}\end{pmatrix}$ independent,
  \begin{equation}
    \label{eq:log-c}
    \log_{\frac{4}{3}}(e) \approx 3.4, \; c=14.
  \end{equation}
  $P_r(t_s \geq c \log_{\frac{4}{3}}(n)) \geq \left(\frac{1}{n}\right)^2$ for one $s$. \\
  $c=14 \implies$ at least $48 \ln(n)$ iterations with probability $\leq \left(\frac{1}{n}\right)^2$. \\
  With probability as least $1-\frac{1}{n}$ for all $s \in S$ it holds that $s$ has $\leq 48 \ln(n)$ comparisons with a pivot. \\
  $\implies$ total number of comparisons $n \cdot 48 \ln(n)$ with probability at least \\
  $1 - \frac{1}{n}$.
  \qed
\end{pro}



\chapter{Monte Carlo methods}


\section{Example 1}

% skica
Area of circle $= \frac{\pi}{4}$. \\
$X_i = \begin{cases}
  1: \text{ if you hit the area of circle} \\
  0: \text{ else}
\end{cases}$ \\
$P_r(X_i = 1) = \frac{\frac{\pi}{4}}{1} = \frac{\pi}{4}$. \\
$E(X_i) = \frac{\pi}{4}$. \\
$X = \frac{\sum_{i=1}^{n} X_i}{n}$. \\
$E(X) = \frac{n \cdot E(X_i)}{n} = E(X_i)$.


% 4. predavanje: 27.10.

\section{Example 2}

$I = \int_{\Omega} f(x) dx$ - volume. \\
% skica
$X_i = \begin{cases}
  1: \; F(x_i,y_i) \leq z_i \\
  0: \text{ otherwise}
\end{cases}$ \\
$v \cdot E\left(\frac{\sum_{i=1}^{n} X_i}{n}\right) = I$.


\section{$(\varepsilon,\delta)$-approximation}

\begin{defn}[$(\varepsilon,\delta)$-approximation]
  A random algorithm gives a \\
  $(\varepsilon,\delta)$-approximation for value $v$ if the output $X$ satisfies:
  \begin{equation*}
    P_r\left(|X-v| \leq \varepsilon v\right) \geq 1 - \delta.
  \end{equation*}
\end{defn}
\begin{theorem}
  Let $X_1 \dots X_n$ be independent and identically distributed indicator variables.
  Let $\mu = E(X_i), \; Y = \frac{\sum_{i=1}^{m} X_i}{m}$.
  If $m \geq \frac{3 \ln\left(\frac{2}{\delta}\right)}{\varepsilon^2 \mu}$, then
  $P_r\left(|Y-\mu| \geq \varepsilon \mu\right) \leq \delta$
  $\implies Y$ is $(\varepsilon,\delta)$-approximation for $\mu$.
\end{theorem}
\begin{pro} \text{} \\
  $X = \sum_{i=1}^{n} X_i$ \\
  $E(X) = m E(x_i) = m \mu$ \\
  $m \geq \frac{3 \ln\left(\frac{2}{\delta}\right)}{\varepsilon^2 \mu}$
  \begin{align*}
    P_r\left(|Y-\mu| \geq \varepsilon \mu\right) &= P_r\left(\left|\frac{X}{m}-\mu\right| \geq \varepsilon \mu\right) \\
    &= P_r\left(\frac{1}{m} \left|X-E(X)\right| \geq \frac{1}{m} \varepsilon E(x)\right) \\
    &\stackrel{\text{Chernoff}}{\leq} 2 e^{-\frac{\varepsilon^2 E(x)}{3}} \\
    &= 2 e^{-\frac{\varepsilon^2 \mu m}{3}} \\
    &\leq 2 e^{-\frac{\varepsilon^2 \mu}{3} \cdot \frac{3 \ln\left(\frac{2}{\delta}\right)}{\varepsilon^2 \mu}} = \delta.
  \end{align*}
  \qed
\end{pro}
Back to example 1: \\
$E(Y) = \frac{\pi}{4}, \delta = \frac{1}{1000}$ ($99.9\%$ sure), $\varepsilon = \frac{1}{10000}$ \\
$\implies M = \frac{3 \ln\left(\frac{2}{\frac{1}{1000}}\right) 4}{\pi \left(\frac{1}{10000}\right)^2} \approx 29106$. \\
Problems for MC (Monte-Carlo):
\begin{itemize}
  \item rare events, e.g. $X \sim \begin{pmatrix}0 & 10^{100} \\ 1-10^{-20} & 10^{-20}\end{pmatrix}, \; E(X) = 10^{80}$
    % skica
\end{itemize}


\section{DNF counting}

CNF: $(X_{i_1} \lor \overline{X_{i_2}} \lor X_{i_4}) \land (X_{i_1} \lor \overline{X_{i_3}}) \land \dots$ \\
DNF: $(\overline{X_{i_1}} \land X_{i_2} \lor \overline{X_{i_4}}) \lor \dots$
- easy to determine if solution exists. \\
Question: number of solutions to a given DNF? \\
Observation: CNF $F$ has a solution $\iff$ DNF $\neg F$ has less than $2^n$ solutions, $n$ is number of samples.
\begin{lstlisting}
  ALG_1(F):
    $x = 0$
    for $i$ in range(1,$m+1$):
      $x_1 \dots x_n$ uniformly random from $\{0,1\}^n$
      if $F(x_1 \dots x_n) = 1$:
        $x += 1$
    return $\frac{x}{m} \cdot 2^n$
\end{lstlisting}
$Y = \frac{\sum_{i=1}^{m} X_i}{m}$ \\
$(\varepsilon,\delta)$-approximation for $Y$. \\
$E(Y) = \frac{\text{number of solutions of }F}{2^n} = \frac{c(F)}{2^n}$ \\
$m \geq \frac{3 \ln\left(\frac{2}{\delta}\right)}{\varepsilon^2 E(X)} =
\frac{3 \ln\left(\frac{2}{\delta}\right)}{\varepsilon^2} \cdot \frac{2^n}{c(F)}$ \\
$c(F)$ very small $\to$ $m$ exponentially big $\to$ not good (we need a lot of samples).
\begin{defn} \text{} \\
  $SC_i = \{(a_1 \dots a_n) \in \{0,1\}^n \text{ such that } F = F_1 \lor \dots \lor F_t, \; F_i(a_1 \dots a_n) = 1\}$.
  % skica
\end{defn}
$|SC_i| = 2^{n-l_i}$, $l_i$: number of values in $F_i$ \\
$U = \{(i,a) \mid i \in \{1,2 \dots t\}, a \in SC_i\}$ \\
$U = \sum_{i=1}^{t} |SC_i|$ - $O(tn)$ (space smaller than $\{0,1\}^n$) \\
$S = \{(i,a) \in U \mid a \in SC_i, \; a \not \in SC_j \; 1 \leq j < i\}$ \\
$|S| = |SC_1| + \dots + |SC_t| = c(F)$.
\begin{lstlisting}
  ALG_2(F):
    $x = 0$
    for i in range(1,$m+1$):
      $(i, a)$ uniformly random from U $\refeq{details}$
      if $(i, a) \in S$: $\refeq{eq:sci}$
        $x += 1$
    return $\frac{x}{m} \cdot |U|$
\end{lstlisting}
\begin{align}
  &a \in SC_i \to O(n), \; a \notin SC_j \; j = 1 \dots i-1 \to O(tn) \label{eq:sci} \\
  &\implies \; O(tn), m \text{ times}. \nonumber
\end{align}
\begin{align}
  &\text{watch for details on how to, e.g. } x_2, x_2 \land x_3 \label{details} \\
  &x_2 \text{ is more probable than } x_2 \land x_3 \to O(1). \nonumber
\end{align}
\begin{theorem}
  For $m = \left\lceil\frac{3t \ln(\left(\frac{2}{\delta}\right))}{\varepsilon^2}\right\rceil$
  algorithm returns $(\varepsilon,\delta)$-approximation
  in $O\left(\frac{t^n n \ln\left(\frac{2}{\delta}\right)}{\varepsilon^2}\right)$ time.
\end{theorem} 
\begin{pro} \text{} \\
  $O(t \cdot n \cdot m)$. \\
  Insert $m = ...$
  \qed
\end{pro}
Prove
\begin{equation*}
  P_r(Y|U| - c(F) > \varepsilon c(F)) < \delta:
\end{equation*}
$c(F) = |S|, E(Y) = \frac{|S|}{|U|}$
\begin{align*}
  P_r(Y|U| - c(F) > \varepsilon c(F)) &= P_r(|U|(Y - E(Y)) > \varepsilon |U| E(Y)) \leq \delta
\end{align*}
if
\begin{equation*}
  m \geq \frac{3 \ln\left(\frac{2}{\delta}\right)}{\varepsilon^2 E(Y)} \geq \frac{3 \ln\left(\frac{2}{\delta}\right)t}{\varepsilon^2}
\end{equation*}
where
\begin{equation*}
  E(Y) = \frac{|S|}{|U|} \geq \frac{1}{t}
\end{equation*}
($=$ if disjoint). \\
In new space $E(Y)$ much larger $\implies$ $m$ smaller.
% skica



\chapter{Polynomials}


Let $\F$ be a field. \\
$\F$ can be $\R, \C, \Z_p, \F_{p^n}$. \\
$\F[x_1 \dots x_n]$ algebra of polynomials with values $x_1 \dots x_n$. \\
$f \in \F[x_1 \dots x_n]$ \\
$deg(f[x_1 \dots x_n]) := deg(f[x \dots x])$.
\begin{theorem}
  Let $p(x_1 \dots x_n) \in \F[x_1 \dots x_n]$ have the degree $d \geq 0$ and $p \neq 0$.
  Let $S \subset \F$ be finite.
  If $(r_1 \dots r_n)$ is uniformly at random element from $S^n$.
  Then $P_r(p(r_1 \dots r_n) = 0) \leq \frac{d}{|S|}$.
\end{theorem}
\begin{pro}
  Induction on $n$. \\
  $n=1$:
  \begin{align*}
    &p(x) = (x-z_1) (x-z_2) \dots (x-z_j) q(z) \\
    &\text{number of zeros } \leq \text{ degree - fact} \\
    &P_r(p(r_1) = 0) = \frac{\text{number of zeros}}{|S|} \leq \frac{d}{|S|}.
  \end{align*}
  $n-1 \to n$:
  \begin{align*}
    &\text{rewrite }p: \\
    &p(x_1 \dots x_n) = \sum_{i=0}^{j} x^i p_i(x_2 \dots x_n) \\
    &j \leq d \\
    P_r(p(r_1 \dots r_n) = 0) &= P_r(p(r_1 \dots r_n = 0) \mid p_j(r_2 \dots r_n) = 0) \cdot P_r(p_j(r_2 \dots r_n) = 0) \\
    &+ P_r(p(r_1 \dots r_n = 0) \mid p_j(r_2 \dots r_n) \neq 0) \cdot P_r(p_j(r_2 \dots r_n) \neq 0) \\
    &\leq 1 \cdot \frac{d-j}{|S|} + \frac{j}{|S|} \cdot 1 \\
    &= \frac{d}{|S|}, \\
  \end{align*}
  because
  \begin{align*}
    &P_r(p_j(r_2 \dots r_n) = 0) \leq \frac{d-j}{|S|} \\
    &P_r(p(r_1 \dots r_n = 0) \mid p_j(r_2 \dots r_n) \neq 0) \leq \frac{j}{|S|}.
  \end{align*}
  \qed
\end{pro}
\underline{Problem}: \\
Let $A,B,C \in \F^{n \times n}$, is $A \cdot B = C$? \\
Computing $A \cdot B$:
\begin{itemize}
  \item school-book algorithm: $O\left(n^3\right)$,
  \item Strassen algorithm: $O\left(n^{2,807\dots}\right)$,
  \item galactic algorithm: $O\left(n^{2.372\dots}\right)$ - has enormous constants.
\end{itemize}
\begin{lstlisting}
  RAND_ABC(A,B,C):
    for $i$ in range(1,$k+1$):
      $x$ uniformly at random from $\{0,1\}^{n}$
      if $A \cdot (B \cdot x) \neq x$:
        return false
    return true
\end{lstlisting}
$O\left(k n^2\right)$. \\


% 5. predavanje: 3.11.

If $A \cdot B = C$, algorithm returns true. \\
If $A \cdot B \neq C$: \\
\begin{align*}
  P_r(ABx = Cx) &= P_r((AB-C)x = 0) \\
  &= P_r\left(||(AB-C)x||^2 = 0\right) \stackrel{\text{Poly}}{\leq} \frac{2}{3}.
\end{align*}
$||(AB-C)x||^2$ - polynomial in $x_1 \dots x_n$ of degree $2$. \\
If $A \cdot B \neq C$, then algorithm return false with probability at least $1 - \left(\frac{2}{3}\right)^k$. \\
\underline{Problem}: \\
1-factor in bipartite graphs. \\
% skica
$|V(g)| = 2n$. \\
Represent $G$ with $n \times n$ matrix $Z = (Z_{ij})_{i,j=1}^n$ \\
$Z_{ij} = \begin{cases}
  X_{ij}: &\text{ if } a_i b_j \in E(x) \qquad \text{(X: variable)} \\
  0: &\text{ else}
\end{cases}$ \\
\begin{align*}
  det Z(x_{11} \dots x_{nn}) &= \sum_{\pi \in S_n} sign(\pi) z_{1,\pi(1)} \dots z_{n,\pi(n)} \\
  &= \sum_{\pi \in S_n, \pi \text{ defines 1-factor}} sign(\pi) x_{1,\pi(1)} \dots x_{n,\pi(n)}.
\end{align*}
$det Z \neq 0 \iff G$ has 1-factor.
\begin{lstlisting}
  Rand_1factor(G):
    construct Z with variables $x_{11} \dots x_{nn}$
    for $i$ in range(1,$k+1$):
      $u$ <- uniformly at random from  $\{1,2..2n-1\}^{n^2}$ (r_{11} ... r_{nn}?)
      compute $d = det Z(r_{11} ... r_{nn})$
      if $d \neq 0$:
        return true
    return false
\end{lstlisting}
Complexity: $k \cdot$ computing determinant: $O\left(n^3\right)$ (Gaussian elimination). \\
or apply approximation algorithm:
\begin{itemize}
  \item if $G$ has no 1-factor it always returns false,
  \item if $G$ has 1-factor, it returns true with probability at least \\
    $1-\left(\frac{n}{2n}\right)^k = 1 - \left(\frac{1}{2}\right)^k$
    ($k$ konstant, larger set $\implies$ smaller $k$ needed).
\end{itemize}



\chapter{Random graphs}


\section{G(n,p) model}

$G$ is a random Erdös-Rény graph if it has $n$ vertices and each pair of vertices is connected with probability $p$.
\begin{ex}
  $G\left(5, \frac{1}{2}\right)$.
  % skica
\end{ex}
$E($edges in $G$ from $G(n,p)) = \sum_{1 \leq i < j \leq n} E(X_{ij}) = \binom{n}{2} p$. \\
$X_{ij} = \begin{cases}
  1: \text{ if } i \text{ and } j \text{ have edge} \\
  0: \text{ otherwise}
\end{cases}$ \\
$p$ can be function of $n$. \\
$Y_v:$ degree of $v$. \\
$E(Y_v) = (n-1)p$.
\begin{defn} \text{} \\
  We say that a random graph has some property almost surely (A.S.) if \\
  $P_r(G \in G(n,p) \text{ has property}) \stackrel{n \to \infty}{\longrightarrow} 1$.
\end{defn}
\begin{claim} \text{} \\
  Let $p$ be constant.
  Then $G \in G(n,p)$ has diameter 2 A.S.
\end{claim}
\begin{pro} \text{} \\
  Let $u,v \in V(G)$ \\
  % skica
  $X_w = \begin{cases}
    1: \text{ if } uw \in E(G) \text{ in } vw \in E(G) \\
    0: \text{ else}
  \end{cases}$ \\
  $P_r(X_w = 1) = p^2$ \\
  $P_r(X_w = 0 \text{ for all } w \neq u,v) = \left(1-p^2\right)^{n-2}$. \\
  \begin{align*}
    P_r(G \text{ has diameter} > 2) &= P_r(X_w = 0 \text{ for all } w \notin u,v  \text{ for some } u,v) \\
    &\leq \binom{n}{2} (1-p^2)^{n-2} \stackrel{n \to \infty}{\longrightarrow} 0
  \end{align*}
  $\binom{n}{2}$ - polynomial, $e^{\dots}$ - exponent.
  \qed
\end{pro}
$p = f(n)$ \\
$\frac{1}{n}, \frac{1}{n^3}, \frac{\log n}{n}$
\begin{theorem} (without proof) \\
  Let $p$ be a function of $n$, let $G \in G(n,p)$:
  \begin{itemize}
    \item $np < 1 \implies G$ A.S. disconnected with connected components of size $O(\log n)$,
    \item $np = 1 \implies G$ A.S. has $1$ large component of size $O\left(n^{\frac{2}{3}}\right)$,
    \item $np = c > 1 \implies G$ A.S. has giant component of size $dn, \; d \in (0,1)$,
    \item $np \leq (1-\varepsilon) \ln n \implies G$ A.S. disconnected with isolated vertices,
    \item $np > (1-\varepsilon) \ln n \implies G$ A.S. connected.
  \end{itemize}
\end{theorem}
\begin{theorem} \text{} \\
  Let $np = \omega(n) \ln(n)$ for $\omega(n) \to \infty$ \sn{very slowly} think of $\omega(n) = \log (\log n)$,
  then $diam(G)$ in $\Theta\left(\frac{\ln n}{\ln (np)}\right)$ for $G$ in $G(n,p)$.
\end{theorem}
\begin{lemma} \text{} \\
  Let $S \subset V(G), |S| = cn$ for $c \in (0,1]$ and $v \notin S$. \\
  % skica
  then $cnp(1-\omega^{-\frac{1}{3}}) \leq N_S(v) \leq cnp(1+\omega^{-\frac{1}{3}})$ A.S.
  ($\omega^{-\frac{1}{3}} \to 0$ very slowly).
\end{lemma}
\begin{pro}(Lemma): \\
  $E(N_s(v)) = c \cdot n \cdot p, \delta = \omega^{-\frac{1}{3}}$ \\
  \begin{align*}
    P_r(|N_s(v) - cnp| \geq \delta cnp) &\stackrel{\text{Chernoff}}{\leq} 2 e^{-\frac{\omega^{-\frac{2}{3}} cnp}{3}} \\
    &= 2 e^{-\frac{cnp}{3 \omega(n)^{\frac{2}{3}}}} \stackrel{n \to \infty}{\longrightarrow} 0.
  \end{align*}
  For all $v$: $n \cdot 2 e^{-\frac{cnp}{3 \omega(n)^{\frac{2}{3}}}} \stackrel{n \to \infty}{\longrightarrow} 0$.
  \qed
\end{pro}
\begin{pro}(Theorem): \\
  % skica
  $k$ be such that $\sum_{i=0}^{k-1} |N_i| \leq \frac{n}{2}, \sum_{i=0}^{k} |N_i| > \frac{n}{2}$. \\
  $|N_0| = 1$ \\
  $|N_i| \leq |N_{i-1}| \cdot n \cdot p \cdot (1+\omega^{-\frac{1}{3}})$: \\
  $|S| \leq n, \; np(1+\omega^{-\frac{1}{3}})$-each element. \\
  $k \stackrel{?}{=}
  \frac{\log \left(\frac{n}{3}\right)}{\log \left(n \cdot p \cdot \left(1+\omega^{-\frac{1}{3}}\right)\right)}
  = \log_{np(1+\omega^{-\frac{1}{3}})} \frac{n}{3} = \Theta\left(\frac{\ln(n)}{\ln(np)}\right)$. \\
  $|N_{\leq k}| = |N_1 \cup \dots \cup N_k|$.
  \begin{align*}
    |N_{\leq k}| &\leq \sum_{i=0}^{k} (np(1+\omega^{-\frac{1}{3}}))^i \\
    &= \frac{(np(1+\omega^{-\frac{1}{3}}))^{k+1}-1}{np(1+\omega^{-\frac{1}{3}}) - 1} \\
    &< \frac{np(1+\omega^{-\frac{1}{3}})^{k+1}}{\frac{1}{2} np(1+\omega^{-\frac{1}{3}})} \\
    &= 2 np(1+\omega^{-\frac{1}{3}})^k \\
    &\stackrel{k}{=} 2 \cdot \frac{n}{3} \text{ - haven't covered all} \\
    &\implies diam(G) > k \text{ bound from below}.
  \end{align*}


% 6. predavanje: 10.11.

  % skica
  $N_i \subseteq S$ \\
  $\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right) \cdot |N_{i-1}| \leq |N_i|$ \\
  \begin{align*}
    n & \geq \sum_{i=0}^{k} |N_i| \\
    & \geq \sum_{i=0}^{k} \left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)^i \\
    &= \frac{\left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)^{k+1} - 1}
      {\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right) - 1} \\
    &\geq \left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)^k \qquad / \ln
  \end{align*}
  $\frac{\ln n}{\ln (np)} \approx \frac{\ln n}{\ln \left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)} \geq k$. \\
  $\implies w \in S^{'}$. \\
  Number of neighbors in $N_k$ A.S. $\geq 1$, \\
  $|N_k| \geq \left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)^k \approx c \cdot n$ \\
  $\implies diam (G) = k + 1 $ A.S.
  \qed
\end{pro}

\subsection{Scale free property}

$G \in G(n,p)$. \\
% skica
In real world:
% skica, skica
$p(k) = $ proportion of degree $k$ vertices. \\
$\log(p(k)) = -\gamma \cdot \log k$ \\
$p(k) = k^{-\gamma}$. \\
Internet: $\gamma \approx 3.42$, \\
protein reactions: $\gamma \approx 2.89$.


\section{Barbási-Albert Model}

B.A. model. \\
Start with $m$ modes. \\
Grow:
\begin{itemize}
  \item add node $v$,
  \item add $m$ edges from $v$ (to $u$),
  \item for each new edge: $P(v \sim u) = \frac{deg u}{\sum_x deg x}$.
\end{itemize}
\begin{theorem} \text{} \\
  B.A. model has scale free property, in particular \\
  $p_k = \frac{2 m (m+1)}{k (k+1) (k+2)}$.
\end{theorem}
\begin{defn} \text{} \\
  $p_n(k)$: expected proportion of degree $k$ vertices in graph with $k$ vertices, \\
  $p_k := \lim_{n \to \infty} p_n(k)$.
\end{defn}
\begin{pro} \text{} \\
  $p_n(k) \cdot n$: expected number of degree $k$ vertices, \\
  $p_n(k) n \cdot \frac{k}{\sum_u deg u} m = p_n(k) \cdot \frac{k}{2}$:
    expected number of degree $k$ vertices changing into degree $k+1$ vertices. \\
  $\sum_u deg u = 2 |E|$ \\
  $p_{n+1}(k) \cdot (n+1) = p_n(k) \cdot n - p_n(k) \cdot \frac{k}{2} + p_n(k-1) \cdot \frac{k-1}{2}$, where \\
  $p_n(k) \cdot n$: degree $k \to k$, \\
  $p_n(k) \cdot \frac{k}{2}: k \to k+1$, \\
  $p_n(k-1) \cdot \frac{k-1}{2}: k-1 \to k$. \\
  For $n$ very big (very close to limit): \\
  $p_k \cdot (n+1) = p_k \cdot n - p_{k-1} \cdot \frac{k}{2} + p_{k-1} \cdot \frac{k-1}{2}$ \\
  $\implies p_k = \frac{k-1}{k+2} p_{k-1}$. \\
  For degree $m$: \\
  $(n+1) \cdot p_{n+1}(m) = p_n(m) \cdot n - p_n(m) \cdot \frac{m}{2} + 1$ \\%??
  $(n+1) \cdot p_m = n \cdot p_m - \frac{m}{2} \cdot p_m + 1$
  \begin{align*}
    &p_m = \frac{2}{m+2} \\
    \implies &p_{m+1} = \frac{2}{m+2} \cdot \frac{m}{m+3} \\
    \implies &p_{m+2} = \frac{2m(m+1)}{(m+2)(m+3)} \\
    \implies &p_k = \frac{2m(m+1)}{k(k+1)(k+2)}.
  \end{align*}
  \qed
\end{pro}



\chapter{Markov chains}


$\Omega$: finite set (of states).
\begin{defn}[Markov chain] \text{} \\
  (Discrete time) Markov chain is a sequence of random variables $X = X_0, X_1, X_2 \dots$ with image $\Omega$ and properties:
  \begin{itemize}
    \item $P(X_{i+1} = x \mid X_i = x_i, X_{i-1} = x_{i-1} \dots X_0 = x_0) =\\ P(X_{i+1} = x \mid X_i = x_i)$
      - Markov property,
    \item $P(X_{i+1} = x \mid X_i = y) = P(X_1 = x \mid X_0 = y)$ - time is homogenous.
  \end{itemize}
\end{defn}
\begin{ex} \text{} \\
  $\Omega = \Z_5$ \\
  $P(X_{i+1} = x+1 \mid X_i = x) = \frac{1}{2}$ \\
  $P(X_{i+1} = x-1 \mid X_i = x) = \frac{1}{2}$. \\
  % skica
\end{ex}
\begin{defn}[Transition matrix] \text{} \\
  $\Omega = \{x_1 \dots x_n\}$ \\
  $p_{ij} = P(X_{t+1} = j \mid X_t = i)$ \\
  $\begin{bmatrix}
    p_{11} & \dots \\ p_{1n} \\
    \vdots & & \vdots \\
    p_{n1} & \dots & p_{nn}
  \end{bmatrix}$.
\end{defn}
\begin{defn}[Transition graph] \text{} \\ 
  Edge between states $i$ and $j$ exists if $p_{ij} > 0$.
  % skica
\end{defn}
$P$ is stochastic matrix: \\
$p_{ij} \in [0,1]$ \\
$\sum_j p_{ij} = 1$ (row sum). \\
We choose beginning state randomly. \\
$q(0) = (q_1(0) \dots q_n(0))$ \\
$P(X_0 = i) = q_i(0)$. \\
Let $q(t) = (q_1(t) \dots q_n(t))$ \\
$P(X_t = i) = q_i(t)$. \\
It holds: $q(t) = q(t-1) \cdot P = q(0) \cdot P^t$. \\
$\begin{bmatrix}
  0 & \frac{1}{2} & 0 & 0 & \frac{1}{2} \\
  \frac{1}{2} & 0 & \frac{1}{2} & 0 & 0 \\
  0 & \frac{1}{2} & 0 & \frac{1}{2} & 0 \\
  0 & 0 & \frac{1}{2} & 0 & \frac{1}{2} \\
  \frac{1}{2} & 0 & 0 & 0 & \frac{1}{2}
\end{bmatrix}$ \\
$q(0) = (1, 0, 0, 0, 0)$ \\
$q(1) = (1, \frac{1}{2}, 0, 0, \frac{1}{2})$ \\
$q(2) = (\frac{1}{2}, 0, \frac{1}{4}, \frac{1}{4}, 0)$ \\
$\vdots$ \\
% skica
\begin{defn} \text{} \\
  \begin{itemize}
    \item Distribution $\pi$ is stationary if $\pi = \pi \cdot P$,
    \item $f_{ij}$: probability that $X_t = x_j$ for some $t$ assuming $X_0 = x_i$,
      % skica
    \item $h_{ij}$: expected number of steps needed to get to state $X_j$ strting in $X_i$ (hitting time),
    \item $N(i, t, q(0))$: expected number of times we visit $x_i$ after $t$ steps starting with distribution $q(0)$,
    \item $\forall f_{ij} > 0 \; \iff$ transition graph is strongly connected $\iff$ we say the chain is irreducible,
    \item M.C. is aperiodic if there is no $c \in \{2, 3, 4 \dots\}$ such that all lengths of cycles are divisible by $c$.
  \end{itemize}
\end{defn}
\begin{theorem} \text{} \\
  Let $X$ be finite irreducible M.C. Then:
  \begin{enumerate}[label=\alph*)]
    \item there exists unique stationary distribution $\pi = (\pi_1 \dots \pi_n)$,
    \item $f_{ii} = 1, h_{ii} = \frac{1}{\pi_i}$,
    \item $\lim_{t \to \infty} \frac{N(i, t, q(0))}{t} = \pi_i$ - approaches $\pi$ regardless of $q(0)$,
    \item if $X$ is aperiodic: $\lim_{t \to \infty} q(0) \cdot P^t = \pi$. 
  \end{enumerate}
\end{theorem}


% 7. predavanje: 17.11.

\begin{ex} \text{} \\
  % skica
  $P = \begin{bmatrix}
    0 & \frac{1}{2} & \dots & \frac{1}{2} \\
    \frac{1}{2} & 0 & \dots & 0 \\
    \vdots & & & \vdots \\
    \dots & & \frac{1}{2} & 0
  \end{bmatrix}$ \\
  $\pi = (\frac{1}{n} \dots \frac{1}{n})$ \\
  $h_{i,i} = n$ \\
  $n = h_{i,i} = 1 + \frac{1}{2} h_{i-1,i} + \frac{1}{2} h_{i+1,i}, \quad h_{i-1,i} = h_{i+1,i}$ \\
  $n-1 = h_{i-1,i}$ \\
  $E(\text{steps around}) \leq h_{0,1} + h_{1,2} + \dots + h_{n-1,n} \leq n(n-1)$.
\end{ex}


\section{2-SAT}

Recall: k-SAT:
\begin{align*}
  &F = C_1 \land \dots \land C_m \\
  &C_i = X_{i1} \lor \dots \lor X_{ik}.
\end{align*}
3-SAT: NP complete.
\begin{lstlisting}
  Algorithm:
    def rand2SAT(F):
      $b^0$ = $(b_0^0 \dots b_n^0)$
      for $i$ in range($t$):
        if $F(b^{i}) = 1$:
          return True
        $C_l$ <- clause that is False
        $x_j$ <- uniformly at random from $x_{l1}$ and $x_{l2}$
        $b^{i+1} = (b_0^{i} \dots \overline{b_j^{i}} \dots b_n^{i})$
      if $F(b^{t}) = 1$:
        return True
      return False
\end{lstlisting}
\begin{theorem} \text{} \\
  If $k = 8n^2$, then $P(\text{rand2SAT = True} \mid \text{correct answer is True}) \geq \frac{3}{4}$.
\end{theorem}
\begin{pro} \text{} \\
  Let $a = (a_1 \dots a_n)$ be a correct solution. \\
  Let $X_i =$ Hamming distance from $b^i$ to $a$. \\
  % skica(i)
  Goal: bound $h_{n,0}$. \\
  $P(\text{distance of } b^{i+1} \text{ to $a$ is } j-1 \mid \text{distance of $b^i$ to $a$ is } j) \geq \frac{1}{2}$. \\
  $P = \begin{bmatrix}
    0 & 1 & \dots & 0 \\
    \frac{1}{2} & 0 & \dots & 0 \\
    \vdots & & & \vdots \\
    \dots & & 1 & 0
  \end{bmatrix}$ \\
  $\pi \stackrel{?}{=} \pi P$ \\
  $\pi = (\frac{1}{2n}, \frac{1}{n} \dots \frac{1}{n}, \frac{1}{2n})$ \\
  By theorem \\
  $h_{i,i} = \frac{1}{\pi_i} = n$ for $i = 1, 2 \dots n-1$ \\
  $h_{0,0} = h_{n,n} = 2n$ \\
  $n = h_{i,i} = 1 + \frac{1}{2} h_{i+1,i} + \frac{1}{2} h_{i-1,i}$ \\
  $h_{i+1,i} \leq 2n$ \\
  $i = 0: \; 2n = h_{0,0} = 1 + h_{1,0} \implies h_{1,0} < 2n$ \\
  $h_{n,0} \leq h_{n,n-1} + \dots + h_{1,0} \leq 2n^2$ \\
  $E(\text{steps in algorithm to reach correct solution}) = E(Z) \leq 2n^2$ \\
  $P(\text{algorithm hasn't reached correct solution after $8n^2$ steps}) \\
  = P(Z > 8n^2) \stackrel{\text{Markov}}{\leq} \frac{E(Z)}{8n^2} \leq \frac{1}{4}$.
  \qed
\end{pro}


\section{Generating a uniformly random element of a set}

$\Omega$: set. \\
Let $G$ be a symmetric graph on $\Omega$. \\
We form M.C: \\
$P_{x,y} = \begin{cases}
  \frac{1}{M}: &\text{ if } x \neq y \land x \sim y \\
  0: &\text{ if } x \neq y \land x \nsim y \\
  1 - \frac{|N(x)|}{M}: &\text{ if } x = y
\end{cases}$ \\
$M \geq \max_{v \in \Omega} |N(v)|$. \\
If $G$ is connected $\implies$ M.C. is irrecudible. \\
$\pi = (\frac{1}{|\Omega|} \dots \frac{1}{|\Omega|})$ \\
$\pi \stackrel{?}{=} \pi P$ \\
\begin{align*}
  (\pi P)_x &= \sum_y \pi_y P_{y,x} \\
  &= \sum_{y \in N(x)} \frac{1}{M} \cdot \frac{1}{|\Omega|} + \frac{1}{|\Omega|}
    \left(1 - \frac{|N(x)|}{M}\right) = \frac{1}{|\Omega|} = \pi_x.
\end{align*}
$\implies$ if we walk on the Markov chain long enough, we end up in state $x$ with probability $\pi_x = \frac{1}{|\Omega|}$ \\
$\implies$ we can sample uniformly.
\begin{ex} \text{} \\
  $G$ graph, finding largest independent set $(\forall u,v: u \nsim v)$ is NP-complete. \\
  Lets try sampling a uniformly random independent set \\
  $\Omega = \{\text{independent sets}\}$ \\
  $u \sim v$ if $|u \triangle v| = 1$ $((u \cup \{el\}) = v)$ \\
  M.C.: $X_0$ = arbitrary independent set \\
  $X_{i+1}$:
  \begin{itemize}
    \item pick uniformly at random $v \in V(G)$,
    \item if $v \in U$ then $X_{i+1} = U \textbackslash \{v\}$,
    \item if $U \cup \{v\}$ is independent then $X_{i+1} = U \cup \{v\}$,
    \item else $X_{i+1} = U$.
  \end{itemize}
  $M$ is number of vertices \\
  $\implies \; \forall u \in \Omega: \lim_{t \to \infty} P(X_t = u) = \frac{1}{|\Omega|}$. \\
  Note: irredudicle; $U \to \emptyset \to V$, aperiodic.
\end{ex}


\section{Metropolis algorithm}

$\Omega$: set, \\
$\pi$: chosen distribution on $\Omega$. \\
Make $G$ graph on $\Omega$ \\
$P_{x,y} = \begin{cases}
  \frac{1}{M} \cdot \min \left(1, \frac{\pi_y}{\pi_x}\right): &\text{ if } x \neq y \land x \sim y \\
  0: &\text{ if } x \neq y \land x \nsim y \\
  1 - \sum_{y \in N(x)}: &\text{ if } x = y
\end{cases}$ \\
$M \geq \max_{v \in \Omega} |N(v)|$ \\
$\pi \stackrel{?}{=} \pi P$
\begin{align*}
  (\pi P)_x &= \sum_y \pi_y P_{y,x} =
    \sum_{y \in N(x)} \pi_y \frac{1}{M} \min \left(\left(1, \frac{\pi_y}{\pi_x}\right)\right)
    + \pi_x \left(1 - \sum_{y \in N(x)} \frac{1}{M} \min \left(1, \frac{\pi_y}{\pi_x}\right)\right) \\
  &= \sum_{y \in N(x), \pi_y \geq \pi_x} \pi_y \frac{1}{M} \cdot 1 +
    \sum_{y \in N(x), \pi_y < \pi_x} \pi_y \frac{1}{M} \frac{\pi_y}{\pi_x} + \pi_x \\
  &- \sum_{y \in N(x), \pi_y \geq \pi_x} \pi_x \frac{1}{M} \frac{\pi_y}{\pi_x} -
    \sum_{y \in N(x), \pi_y < \pi_x} \frac{1}{M} \cdot 1 \\
  &= \pi_x.
\end{align*}
\begin{ex} \text{} \\
  $\Omega = \Z \cap [-1000,1000]$ \\
  $\pi \sim e^{-\frac{(x-\mu)^2}{2 \delta}}$ \\
  % skica
  \begin{lstlisting}
    $X_0$ arbitrary
    for $i$ = in range(1,$m$):
      $y$ <- uniformly from $\{X_i-11,X_i+1\}$
      $M$ <- uniformly from $[0,1]$
      if $M \leq \frac{\pi(y)}{\pi(x)}$:
        $X_{i+1} = y$
      else:
        $X_{i+1} = X_{i}$
    return $X_{m}$
  \end{lstlisting}
\end{ex}
\begin{ex} \text{} \\
  Find maximum of a positive function $f$. \\
  Use metropolis algorithm to sample proportional to $f$. \\
  Note: all I need to know is ratios $\frac{f(y)}{f(x)}$.
\end{ex}


% 8. predavanje: 24.11.

Back to independent sets. \\
$G = (V, E)$ \\
$\Omega$ = independent sets. \\
$\lambda \in (1, \infty)$ \\
$\pi(u) \sim \lambda^{|u|}$ \\
$\pi(u) = \frac{\lambda^{|u|}}{\sum_{v \text{ independent set}} \lambda^{|v|}}$. \\
How to calculate the sum? \\
No problem: only need proportions. \\
$X_0$: arbitrary independent set. \\
$X_i \to X_{i+1}$:
\begin{itemize}
  \item we pick $v \in V$ uniformly at random,
  \item if $v \in X_i \implies$
    \begin{itemize}
      \item $X_{i+1} = X_i \setminus \{v\}$ qith probability $\frac{1}{\lambda} = \min \{1, \frac{\pi_y}{\pi_x}\}$,
      \item $X_{i+1} = X_i$ with probability $1 - \frac{1}{\lambda}$,
    \end{itemize}
  \item if $v \in X_j$ and $X_i \cup \{v\}$ is independent $\implies \; X_{i+1} = X_i \cup \{v\}$,
  \item otherwise $X_{i+1} = X_i$.
\end{itemize}
\begin{ex} \text{} \\
  Bayes: $P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B \mid A) P(A)}{P(B)}$. \\
  $B \leftarrow$ machine is giving values, e.g. $y_1 = 0.05, y_2 = -0.1, y_3 = 0.07, y_4 = 3$. \\
  We believe $B \sim N(\mu, 0,05)$. \\
  $\mu = laplacian(0, 0.01)$. \\
  % skica
  $P(\mu \mid B) = \frac{e^{\frac{|\mu|}{0.01}} e^{-\sum \frac{(x_i-\mu)^2}{0.05}}}{\int \dots}$. \\
  Integral is difficult to calculate. \\
  Sample $\mu$ with Metropolis algorithm.
  % skica
\end{ex}


\section{M.C. for 1-factor in bipartite graphs}

$G$ regular graph \\
% skica
$|A| = |B|$. \\
How to find $1$-factor? \\
Augmenting paths. \\
Let $M$ be (suboptimal) matching. \\
% skica
If we find $s-t$ path, we switch edges and get bigger matching. \\
Starting point. \\
$G \; d$-regular graph. \\
Graph $G = (A \cup B, E)$, $M$ suboptimal matching.
\begin{itemize}
  \item Add $s$ and add directed edges to vertices in $A$ that are not matched with weight $d$,
  \item add $t$ and add directed edges to vertices in $B$ that are not matched with weight $d$,
  \item orient edges in $M$ from $B$ to $A$ that weight $d-1$,
  \item orient edges in $E \setminus M$ from $B$ to $A$ that weight $1$,
  \item we add edge from $t$ to $s$ that weight $(|A| - |M|) d$.
\end{itemize}
% skica
Observation:
\begin{itemize}
  \item for each vertex $x: \; deg^{-}(x) = deg^{+}(x)$ (out weights = in weights),
  \item if $|A| > |M|$, then graph is eulerian $\implies$ there is an augmenting path.
\end{itemize}
How to find $s-t$ path? \\
Do a random walk. \\
Expected time to get from $s$ to $t$ is $h_{s,t}$ \\
$\frac{1}{\pi(s)} = h_{s,s} = h_{s,t} + 1$.
\begin{lemma} \text{} \\
  Let $X$ be a M.C. defined as a random walk on directed (weighted) graph with $deg^{-}(x) = deg^{+}(x)$ for each $x$.
  Then the stationary distribution is $\pi = \left[\frac{deg^{+}(x_i)}{|E|}\right]_{i=1}^n$.
\end{lemma}
$w_{ij}$: weight from $i$ to $j$.
\begin{pro} \text{} \\
  $\pi P = \pi \left[\frac{w_{ij}}{deg^{+}(x_i)}\right]_{i,j=1}^n = \left[\frac{\sum_j w_ji}{|E|}\right]_{i=1}^n
  = \left[\frac{deg^{-}(x_i)}{|E|}\right]_{i=1}^n = \left[\frac{deg^{+}(x_i)}{|E|}\right]_{i=1}^n$.
  \qed
\end{pro}
$h_{s,s} = \frac{1}{\pi_s} \leq \frac{|E|}{deg^{+}(s)} 
\leq \frac{3(|A| - |M|) d + |M| (d-1) + (|A| - |M|) d + |M| (d-1)}{(|A| - |M|) d} \leq \frac{4 |A|}{|A| - |M|}$. \\
Expected time to find augmenting path $\leq \frac{4 |A|}{|A| - |M|}$. \\
$|A| = n$ \\
Expected time to find 1-factor $\leq \sum_{i=1}^{n-1} \frac{4n}{n-i} = 4n \sum_{i=1}^{n-1} \frac{1}{i} \leq 4n (1 + \ln n)$ -
in $O(n \log n)$.

\subsection*{Network centrality}

% skica
Degree as measure - natural idea. \\
Use M.C: walk randomly on the network, those that are visited more oftenly are more important. \\
Pagerank. \\
Let $A$ be the adjacency matrix of $G$. \\
$P_{ij} = \alpha \frac{A_{ij}}{deg i} + (1-\alpha) \frac{1}{n}$; \\
$\alpha$: normal random walk, \\
$1-\alpha$: jump to any. \\
$\alpha = 0.85$.



\chapter{Randomized incremented constructions (RIC)}


Observation: \\
Let $S$ be a set of $n$ distinct elements. \\
Let $X_1 \dots X_n$ be a random permutation of the elements. \\
Let $S_i = \{X_1 \dots X_i\}$. \\
$P(X_i = \min (S_i)) = \frac{1}{i}$. \\
$Y = \left| \{j \in \{1 \dots n\} \mid j = \text{ minimal of } S_j\} \right|$ \\
$Y = Y_1 + \dots + Y_n$ \\
$Y_j = \begin{cases}
  1: \text{ if } i = \min S_i \\
  0: \text{ otherwise}
\end{cases}$ \\
$E(Y) = \sum_{i=1}^{n} E(Y_i) = \sum_{i=1}^{n} \frac{1}{i}$ in $O(\log n)$.
\begin{lstlisting}
  Alg():
    $X_1 \dots X_n$ = random permutation of $S$
    $min = X1$
    for $i$ in range(1,$n+1$):
      if $Xi < min$:
        print(\sn{HA})
        $min = Xi$
\end{lstlisting}
We get $O(\log n)$ \sn{HA} printed. \\
Incremental construction (IC). \\
Input $S = \{s_1 \dots s_n\}$. \\
We will build structures $DS(S_i)$: \\
$DS(S_1 \to \dots \to DS(S_n))$. \\
$DS(S_n)$ will help us give answer. \\
Randomized: permute $S$ at the beginning.


\section{Quicksort as RIC}

$S$: set of elements we want to order. \\
$X_1 \dots X_n$: random permutation of $S$. \\
$S_i = \{X_1 \dots X_i\}$. \\
$S_i$ splits $\R$. \\
% skica
Define $DS(S_i)$:
\begin{itemize}
  \item save intervals: each interval will be saved by endpoints,
  \item for each interval we will be saving its points,
  \item for each $X_j$, $j > i$ we will save in which interval it is,
  \item for each left point of the interval we will save the right point.
\end{itemize}
\begin{lstlisting}
  QuicksortRIC(S):
    # start of DS(Si)
    $I=[(-\infty,\infty)]$
    $P[(-\infty,\infty)] = S$
    for each $X_i$:
      $Int(X_i) = (-\infty,\infty)$
    $Next(-\infty) = \infty$
    # end of DS(Si)
    for $i$ in range(1,$n+1$):
      $I_i = Int(X_i) = (X_j,X_k)$  # Ii splits interval (X_j,X_k)
      $I_{i1} = (X_j,X_i)$
      $I_{i2} = (X_i,X_k)$
      for $Xl \neq X_i, X_l \in P(I)$: 
        add $X_l$ to $P(I_{i1})$ or $P(I_{i2})$ depending on $X_l < X_i$ or $X_l > X_i$
      $Next(X_j) = X_i$
      $Next(X_i) = X_k$
    return $[Next(-\infty), Next(Next(-\infty)) \dots]$
\end{lstlisting}
Similarity to quicksort: spliting intervals.


% 9. predavanje: 1.12.

% skica
Analysis: \\
for set $i$, we need $O\left(|P(I_i)|\right)$, \\
$E\left(|P(I_i)|\right) = ?$ \\
e.g. \\
if $x_4 = a_4$: \\
% skica
if $x_4 = a_2$: \\
% skica
$P(X_i = a_j) = \frac{1}{i}, \; j \in \{1, 2 \dots i\}$. \\
Expected value of steps in iteration $i$ \\
$\sum_{j=1}^{i} \frac{1}{i} \left(P\left((a_{j-1},a_j)\right) + P\left((a_j,a_{j+1})\right)\right)
\leq \frac{1}{i} 2 (n-i) \leq \frac{2n}{i}$ \\
\begin{align*}
  E \left(\text{number of steps in QuicksortRIC}\right)
  &\leq \sum_{i=1}^{n} \frac{2n}{i} \\
  &\leq 2n (1 + \log n) \quad \to \text{ in } O(n \log n).
\end{align*}

\section{Linear programming}

Task: maximize $f(x_1 \dots x_n) = c_1 x_1 + \dots + c_d x_d$. \\
Constraints:
\begin{align*}
  &a_{11} x_1 + \dots + a_{1d} x_d \leq b_1 \\
  &\vdots \\
  &a_{n1} x_1 + \dots + a_{nd} x_d \leq b_n. \\
\end{align*}
Geometric interpretation. \\
% skica
Cases:
\begin{itemize}
  \item infeasible region
    % skica
  \item unbounded
    % skica
  \item multiple solutions.
    % skica
\end{itemize}
Alg:
\begin{itemize}
  \item symplex algorithm worst case $O\left(2^n\right),$
  \item interior point method (polynomial algorithm).
\end{itemize}
Seidel's algorithm: \\
running in expected $O(n)$ time when $d$ is constant. \\
One dimension.
\begin{align*}
  \max \; &c x \\
  &a_1 x \leq b_1 \\
  &\vdots \\
  &a_n x \leq b_n,
\end{align*}
where $n$ is number of constraints. \\
% skica
\begin{itemize}
  \item $a_i$ positive: $(-\infty, \frac{b_i}{a_i}] \quad \left(x_i \leq \frac{b_i}{a_i}\right)$,
  \item $a_i$ negative: $[\frac{b_i}{a_i}, \infty) \quad \left(x_i \geq \frac{b_i}{a_i}\right)$.
\end{itemize}
$a_i \neq 0$. \\
Alg:
\begin{itemize}[label={}]
  \item $R = \min_i \{\frac{b_i}{a_i}; a_i > 0\}$,
  \item $L = \max_i \{\frac{b_i}{a_i}; a_i < 0\}$,
  \item if $L > R$: program infeasible,
  \item else:
    \begin{itemize}[label={}]
      \item if $c > 0$: return $R$,
      \item if $c < 0$: return $L$.
    \end{itemize}
\end{itemize}
2-dim: assume general position.
\begin{align*}
  \max \; &c_1 x + c_2 y \\
  &a_{11} x + a_{12} y \leq b_1 \\
  &\vdots \\
  &a_{n1} x + a_{n2} y \leq b_n \\
  &x \leq M \text{ or } x \geq -M \\
  &y \leq M \text{ or } y \geq -M.
\end{align*}
$\leq, \geq$ depending on $c_1, c_2$. \\
% skica
Notation:
\begin{itemize}[label={}]
  \item $h_i$: halfspace defined by $a_{i1} x + a_{i2} y \leq b_i$,
  \item $m_i$: added halfspaces, defined by $X, Y \leq M$ or $\geq -M$,
  \item $l_i$: line that bounds.
\end{itemize}
Alg:
\begin{itemize}
  \item first randomly permute $h_i$,
  \item $H_i = \{m_1, m_2, h_1 \dots h_i\}$,
  \item $v_i \in \cap H_i$ optimal solution after $i$ constraints,
  \item $v_0 = (\pm M, \pm M)$,
  \item inductively add $h_i$.
    % skica
\end{itemize}
Cases:
\begin{itemize}[label={}]
  \item if $v_{i-1} \in h_i \; \implies \; v_i = v_{i-1}$,
  \item if $v_{i-1} \notin h_i \; \implies \; v_i \in h_i$:
    \begin{itemize}[label={}]
      \item $a_{i1} x + a_{i2} y = b_i$
      \item $a_{i1}$ or $a_{i2} \neq 0$, e.g. $a_{i1}$;
      \item $x = \frac{b_i - a_{i2} y}{a_{i1}}$.
    \end{itemize}
\end{itemize}
Insert $x$ in all constraints $\implies$ linear program in 1-dim, i (i-1?) constraints
$\implies$ get $v_i$ in $O(i)$. \\
Analysis:
\begin{itemize}
  \item worst case: $\sum_{i=1}^{n} O(i) = O(n^2)$,
  \item expected: $E(X) = \sum_{i=1}^{n} E(X_i)$,
  \item $X_i =$ running time of $i$-th iteration,
  \item $X_i = \begin{cases}O(1): &\text{ case 1} \\ O(i): &\text{ case 2}\end{cases}$
  \item $P(\text{case 2}) \leq \frac{2}{i}$ - optimal point on at most $2$ lines,
  \item $E(X) \leq \sum_{i=1}^{n} O(1) \cdot 1 + O(i) \cdot \frac{2}{i} = O(n)$.
\end{itemize}
$d$-dim
\begin{itemize}
  \item constraints define half-spaces,
  \item boundary is hyperplane ($d-1$ dimensional),
  \item general position: intersection of $d-i$ hyperplanes is $i$ dimensional,
    intersection of $d+1$ hyperplanes is $\emptyset$.
\end{itemize}
Alg:
\begin{itemize}[label={}]
  \item first add $X_i \leq M$ or $X_i \geq -M$ depemding on $c_i$,
  \item random permutation ($h_1 \dots h_n$),
  \item $H_i = \{m_1 \dots m_d, h_1 \dots h_i\}$,
  \item $v_0 \in \cap \partial m_i$,
  \item inductively add $h_i$:
    \begin{itemize}[label={}]
      \item $v_{i-1} \in h_i \; \implies \; v_i = v_{i-1}$,
      \item $v_{i-1} \notin h_i \; \implies$
        we need to solve LP in $d-1$ dimensions with $i$ constraints ($O(i)$ expected),
      \item $P(v_{i-1} \notin h_i) \leq \frac{d}{i}$,
    \end{itemize}
  \item $E(X) \leq \sum_{i=1}^{n} O(1) + \frac{d}{i} O(i) = O(n)$.
\end{itemize}
$X$: running time. \\
Careful implementation runs in $O(d! \, n) \; \implies$ very useful for low dimensions. \\
Problem: let $P$ be convex polygon given by ordered set of vertices \\
% skica
$y = a_i x + b_i$. \\
Find largest disc embeddable in $P$. \\
Input: $P_1 \dots P_n$, \\
output: $(s_1, s_2), r$.
\begin{align*}
  \max \; &r \\
  &r = \left| \frac{s_2 - a_i s_1 - b_i}{\sqrt{a_i^2 + 1}}\right| \\
  &\frac{s_2 - a_i s_1 - b_i}{\sqrt{a_i^2 + 1}} \geq r \text{ - line above } P \\
  &-\frac{s_2 - a_i s_1 - b_i}{\sqrt{a_i^2 + 1}} \leq -r \text{ - line below } P
\end{align*}
$\implies$ LP in 3 dim. \\
Note: $\frac{s_2 - a_i s_1 - b_i}{\sqrt{a_i^2 + 1}}$ positive if $(s_1, s_2)$ above the line,
negative otherwise.



\chapter{Hashing}


A hash function is a randon function, \\
$h: U \to \{0, 1 \dots n-1\} = M$, \\
$U$ - universe, \\
$u = |U|$, \\
$m = |M|$. \\
Ideally we would like for $h$ to be as completely random: $P(h(x) = t) = \frac{1}{m}$. \\
Standard application. \\
Let $V \subset U, \; |V| << |U|$. \\
We would like to quickly answer if $x \in V$ for every $x \in U$. \\
Solution:
\begin{itemize}
  \item take $h: U \to M$,
  \item make a table $T = [0, 1 \dots n-1]$,
  \item for $v \in V$:
    \begin{itemize}[label={}]
      \item $T[h(v)] = 1$,
      \item $T[y] = 0 \; \forall y \in h(V)$.
    \end{itemize}
  \item Let $x \in V$. Check
    \begin{itemize}
      \item if $T[h(x)] = 1: \; x \in V$,
      \item else: $x \notin V$.
    \end{itemize}
\end{itemize}
Note: this is not OK: $h$ not injective.


% 10. predavanje: 8.12.

For $x \in U$, tell if $x \in V$ in $O(1)$. \\
$h = \text{SHA256}: U \to \{0, 1\}^{256}$. \\
Approach:
\begin{itemize}
  \item design a family of hash functions,
  \item study collisions $P_h(h(x) = h(y))$,
  \item $H$ meeds to be \sn{simple}.
\end{itemize}
Bad example: $H$ = all functions from $U$ to $M$ storing $h \in H$ would take $|U| \log_2 |M|$ bits.
\begin{defn}
  A family of hash functions to be universal if for \\
  $\forall x, y \in U, x \neq y, h \in H: \; P(h(x) = h(y)) \leq \frac{1}{m}$ (probability of collision).
\end{defn}
k-independent if $\forall x_1 \dots x_k \in U$ pairwise different, $\forall t_1 \dots t_k \in M \\
P_r(h(x_i) = t_i \; \forall i) \leq \frac{1}{m^k}$.
\begin{ex} \text{} \\
  $U = \{0, 1, 2, 3\}$, \\
  $M = \{0, 1\}$, \\
  $H = \{h_0, h_1, h_2\}$, \\
  $h_0: \{0 \to 0, 1 \to 0, 2 \to 1, 3 \to 1\}$, \\
  $h_1: \{0 \to 0, 1 \to 1, 2 \to 0, 3 \to 1\}$, \\
  $h_2: \{0 \to 0, 1 \to 1, 2 \to 1, 3 \to 0\}$. \\
  $P(h(0) = h(2)) = \frac{2}{3} > \frac{1}{2}$ - not universal.
\end{ex}
Why universal? \\
$U, V, M$ \\
$H$ universal: $\forall x, y: \; P(h(x) = h(y)) \leq \frac{1}{m}$. \\
$X$: number of collisions of $V$. \\
$E(X) = E\left(\sum_{x,y \in V, x \neq y} X_{x,y}\right)$ \\
$X_{x,y} = \begin{cases}
  1: \text{ if } h(x) = h(y) \\
  0: \text{ else}
\end{cases}$ \\
$E(X) = \sum_{x,y \in V, x \neq y} E(X_{x,y}) \leq \binom{n}{2} \cdot \frac{1}{m}$. \\
$U, V, M, H$ \\
$T[0 \dots m-1]$ \\
$\forall v \in V$ \\
$T[h(v)] = v$. \\
For $x \in V$ we check $T[h(x)]$ if equals $x$, \\
for $y \in U \setminus V$, $T[h(y)] \neq y$. \\
For $z \in V$, $T[h(z)]$ can happen $\neq z$ if $h$ has collisions in $V$.
\begin{lemma} \text{} \\
  Let $m \geq n^2$ and $H$ universal.
  Then the probability that $h$ has no collisions in $V \; \geq \frac{1}{2}$.
\end{lemma}
\begin{pro} \text{} \\
  $X$: number of collisions \\
  $E(X) \leq \binom{n}{2} \cdot \frac{1}{m} < \frac{n^2}{2} \cdot \frac{1}{n^2} = \frac{1}{2}$ \\
  $P(X \geq 1) \stackrel{\text{Markov}}{\leq} \frac{E(X)}{1} = \frac{1}{2}$ \\
  $P(X = 0) \geq \frac{1}{2}$.
  \qed
\end{pro}
\begin{ex}[Universal hash family] \text{} \\
  $U = \{0, 1 \dots u-1\}$ (bits $\equiv$ numbers) \\
  $M = \{0, 1 \dots m-1\}$. \\
  Define: let $p \geq u$, $p$ prime number. \\
  Define for $a, b \in \Z_p, \; a \neq 0$. \\
  $h_{a,b} = (ax + b) \mod m$ \\
  $ax + b \in \Z_p$ \\
  $H = \{h_{a,b} \mid a, b \in \Z_p, \; a \neq 0\}$.
\end{ex}
\begin{pro} \text{} \\
  $P(h_{a,b}(x) = h_{a,b}(y)) = ?$ \\
  $x, y$ fixed. \\
  For any $a, b$ denote \\
  $ax + b = t_x \\
  ay + b = t_y: \\
  a \sqcup + b \in \Z_p$. \\
  $\begin{bmatrix}x & 1 \\ y & 1\end{bmatrix} \begin{bmatrix}a \\b\end{bmatrix} =
  \begin{bmatrix}t_x \\ t_y\end{bmatrix}$ \\
  $\det \begin{bmatrix}x & 1 \\ y & 1\end{bmatrix} \neq 0$, because $x \neq y$ \\
  $\begin{bmatrix}a \\b\end{bmatrix} = \begin{bmatrix}x & 1 \\ y & 1\end{bmatrix}^{-1}
  \begin{bmatrix}t_x \\ t_y\end{bmatrix}$. \\
  For each $t_x, t_y$ there exists 1 $a, b$ mapping to $t_x, t_y$. \\
  $h_{a,b}(x) = h_{a,b}(y) \; \iff \; t_x = t_y \mod m$. \\
  This holds for $p\left(\lceil\frac{p}{m}\rceil + 1\right)$ \\
  $p$: choice of $t_y$ \\
  $t_x = t_y + km$ \\
  $P\left(h_{a,b}(x) = h_{a,b}(y)\right) \leq \frac{p\left(\lceil\frac{p}{m}\rceil-1\right)}{p(p-1)}
  \leq \frac{\frac{p-1}{m}}{p-1} = \frac{1}{m}$.
  \qed
\end{pro}
Function random for $2$ elements, fixed for $\geq 3$. \\
Higher k-independent: better.

\section{Chaining}

$V, U, h: U \to V$. \\
Answer $x \in V$ in $O(1)$. \\
$T[0 \dots m-1]$ \\
% skica
$n = |V|$ \\
$\forall v \in V$: \\
$h(v_1) = h(v_2) \; \to \; [v_1 \; v_2 \dots]$ - linked list. \\
Now: \\
$x \in U$. \\
Check if $x$ is in list at $T[h(x)]$. \\
Check takes $O(\text{length of a list at }h(x))$ = $1 +$ number of collisions with $x$. \\
$X_x$: number of collisions with $x$. \\
$E(X_x) = \sum_{y \in V} E(X_{x,y}) \leq n \cdot \frac{1}{m}$ if hash function is universal. \\
$\alpha = \frac{n}{m}$: load factory (how many elements in $1$ place). \\
$E(X_x) = 1$ \\
$E(\max_x X_x) \neq \max_x E(X_x) = 1$.
\begin{theorem}
  Assume we throw $n$ balls into $n$ bins uniformly at random.
  Then with high probability the fullest contains
  $\theta\left(\frac{\log n}{\log (\log n)}\right)$ balls.
\end{theorem}
\begin{pro} \text{} \\
  $\stackrel{?}{\leq} \frac{3 \ln n}{\ln \ln n}$. \\
  Let $X_j$ be the number of balls in bin $j$. \\
  $P\left(X_j \geq \frac{3 \ln n}{\ln \ln n}\right) = P($there exists subset $S$ of balls thrown to bin $j)$. \\
  $|S| = k$
  \begin{align*}
    &P\left(\cup_{S \text{ balls}, |S|=k} \text{ balls from $S$ are thrown to bin }j\right) \\
    &\leq \sum_{S \text{ balls}, |S|=k} P(\text{balls from $S$ are thrown to }j) \\
    &= \binom{n}{k} \left(\frac{1}{n}\right)^k \\
    &\leq \frac{n^k}{k!} \cdot \frac{1}{n^k} \\
    &= \frac{1}{k!}
    &\stackrel{\refeq{eq:exp-fac}}{\leq} \frac{e^k}{k^k} \\
    &= \left(\frac{e \ln n}{3 \ln \ln n}\right)^{\frac{3 \ln n}{\ln \ln n}} \\
    &\leq e^{\frac{3 \ln n}{\ln \ln n} \cdot \left(\ln \ln \ln n - \ln \ln n\right)} \\
    &= e^{-3 \ln n + \frac{\ln \ln \ln n \cdot (\ln n \cdot 3)}{\ln \ln n}}
    &\stackrel{\refeq{eq:lnlnln}}{\leq} e^{-3 \ln n + \ln n} \\
    &= \frac{1}{n^2};
  \end{align*}
  \begin{equation}
    \label{eq:exp-fac}
    e^x = \sum_{i=1}^{\infty} \frac{k^i}{i!} \geq \frac{k^k}{k!},
  \end{equation}
  \begin{equation}
    \label{eq:lnlnln}
    \frac{\ln \ln \ln n}{\ln \ln n} \to 0.
  \end{equation}
  $P(\text{at least for $1$ bin }j \geq k) = n \cdot \frac{1}{n^2} = \frac{1}{n}$.
  \qed
\end{pro}


% 11. predavanje: 15.12.

$U, V$, $H$ hash family, $h: U \to M$ \\
$v \in V$ \\
$n = |V|$ \\
max load $O\left(\frac{\log n}{\log (\log n)}\right)$. \\
Perfect hashing: we would like
\begin{itemize}
  \item $O(1)$ lookup (worst case)
  \item $O(n)$ size of table.
\end{itemize}


\section{2 level hashing}

Input: $V$ \\
$n = |V|$. \\
% skica
Take hash function from universal family with $m = |M| = n$. \\
Count total collisions $X$. \\
$E(X) \leq \binom{n}{2} \cdot \frac{1}{m} \leq \frac{n}{2}$ \\
$P(x \geq n) \stackrel{\text{Markov}}{\leq} \frac{1}{2}$ \\
$\implies$ by repeating sample $h$ we can guarantee
\begin{itemize}
  \item for each $i \in M$ we store at $T[i]$ another hash table of size $C_i^2$,
    where $C_i = $ number of elements of $V$, hashed in $i$,
  \item we sample $h_i$ from universal hash family with $M_i = C_i^2$.
\end{itemize}
$P(h_i \text{ has no collisions}) \geq \frac{1}{2}$ (by lemma). \\
We resample if $h_i$ has collisions. \\
$E(\text{sampling } h_i) = 2$. \\
Construction time:
\begin{itemize}
  \item step 1: $O(n)$
  \item step 2: $O(C_1 + \cdots + C_n) = O(n)$;
\end{itemize}
together $O(n)$. \\
Lookup time: $O(1)$ (evaluating $h(x)$ and $h_{h(x)}(x)$). \\
Space: $O(C_1^2 + \dots + C_n^2)$ in $O(n)$. \\
By first step: \\
$n >$ number of collisions of $h = \sum_{i=1}^{n} \binom{C_i}{2} = \sum_{i=1}^{n} \frac{C_i^2 - C_i}{2}$ \\
$\implies \sum_{i=1}^{n} C_i^2 < 2n + \sum_{i=1}^{n} C_i = 3n$.


\section{The power of 2 choices}

Variant: placing $n$ balls in $n$ bins but for each ball we choose $d$ bins uniformly at random and
put the ball in bin with minimal load.
\begin{theorem}
  The above process with $d \geq 2$ results in at most maximum load of $O\left(\frac{\ln (\ln n)}{\ln d}\right)$.
\end{theorem}
\begin{pro}(sketch). \\
  $b_i$ = upper bound of the number of bins with load at most $i$. \\
  Height of a ball = the number of balls in the bin, where the ball is placed. \\
  $P(\text{a ball has height at least } i+1) \leq \left(\frac{b_i}{n}\right)^d$ (choose $d$ times independently). \\
  $X^{i+1}$: number of balls with height $ \geq i+1$. \\
  $X^{i+1} = \sum_{j=1}^{n} X_j^{i+1}$ \\
  $X_j^{i+1}$: indicator variable of $j$-th ball having height $i+1$. \\
  $E(X^{i+1}) \leq \sum_{j=1}^{n} \left(\frac{b_i}{n}\right)^d = n \cdot \left(\frac{b_i}{n}\right)^d$. \\
  Chernoff bound: with high probability $X^{i+1} \leq 2 n \left(\frac{b_i}{n}\right)^d$. \\
  $X^{i+1} \geq$ number of bins with load at least $i+1$. \\
  Define (set) \\
  $b_{i+1} = \frac{\sum b_i^d}{n^{d-1}}$ \\
  $b_4 = \frac{n}{4}$ \\
  $b_{i+4} \stackrel{?}{=} \frac{n}{2^{2 \cdot d^i - \sum_{j=0}^{i-1} d^j}}$
  \begin{itemize}[label={}]
    \item $i=0$: $b_4 = \frac{n}{2^{2^1}} = \frac{n}{4}$
    \item $i \to i+1$:
      \begin{align*}
        b_{i+4} &= \frac{2 \cdot b_{i+3}}{n^{d-1}} \\
        &\stackrel{IH}{=} \frac{2 \cdot \left(\frac{n}{2^{2 \cdot d^i - \sum_{j=0}^{i-1} d^j}}\right)^d}{n^{d-1}} \\
        &= \frac{2^1 \cdot n^d}{n^{d-1} \cdot 2^{2 \cdot d^{i+1} - \sum_{j=1}^{i} d^j}} \\
        &= \frac{n}{2^{2 \cdot d^{i+1} - \sum_{j=0}^{i} d^j}}.
      \end{align*}
  \end{itemize}
  In particular: $b_{i+4} \leq \frac{n}{2^{d^i}} < 1$ when?
  \begin{align*}
    n &< 2^{d^i} \\
    \log_2 n &< d^i \\
    \log_d \log_2 n &< i
  \end{align*}
  $\implies$ for $i = \frac{\log (\log_2 n)}{\log d}$ is $b_i < 1$ $\implies$ no bins with load
  $> \frac{\log (\log_2 n)}{\log d}$.
  \qed
\end{pro}
Application: \\
We sample 2 hash functions $h_1, h_2: U \to M$. \\
For element $v \in V$ we insert in $T[h_1(v)]$ or $T[h_2(v)]$ depending on which list is shorter. \\
Max load in $O(\log (\log n))$.


\section{Cockoo hashing}

Idea: use 2 hash functions but allow moving elements later. \\
We want to have at most 1 element at each entry in the table. \\
Inserting: % skica
\begin{itemize}
  \item if empty: insert,
  \item if not empty: push other element to its other choise, repeat recursively.
\end{itemize}
Questions:
\begin{itemize}
  \item how many do I need to move,
  \item how many elements can I insert before problems?
\end{itemize}
We can think of positions in the table as vertices and elements of $V$ as edges. \\
$|V|$ edges are inserted uniformly at random (if ideal hash function)
$\implies$ random graph. \\
Erdös-reny model: $G_{n,m} \approx G_{n,p}$ if $m = \binom{n}{2} \cdot p$ (A.S. properties). \\
If $np < 1 - \varepsilon$: all connected components have size at most $O(\log n)$,
components are trees or at most 1 cycle per component, expected size of a component is $O(1)$. \\
% skica
% skica
Fact: if graph has at most 1 cycle per component, then inserting can be done and takes at most
$2 \cdot $(size of component) time (each edge changes direction at most 2 times).
\begin{theorem} \text{} \\
  Let $n = |U|$, $h_1, h_2: U \to M$, $m = |M| = 2 \cdot (1 + \varepsilon) \cdot n$,
  then with high probability cockoo hashing works correctly with
  \begin{itemize}
    \item inserting time:
      \begin{itemize}
        \item $O(\log n)$ time worst case,
        \item $O(1)$ expected case,
      \end{itemize}
    \item space: $O(n)$,
    \item lookup time: $O(1)$.
  \end{itemize}
\end{theorem}
Dynamically add element: \\
$m = 2 \cdot (1 + \varepsilon) \cdot n$ \\
$p = \frac{m^{'}}{\binom{n^{'}}{2}} = \frac{2 m^{'}}{n^{'} (n^{'}-1)}$ \\
$pn^{'} = \frac{2m^{'}}{(n^{'}-1)} = \frac{2n^{'}}{2 (1+\varepsilon) n^{'}} = \frac{1}{1+\varepsilon} < 1 + \varepsilon^{'}$


% 12. predavanje: 22.12.

\section{Bloom filter}

Take $k$ hash functions $h_1 \dots h_k$ at random, $h_i: U \to M, T[0 \dots m-1]$. \\
$V \subset U$, for every element $v \in V$ set $T[h_i(v)] = 1 \; \forall i \in \{1 \dots k\}$. \\
False positives: $x \notin V$ such that $T[h_i(x)] = 1 \; \forall i \in \{1 \dots k\}$. \\
For each $T[j] \; P(T[j] = 0) = \left(\left(1 - \frac{1}{m}\right)^n\right)^k \approx e^{-\frac{nk}{m}}$; \\
$k$: each hash function, $n$: for each $v$. \\
Now \\
$P\left(T[h_i(x)] = 1 \; \forall i, \forall x \notin V\right) \approx \left(1 - e^{-\frac{nk}{m}}\right)^k = f(k)$
- probability of a false positive. \\
$\left(1 - e^{-\frac{nk}{m}}\right)$: 1 position. \\
Searching for a minimum: \\
$f^{'}(k) = 0$ \\
$\implies k = \ln 2 \cdot \frac{m}{n}$ \\
$f\left(\ln 2 \cdot \frac{m}{n}\right) = \left(\frac{1}{2}\right)^{\ln 2 \frac{m}{n}} \approx 0.6185^{\frac{m}{n}}$ \\
$\implies$ we choose $m$ such that $0.6185^{\frac{m}{n}}$ small (in $O(n)$) \\
$\implies$ calculating $k = \ln 2 \cdot \frac{m}{n}$
\begin{itemize}[label={}]
  \item $\implies$ hashing with space $O(n)$
  \item $\implies$ checking in $O(1)$
  \item $\implies$ probability of error small.
\end{itemize}


\section{Linear probing}

$V \subset U$, $h: U \to M$, $T[0 \dots m-1]$.
\begin{itemize}
  \item Insert $v \in V$: check $T[h(v)], T[h(v)+1], T[h(v)+1] \dots$ until finding empty space,
    then insert it.
    % skica
  \item Check if $x \in V$ by checking $T[h(x)] \stackrel{?}{=} x, T[h(x)+1] \stackrel{?}{=} x \dots$
    until finding $x$ or finding empty.
\end{itemize}
$x \in U$ \\
$X$: number of steps to check if $x \in V$. \\
$E(X) = ?$ \\
% skica
Block of size $2^l$ is bad if it has more than $2^l \cdot \frac{2}{3}$ values. \\
Set $\frac{n}{m} = \frac{1}{3}$. \\
Expected number of elements hashed in block of size $2^l$ is $\frac{1}{3} \cdot 2^l$. \\
\begin{align*}
  E(X) &= \sum_{i=0}^{n} P(X = i) \cdot i \\
  &\leq \sum_{j=0}^{\log_2 n} P(2^{j-1} < X \leq 2^j) \cdot 2^j \\
  &\leq \sum_{j=0}^{\log_2 n} P(\text{block above $h(x)$ of size $2^j$ is bad}) \cdot c \cdot 2^j.
\end{align*}
$c$: not aligned? \\
$P(\text{block of size $2^j$ is bad}) = P(Y > \frac{2}{3} \cdot 2^j) =
P(Y - \frac{1}{3} \cdot 2^j > \frac{1}{3} \cdot 2^j)$; \\
$Y$: number of elements hashed to the block. \\
$E(Y) = \frac{1}{3} \cdot 2^j$ \\
$E(X) \stackrel{\text{Chernoff}}{\leq} e^{-k \cdot 2^j}$; Chernoff: sum of independent indicators. \\
$E(X) < O(1) \cdot \sum_{j=0}^{\log_2 n} 2^j \cdot e^{-k \cdot 2^j}$ in $O(1)$ \\
$\implies$ checking in $O(1)$. \\
Chernoff: if ideal hash function; 5 independent is enough.


\chapter{Data streams}


Stream of values \\
$\sigma = a_1, a_2 \dots a_n$ \\
$a_i$: tokens \\
$a_i \in [n]$ \\
$m$: length of stream (very large).
\begin{defn}
  $f_i = \left| \{j \mid a_j = i\} \right|$
\end{defn}
We could be interested in
\begin{itemize}
  \item number of different token,
  \item frequency of some token,
  \item frequent tokens: $\{i \in [n] \mid f_i \geq \frac{m}{10}\}$
  \item moments: $\lVert f \rVert^2 = \sum_{i \in [n]} f_i^2$
  \item $\vdots$
\end{itemize}
We want to use memory in $O(poly(\log n, \log m)) << O(n, m)$. \\
Most problems cannot be solved precisely, hence we search for $(\varepsilon, \delta)$-approximation. \\
Algorithm $A(G)$:
\begin{itemize}
  \item initialitazion,
  \item incremental steps,
  \item finalization
\end{itemize}
using randomness (oblivious stream - it doesn't know which randomly,
e.g. we can choose stream that \sn{attacks algorithm}).


\section{Count min sketch}

For a given $i \in [n]$ (token) at the end of stream give $f_i$.
\begin{lstlisting}
  $A(\sigma, \varepsilon, \delta)$:
    Init: $k = \lceil \frac{2}{\varepsilon} \rceil, t = \lceil \log_2 \left(\frac{1}{\delta}\right) \rceil$.
    We choose $t$ hash functions $h_1 \dots h_t: [n] \to M = [k] = \{1 \dots k\}$
    from a universal family $H$.
    Let $C[0 \dots t-1][0 \dots k-1]$ be 2-dim (hash) table
    $C[i][j] = 0 \; \forall i,j$.
    Updates:
        for every token $a_i \in \sigma$ we update $C$
          for $j = 0, 1 \dots t-1$:
            $C[i][h_i(a_j)] += 1$
    Output: we asked $a \in [n]$ return $\overline{f_a} = \min_{0 \leq j \leq t-1} C[j][h_j(a)]$;
      min collisions.
\end{lstlisting}
\begin{theorem} \text{} \\
  For every $a \in [n]$ it holds \\
  $f_a \leq \overline{f_a} \leq f_a + \varepsilon m$ \\
  with probability at least $1 - \delta$.
\end{theorem}
Notice: space needed $O(t \cdot k \cdot \log m) =
O\left(\frac{2}{\varepsilon} \cdot \log_2 \left(\frac{1}{\delta}\right) \log m\right)$.
\begin{pro} \text{} \\
  $\forall i \in [t]: C[i][h_i(a)] \geq f_a \implies \overline{f_a} \geq f_a$. \\
  Fix $a$. \\
  Let $X_i = C[i][h_i(a)] - f_a$ excess of $i$-th count. \\
  $I_{x,y}^i = \begin{cases}
    1: \text{ if } h_i(x) = h_i(y) \\
    0: \text{ else}
  \end{cases}$ \\
  $X_i = \sum_{y \in [n], y \neq a} I_{x,y}^i \cdot f_y$.
  \begin{align*}
    E(X_i) &= \sum_{y \in [n], y \neq a} E(I_{x,y}^i) \cdot f_y \\
    &\stackrel{\refeq{eq:universal}}{\leq} \sum_{y \in [n], y \neq a} \frac{1}{k} \cdot f_y \\
    &\leq \frac{1}{n} \cdot m \\
    &\stackrel{\refeq{eq:count-min-prob}}{\leq} \frac{m}{2};
  \end{align*}
  \begin{equation}
    \label{eq:universal}
    \text{hash function from universal family,}
  \end{equation}
  \begin{equation}
    \label{eq:count-min-prob}
    P(X_i \geq \varepsilon m) \stackrel{\text{Markov}}{\leq} \frac{\varepsilon m}{2 \varepsilon m} = \frac{1}{2}
    \text{ for fixed } i.
  \end{equation}
  \begin{align*}
    P(\overline{f_a} - f_a \geq \varepsilon m) &\leq P(X_i \geq \varepsilon m \; \forall i) \\
    &\stackrel{\text{indep.}}{=} \left(\frac{1}{2}\right)^t \leq \delta.
  \end{align*}
  \qed
\end{pro}


% 13. predavanje: 5.1.

\section{Estimating the number of distinct elements}

We want $d = |\{i \in [n], f(i) > 0\}|$. \\
Define for $x \in \N$: \\
$zeros(x) = \max \{i \mid 2^i \text{ divides } x\}$: number of zeros at the end in binary representation of $x$.
\begin{lstlisting}
  Alg($\sigma$):
    Init:
      $h$: random hash function from 2-independent family.
      $\#$ recall: $[n]$: all possible elements of $\sigma$.
      $h: [n] \to [n]$
      unlog? $n = 2^{n^{'}}$
      $z = 0$
    Update:
      $a_i \in \sigma$
      if $zeros(h(a_i)) \geq z$:
        $z = zeros(h(a_i))$
    Output:
      $\overline{d} = 2^{z + \frac{1}{2}}$
\end{lstlisting}
Define $\forall a \in [n], r \in \N$ \\
$X_{r, a} = \begin{cases}
  1: \text{ if } zeros(h(a)) \geq r \\
  0: \text{ else}
\end{cases}$ \\
$Y_r = \sum_{a \in \sigma} X_{r, a}$. \\
Let $\overline{z}$ be $z$ at the end of the algorithm: $\overline{d} = 2^{\overline{z} + \frac{1}{2}}$. \\
Notice: \\
$Y_r > 0 \iff \overline{z} \geq r$ \\
$Y_r = 0 \iff \overline{z} < r$.
\begin{lemma} \text{} \\
  $P(X_{r, a} = 1) = \frac{1}{2^r}$, \\
  $P(X_{r, a_1} = 1 \land X_{r, a_2} = 1) = \frac{1}{\left(2^r\right)^2}$.
\end{lemma}
\begin{pro} \text{} \\
  $P(X_{r, a} = 1) = P(zeros(h(a)) \geq r) = \frac{2^{n^{'}-r}}{2^{n^{'}}} = \frac{1}{2^r}$; \\
  $2^{n^{'}}$: all, \\
  $2^{n^{'}-r}$: fixed. \\
  $P(X_{r, a_1} = 1 \land X_{r, a_2} = 1) \stackrel{h \text{ 2 indep.}}{=}
  P(X_{r, a_1} = 1) \cdot P(X_{r, a_2} = 1) = \frac{1}{\left(2^r\right)^2}$.
  \qed
\end{pro}
$P(\overline{d} \geq 3d)$ small? \\
$E(Y_r) = \sum_{a \in \sigma} E(X_{a, r}) = \sum_{a \in \sigma} \frac{1}{2^r} = \frac{d}{2^r}$ \\
Let $k \in \N$ be such that $2^{k + \frac{1}{2}} \geq 3d > 2^{k - \frac{1}{2}}$.
\begin{align*}
  P\left(\overline{d} > 3d\right) &\leq P\left(2^{\overline{z} - \frac{1}{2}} > 2^{k - \frac{1}{2}}\right) \\
  &= P\left(\overline{z} + \frac{1}{2} > k - \frac{1}{2}\right) \\
  &= P(\overline{z} \geq k) \\
  &\stackrel{\text{lemma}}{\longeq{25pt}} P(Y_k > 0) \\
  &\stackrel{\in \N}{\longeq{15pt}} P(Y_k \geq 1) \\
  &\stackrel{\text{Markov}}{\leq} \frac{E(Y_k)}{1} = \frac{d}{2^k} \\
  &\stackrel{k}{\leq} \frac{d \cdot 2^{\frac{1}{3}}}{3d} = \frac{\sqrt{2}}{3}.
\end{align*}
$P(\overline{d} \leq \frac{d}{3})$ small? \\
Let $l \in \N$ be such that $2^{l - \frac{1}{2}} \leq \frac{d}{3} < 2^{l + \frac{1}{2}}$.
\begin{align*}
  P\left(\overline{d} < \frac{d}{3}\right) &\leq P\left(2^{\overline{z} + \frac{1}{2}} < 2^{l + \frac{1}{2}}\right) \\
  &= P\left(\overline{z} + \frac{1}{2} < l + \frac{1}{2}\right) \\
  &= P(\overline{z} \leq k) \\
  &\stackrel{\text{lemma}}{\longeq{25pt}} P(Y_l = 0) \\
  &= P\left(Y_l - \frac{d}{2^l} < -\frac{d}{2^l}\right) \\
  &\leq P\left(|Y_l - \frac{d}{2^l}| \geq \frac{d}{2^l}\right) \\
  &\stackrel{\text{Chebisev}}{\leq} \frac{Var(Y_l)}{\left(\frac{d}{2^l}\right)^2} \\
  &\stackrel{l}{\leq} \frac{d \cdot 2^{\frac{1}{3}}}{3d} = \frac{\sqrt{2}}{3};
\end{align*}
\begin{align*}
  Var(Y_l) &= Var\left(\sum_{a \in \sigma} X_{a, l}\right) \\
  &\stackrel{h \text{ 2-indep.}}{\longeq{35pt}} \sum_{a \in \sigma} Var(X_{a, l}) \\
  &= \sum_{a \in \sigma} E(X_{a, l}^2) - E(X_{a, l})^2 \\
  &\stackrel{E(X_{a,l}) \in \{0,1\}}{\leq} \sum_{a \in \sigma} E(X_{a, l}) \\
  &= \frac{d}{2^l}.
\end{align*}
$P\left(\frac{d}{3} < \overline{d} < 3d\right) \geq 1 - \frac{2 \sqrt{3}}{3}$. \\
We use algorithm $k$-times, getting $\overline{d_1} \dots \overline{d_k}$ (we need independent hash functions). \\
Define: $\overline{d} = median(\overline{d_1} \dots \overline{d_k})$. \\
\begin{align*}
  P(\overline{d} \geq 3d) &=
  P\left(\text{at least} \left\lceil \frac{k}{2} \right\rceil \overline{d}-s \text{ are } \geq 3d\right) \\
  &= P\left(X \geq \frac{k}{2}\right) \leq e^{-ck};
\end{align*}
$c$: some constant, \\
$X = \sum_{i=1}^{k} X_i$, \\
$X_i = \begin{cases}
  1: \text{ if } \overline{d_i} \geq 3d \\
  0: \text{ else}
\end{cases}$ \\
$P\left(\overline{d} \leq \frac{d}{3}\right) = \dots$



\chapter{Interactive proofs}


A protocol between $P$ prover and $V$ verifier for function $f$. \\
% skica
Both share $x$, \\
$r$: randomness used, \\
$P, V$: algorithms, \\
$out(V, x, r, P) = \begin{cases}
  1: \; V \text{ agrees that } f(x) = y \\
  0: \text{ else}
\end{cases}$. \\
Goal: minimal communication, minimal work for $V$. \\
Completeness:
\begin{itemize}
  \item for every $x \in D$ (domain)
  \item $P(out(V, x, r, P) = 1) \geq 1 - \delta_c$ for some $\delta_c \in [0, 1)$.
\end{itemize}
Soundness:
\begin{itemize}
  \item for every $x$ such that $f(x) \neq y$
  \item $P(out(V, x, r, P^{'}) = 1) \leq \delta_s$ for every $P^{'}, \delta_s \in [0, 1)$.
\end{itemize}
Computational soundness:
\begin{itemize}
  \item soundness,
  \item $P^{'}$ computationally bounded.
\end{itemize}
Zero-knowledge:
\begin{itemize}
  \item informally: verifier learns nothing behind the claim.
\end{itemize}
\begin{ex} \text{} \\
  Input: $G$ graph, \\
  $f(G) = \begin{cases}
    1: \text{ if $G$ hamiltonian} \\
    0: \text{ else}
  \end{cases}$ \\
  $G \to P \stackrel{m_1: (v_1 \dots v_n)}{\longrightarrow} V \leftarrow G$, \\
  $V$: verifies that $m_1$ is hamiltonian cycle. \\
  Proof: $O(n)$. \\
  Verifier com. $O(n)$.
\end{ex}
\begin{ex} \text{} \\
  Input: $A, B$ matrices, \\
  $f(A, B) = A \cdot B$, \\
  $(A, B) \to P \stackrel{C}{\to} P \leftarrow (A, B)$. \\
  $P$: compute $C = A \cdot B$, send $C$, \\
  $V$: check $A (B v_i) = C v_i$ for random $v_i$. \\
  Prover: matrix multiplication $O(n^3) \; (O(n^{\log_2(7)}))$. \\
  Verifier: $O(n^2)$. \\
  Proof size: $O(n^3)$ (possible to reduce is $O(\log n)$).
\end{ex}
\begin{ex} \text{} \\
  Input: $(n, y) \in \N^2$, \\
  $f(n, y) = \begin{cases}
    1: \text{ if there exists $x$ such that } y = x^2 (\text{mod } n) \\
    0: \text{ else}
  \end{cases}$; \\
  qoaroatic?? reducibility problem. \\
  $(n, y) \to P \to V \leftarrow (n, y)$. \\
  $P$: sample $r \in \Z_n$, $s = r^2$, send $s$, \\
  $V$: sample $b \in \{0, 1\}$, send $b$, \\
  $P$: if $b = 0$: $m_2 = r$, if $b = 1$: $m_2 = r \cdot x$, send $m_2$, \\
  $V$: accepts if $m_2^2 = s \cdot y^b$. \\
  Completeness:
  \begin{itemize}[label={}]
    \item $m_2^2 \stackrel{?}{=} s \cdot y^b$
    \item if $b = 0:$
    \begin{itemize}[label={}]
      \item $m_2 = r$
      \item $r^2 = m_2^2 = s$ \checkmark
    \end{itemize}
    \item if $b = 1:$
    \begin{itemize}[label={}]
      \item $m_2^2 = s y$ 
      \item $r^2 x^2 = s y$ \checkmark ($r^2 = s, x^2 = y$)
    \end{itemize}
  \end{itemize}
  Soundness:
  \begin{itemize}
    \item 2 options for what prover does.
    \begin{itemize}
      \item Send $s$ such that there is no $r$ that $r^2 = s$. \\
        Then with probability $\frac{1}{2}$ is $b = 0$. \\
        Then prover needs to send $m_2$ such that $m_2^2 = s$ (impossible) \\
        $\implies$ fail with probability at least $\frac{1}{2}$.
      \item Send $s$ such that $r^2 = s$. \\
        Then with probability $\frac{1}{2}$ is $b = 1$. \\
        $m_2^2 = s y = r^2 y \implies y = (m_2 r^{-1})^2 \implies \exists x: x^2 = y$: contradiction \\
        $\implies$ fail with probability at least $\frac{1}{2}$.
    \end{itemize}
  \end{itemize}
  With zero-knowledge.
\end{ex}


\section{Sum-check protocol}

Let $g(x_1 \dots x_n)$ be multivariate polynomial of degree $d$ over $\F$. \\
Let $H_g = \sum_{b_1 \dots b_n \in \{0, 1\}} g(b_1 \dots b_n)$. \\
$P$ wants to convince $V$ that $c = H_g $. \\
$g \to P \to V \leftarrow g$. \\
$P$: sends $c$, \\
$P$: compute $g_1(x) = \sum_{b_2 \dots b_n \in \{0, 1\}} g(x, b_2 \dots b_n)$, send $g_1(x)$, \\
$V$: check $g_1(0) + g_1(1) = c$, $deg(g) \leq d$, sample $r_1 \in \F$, send $r_1$, \\
for $j = 2 \dots n-1$:
\begin{itemize}[label={}]
  \item $P$: compute $g_j(x) = \sum_{b_{j+1} \dots b_n \in \{0, 1\}} g(r_1 \dots r_{j-1}, x, b_{j+1} \dots b_n)$, send $g_j(x)$,
  \item $V$: checks $g_j(0) + g_j(1) = g_{j-1}(r_{j-1})$, $deg(g_j) \leq d$, sample $r_j \in \F$, send $r_j$,
\end{itemize}
$P$: compute $g_n(x) = g(r_1 \dots r_{n-1}, x)$, send $g_n(x)$, \\
$V$: checks $g_n(0) + g_n(1) = g_{n-1}(r_{n-1})$, $deg(g_n) \leq d$, 
for random $r_n \in \F$ check $g_n(r_n) = g(r_1 \dots r_n)$. \\
Completeness:
\begin{itemize}[label={}]
  \item \checkmark (sum, all possibilities).
\end{itemize}
Cost:
\begin{itemize}[label={}]
  \item Prover: $O\left(2^n\right)$,
  \item verifier: evaluate $g_i \; \forall i$, $g$ at one point, $<< O\left(2^n\right)$.
\end{itemize}
Communication:
\begin{itemize}[label={}]
  \item $deg(g_1) + \dots + deg(g_{n-1}) + O(n)$ elements of $\F$.
\end{itemize}


% 14. predavanje: 12.1.

Prove that $H_g = \sum_{b_1 \dots b_n \in \{0, 1\}} g(b_1 \dots b_n)$. \\
Soundness:
\begin{itemize}[label={}]
  \item $P$: sends $g_i(x)$.
  \item If $P$ cheats, at least one of polynomials is not correct.
  \item Sends $g_i^{'}(x) \neq g_i(x)$.
  \item Verifier checks $g_i^{'}(r_i) = g_{i+1}(0) + g_{i+1}(1) = g_i(r_i)$
  \item $\to$ probability of this: $\leq \frac{d}{|\F|}$.
  \item Soundness error: $\leq n \cdot \frac{d}{|\F|}$; union bound of rounds.
\end{itemize}
Application 1:
\begin{itemize}[label={}]
  \item Counting solutions of SAT.
  \item $F$ SAT formula with $s$ operations and $n$ variables.
  \item Replace with polynomial $g(x_1 \dots x_n)$ such that $F(b_1 \dots b_n) = g(b_1 \dots b_n)$.
  \item For every $b_1 \dots b_n$:
  \item replace $AND(x,y)$ with $x \cdot y$, $OR(x,y)$ with $x + y - x \cdot y$, $NOT(x)$ with $1-x$.
  \item Number of SAT solutions = $\sum_{b_1 \dots b_n \in \{0, 1\}} F(b_1 \dots b_n) \\
    = \sum_{b_1 \dots b_n \in \{0, 1\}} g(b_1 \dots b_n)$.
  \item Prover can prove that $H_g =$ number of solutions by using sum-check.
  \item Complexity:
    \begin{itemize}[label={}]
      \item prover: $O\left(2^n\right)$,
      \item proof size (communication complexity): $O(n)$ - $n$ polynomials,
      \item verifier: $O(n + s)$.
    \end{itemize}
  \item Error: $\leq \frac{n \cdot s}{|\F|}$; $s$: number of operations.
\end{itemize}
Application 2:
\begin{itemize}[label={}]
  \item Counting triangles in $G$.
  \item $A$: adjacency matrix.
  \item Number of triangles in $G = \frac{tr\left(A^3\right)}{6}$.
  \item We think of $A$ as a mapping.
  \item $[n] \times [n] \to \{0, 1\}$.
  \item Define $A^{'}: \{0, 1\}^{\log_2 n} \times \{0, 1\}^{\log_2 n} \to \{0, 1\}$,
  \item such that $A(i, j) = A^{'}(binary(i), binary(j))$.
  \item For example: $n=16$, $A(0, 3) = A^{'}(0000, 0011)$.
  \item Now we define polynomial $f_A: \F^{\log_2 n} \times \F^{\log_2 n} \to \F$
  \item $f_A(x_1 \dots x_{\log_2 n}, y_1 \dots y_{\log_2 n})$ such that
  \item $f_A(b_1 \dots c_{\log_2 n}) = A^{'}(b_1 \dots c_{\log_2 n})$ for every $b_1 \dots c_{\log_2 n} \in \{0, 1\}$.
  \item Example: $A = \begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix}, f_A(x, y) = x(1-y) + y(1-x)$.
  \item In general:
    \begin{align*}
      &f_A(x_1 \dots x_{\log_2 n}, y_1 \dots y_{\log_2 n}) \\
      = &\sum_{a,b \in \{0, 1\}^{\log_2 n}, A^{'}(a,b)=1} (-1)^{num\_zeros(a,b)}
      (x_1 - (1 - a_1)) \dots (y_{\log_2 n} - (1 - b_{\log_2 n})).
    \end{align*}
  \item Now we define $g_A(x, y, z) = f_A(x, y) \cdot f_A(y, z) \cdot f_A(z, x)$, $x, y, z \in \F^{\log_2 n}$.
  \item Number of triangles $= \frac{\sum_{a, b, c \in \{0, 1\}^{\log_2 n}} g_A(a, b, c)}{6}$
  \item $\implies$ we can use sum-check.
  \item Proof size: $O(3 \cdot \log_2 n)$ (number of rounds $\to$ poly),
  \item verifier: $O(\log_2 n) + O\left(n^2\right)$.
\end{itemize}


\section{SNARK}

Succint Non-interactive ARgument of Knowledge. \\
Succint: proof short and verification fast. \\
Non-interactive: just sending a proof. \\
$x \rightarrow P \rightarrow V \leftarrow x$. \\
$P$: convince $V$ that $f(x) = y$.
\begin{itemize}
  \item $f(x) = x^3 + x + 5$ as a algebraic circuit. \\
    Break down into $+, -, *, /$ in some field $\Z_p$ \\
    % skica
    Proof with states $\overrightarrow{s} = (five, x, out, s_1, s_2, s_3)$. \\
    Example: proof that $f(3) = 35$. \\
    $\overrightarrow{s} = (1, 3, 35, 9, 27, 30)$.
  \item To R1CS. \\
    Give vectors $\overrightarrow{a_i}, \overrightarrow{b_i}, \overrightarrow{c_i}$
    for each state such that
    \begin{equation*}
      (\overrightarrow{a_i} \cdot \overrightarrow{s}) \cdot
      (\overrightarrow{b_i} \cdot \overrightarrow{s}) =
      (\overrightarrow{c_i} \cdot \overrightarrow{s}) \iff \text{ gate $i$ was correctly calculated}.
    \end{equation*}
    Example.
    \begin{itemize}[label={}]
      \item For gate 1 ($\cdot$):
        \begin{align*}
          \overrightarrow{a_1} &= [0, 1, 0, 0, 0, 0] \\
          \overrightarrow{b_1} &= [0, 1, 0, 0, 0, 0] \\
          \overrightarrow{c_1} &= [0, 0, 0, 1, 0, 0]
        \end{align*}
        $\overrightarrow{a_1} \cdot \overrightarrow{s} = x, \overrightarrow{c_1} \cdot \overrightarrow{s} = s_1$.
      \item For gate 3 ($+$):
        \begin{align*}
          \overrightarrow{a_3} &= [0, 1, 0, 0, 1, 0] \\
          \overrightarrow{b_3} &= [1, 0, 0, 0, 1, 0] \\
          \overrightarrow{c_3} &= [0, 0, 0, 0, 0, 1]
        \end{align*}
        $(x + s_2) \cdot 1 = s_3$.
    \end{itemize}
    We have instead of circuit
    \begin{equation*}
      \begin{bmatrix}\overrightarrow{a_1} \\ \overrightarrow{a_2} \\ \vdots \\ \overrightarrow{a_n}\end{bmatrix}
      \cdot \overrightarrow{s} \odot
      \begin{bmatrix}\overrightarrow{b_1} \\ \overrightarrow{b_2} \\ \vdots \\ \overrightarrow{b_n}\end{bmatrix}
      \cdot \overrightarrow{s} -
      \begin{bmatrix}\overrightarrow{c_1} \\ \overrightarrow{c_2} \\ \vdots \\ \overrightarrow{c_n}\end{bmatrix}
      \cdot \overrightarrow{s} = \overrightarrow{0}.
    \end{equation*}
    $\odot$: coordinate-wise multiplication. \\
    $\overrightarrow{s}$ needs to be solution for
    \begin{equation*}
      A \cdot \overrightarrow{s} \odot B \cdot \overrightarrow{s} = C \cdot \overrightarrow{s};
    \end{equation*}
    $A, B, C \; m \times n$ matrices.
  \item To Quadratic Arithmetic Programs (QAP). \\
    Let $a_i(x)$ be a polynomial such that \\
    $a_i(j) = \overrightarrow{a}_j[i]$ for $i \in [n], j \in [m]$. \\
    $A = \begin{bmatrix}
      a_1(1) & a_2(1) & \dots & a_n(1) \\
      a_1(2) & \dots & & \\
      \vdots & & & \\
      a_1(m) & \dots & & a_n(m)
    \end{bmatrix}$. \\
    Example:
    \begin{itemize}[label={}]
      \item $a_1(x) = -5 + 9.16x + 5x^2 + 0-833x^3$
      \item $a_2(x) = 8 - 11.33x + 5x^2 - 0.666x^3$
      \item $a_3(x) = 0$
      \item $\vdots$
      \item $a_6(x) = \dots$
      \item $[a_1(1) \dots a_6(1)] = [0, 1, 0, 0, 0, 0] = \overrightarrow{a_1}$.
    \end{itemize}
    We get $a_i$ with interpolation $deg \; a_i \leq n-1$.
    \begin{equation*}
      \left([a_1(x), a_2(x) \dots a_n(x)] \cdot \overrightarrow{s}\right) \odot
      \left([b_1(x), b_2(x) \dots b_n(x)] \cdot \overrightarrow{s}\right) -
      \left([c_1(x), c_2(x) \dots c_n(x)] \cdot \overrightarrow{s}\right)
    \end{equation*}
    should have zeros in $1, 2 \dots m$ \\
    $\iff \; A(x) \cdot B(x) \cdot C(x) = (x-1) (x-2) \dots (x-m) \cdot h(x)$. \\
    Summary up to now:
    \begin{itemize}
      \item instead of states we have polynomials,
      \item instead of states, we have coefficients $\cdot \overrightarrow{s}$.
    \end{itemize}
    $a_i(x), b_i(x), c_i(x) \to P \to V \leftarrow a_i(x), b_i(x), c_i(x)$. \\
    $P \to V$: $A(x), B(x), C(x), h(x)$: too much. \\
    $P \to V$: $A(r), B(r), C(r), h(r)$, $r$ random. \\
    $V$: checks $A(r) \cdot B(r) = C(r) + h(r) \cdot t(r)$; \\
    works if $V$ doesn't cheat. \\
    Cryptographic background:
    \begin{itemize}
      \item Lets have pairs \\
        $(g_1, h_1), (g_2, h_2) \dots (g_n, h_n)$, where $g_i^k = h_i$, you don't know $k$. \\
        Cryptographic assumption: if we provide $(g^{'}, h^{'})$ such that \\
        $(g^{'})^{k} = h^{'}$, then $g^{'} = g_1^{k_1} \cdot g_n^{k_n}, h^{'} = h_1^{k_1} \cdot h_n^{k_n}$.
      \item Pairing groups. \\
        In some group one can define a pairing \\
        $e: G \times G \to G_r$ such that \\
        $e(g_1 \cdot g_2, h) = e(g_1, h) \cdot e(g_2, h)$, \\
        $e(g, h_1 \cdot h_2) = e(g, h_1) \cdot e(g, h_2)$, \\
        $e\left(g^x, g^y\right) = e(g, g)^{xy}$.
    \end{itemize}
    Assume $P, V$ have
    \begin{itemize}
      \item $g^{a_1(r)}, g^{a_2(r)} \dots$,
      \item $g^{b_1(r)}, g^{b_2(r)} \dots$,
      \item $g^{c_1(r)}, g^{c_2(r)} \dots$,
      \item $g^{t(r)}$,
      \item $g, g^r, g^{r^2} \dots g^{r^{n-1}}$
    \end{itemize}
    without knowing $r$. \\
    Improved protocol: \\
    $P$ sends to $V$
    \begin{itemize}
      \item $g^{A(r)} = \left(g^{a_1(r)}\right)^{k_1} \dots \left(g^{a_n(r)}\right)^{k_n}$
      \item $g^{B(r)} \dots$
      \item $g^{C(r)} \dots$
      \item $g^{h(r)} = g^{h_0} \cdot \left(g^r\right)^{k_1} \dots \left(g^{r^{n-1}}\right)^{k_{n-1}}$.
    \end{itemize}
    $V$: checks $e\left(g^{A(r)}, g^{B(r)}\right) = e\left(g^{C(r)}, g\right) \cdot e\left(g^{h(r)}, g^{t(r)}\right)$. \\
    Problem: $g^{A(r)}$ needs to be linear combination of $g^{a_1(r)} \dots g^{a_n(r)}$, also $g^{B(r)}, g^{C(r)}$. \\
    We need additional values:
    \begin{itemize}
      \item $g^{a_1(r) \cdot k_1} \dots g^{a_n(r) \cdot k_1}$, $k_1$ unknown,
      \item $g^{b_i(r) \cdot k_2} \dots$,
      \item $g^{c_i(r) \cdot k_3} \dots$,
      \item $g^{k_1}, g^{k_2}, g^{k_3}$.
    \end{itemize}
    Prover also submits:
    \begin{itemize}
      \item $g^{k_1 A(r)} = \left(g^{a_1(r) k_1}\right)^{s_1} \dots \left(g^{a_n(r) k_1}\right)^{s_1}$
      \item $g^{k_2 B(r)} = \dots$
      \item $g^{k_3 C(r)} = \dots$
    \end{itemize}
    Verifier calculates
    \begin{itemize}
      \item $e\left(g^{A(r)}, g^{k_1}\right) = e\left(g^{k_1 \cdot A(r)}, g\right)$,
      \item $e\left(g^{B(r)}, g^{k_2}\right) = \dots$
    \end{itemize}
    $\implies$ by crypto assumption $A(r)$ is linear combination of $a_1(r) \dots a_n(r)$. \\
    Downside: we need $g^{a_1(r)} \dots$
\end{itemize}


%\clearpage
%\phantomsection

%\addcontentsline{toc}{chapter}{Literatura}
%\bibliography{../bibtex/literatura}
%\bibliographystyle{plainnat}


%\clearpage
%\phantomsection

%\chapter*{Dodatki}
%\addcontentsline{toc}{chapter}{Dodatki}
%D.




\end{document}
