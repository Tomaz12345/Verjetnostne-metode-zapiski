\documentclass[a4paper, 12pt]{book}

\usepackage{fancyhdr}

\newcommand{\ttitle}{Verjetnostne metode v računalništvu - zapiski s predavanj prof. Marca}
\newcommand{\ttitleshort}{Verjetnostne metode v računalništvu}
\newcommand{\tauthor}{Tomaž Poljanšek}
\newcommand{\tdate}{študijsko leto 2023/24}

\usepackage{color}
\usepackage{soul}
\usepackage[numbers]{natbib}

\usepackage{physics}

\usepackage[parfill]{parskip}
\usepackage[hyphens]{url}

\usepackage[usestackEOL]{stackengine}[2013-10-15] % formatting Pascal
\usepackage[dvipsnames]{xcolor}

\usepackage{cancel}
\usepackage[export]{adjustbox}

% Related to math
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{mathtools}
\usepackage{youngtab}
\usepackage{tikz}

% encoding and language
\usepackage{lmodern}
\usepackage[slovene, english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% multiline comments
\usepackage{comment}
\usepackage{verbatim}

% random text - for texting
\usepackage{lipsum}
\usepackage{blindtext}

\usepackage{hyperref}

% images
\usepackage{graphicx}
\graphicspath{ {../images/} }

% no blank page
\usepackage{atbegshi}
\renewcommand{\cleardoublepage}{\clearpage}
%\renewcommand{\clearpage}{}

\usepackage{listings}
\usepackage{verbatim}
%\usepackage{fancyvrb}
%\usepackage{bera}

\newcommand*\Eval[3]{\left.#1\right\rvert_{#2}^{#3}}

\lstset{basicstyle=\ttfamily,
escapeinside={||},
mathescape=true}

% theorems
\theoremstyle{definition}
\newtheorem{counter}{Counter}[section]
\newtheorem{defn}[counter]{Definicija}
\newtheorem{lemma}[counter]{Lema}
\newtheorem{conseq}[counter]{Posledica}
\newtheorem{claim}[counter]{Trditev}
\newtheorem{theorem}[counter]{Izrek}
\newtheorem{pro}[counter]{Dokaz}
%%
\theoremstyle{remark}
\newtheorem*{ex}{Primer}
\newtheorem*{exmp}{Zgled}
\newtheorem*{rem}{Opomba}

% QED
\renewcommand\qedsymbol{$\blacksquare$}

\hypersetup{pdftitle={\ttitle}}

\addtolength{\marginparwidth}{-20pt}
\addtolength{\oddsidemargin}{40pt}
\addtolength{\evensidemargin}{-40pt}

\renewcommand{\baselinestretch}{1.3}
\setlength{\headheight}{15pt}
\renewcommand{\chaptermark}[1]
{\markboth{\MakeUppercase{\thechapter.\ #1}}{}} \renewcommand{\sectionmark}[1]
{\markright{\MakeUppercase{\thesection.\ #1}}} \renewcommand{\headrulewidth}{0.5pt} \renewcommand{\footrulewidth}{0pt}

% header
\fancyhf{}
\fancyhead[LE,RO]{\sl \thepage} 
\fancyhead[RE]{\sc \tauthor}
\fancyhead[LO]{\sc \ttitleshort}


\newcommand{\autfont}{\Large}
\newcommand{\titfont}{\LARGE\bf}
\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{empty}\cleardoublepage}}
\setcounter{tocdepth}{1}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\ch}{\operatorname{char}}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{float}
\usepackage{multirow}
\usepackage{icomma}
\usepackage{tabularx}
\usepackage{hhline}

\usepackage{enumitem}
\usepackage{ulem}
\newcommand{\msout}[1]{\text{\sout{\ensuremath{#1}}}} % cross text in math mode

\usepackage{alltt}

\title{\ttitle}
\author{\tauthor}
\date{\tdate}

\newcommand\mymaketitle{
  \begin{titlepage}
    \begin{center}
        \vspace*{4cm}
        \Huge
        \textbf{\ttitle}
                        
        \vspace{1.5cm}
        \huge
        \tauthor
            
        \vspace{3cm}
        \Large
        \tdate
    \end{center}
  \end{titlepage}
}


% inductive hypothesis - IH

\begin{document}

\selectlanguage{slovene}
%\setcounter{page}{1}
\renewcommand{\thepage}{}
\newcommand{\sn}[1]{"`#1"'}

\mymaketitle

\clearpage
%\AtBeginShipoutNext{\AtBeginShipoutDiscard}

\frontmatter

% kazalo
\pagestyle{empty}
\def\thepage{}
\tableofcontents{}

%%
\def\x{\hspace{3ex}}    %BETWEEN TWO 1-DIGIT NUMBERS
\def\y{\hspace{2.45ex}}  %BETWEEN 1 AND 2 DIGIT NUMBERS
\def\z{\hspace{1.9ex}}    %BETWEEN TWO 2-DIGIT NUMBERS
\stackMath

%\clearpage
%\phantomsection
%\addcontentsline{toc}{chapter}{Povzetek}
%\chapter*{Povzetek}

%Predloga.

%\newpage

\pagenumbering{arabic}

\mainmatter
\setcounter{page}{1}
\pagestyle{fancy}


% 1. predavanje: 6.10.


\chapter{Introduction}


\section{Probability}

$(\Omega, F, P_r)$:
\begin{itemize}[label=$\circ$]
  \item $\emptyset \in F$,
  \item $A \in F \implies A^c \in F$,
  \item $A_1, A_2 \dots \in F \implies \cup_{i=1}^{\infty} A_i \in F$.
\end{itemize}
$P_r(A) \geq 0$, \\
$P_r\left(\cup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P_r(A_i)$ if $A_i$ disjoint, \\
$P_r\left(\cup_{i=1}^{\infty} A_i\right) \leq \sum_{i=1}^{\infty} P_r(A_i)$, \\
$\Omega = \{\omega_1, \omega_2 \dots\}$ - countable case. \\
$\begin{pmatrix}
  \omega_1 & \omega_2 & \dots \\
  p_1 & p_2 & \dots
\end{pmatrix}$
\begin{ex} \text{}
  \begin{verbatim}
    Alg():
      while True:
        B = sample as random from {0,1}  # 1 with probability p
        if B = 1:
          return
  \end{verbatim}
  $\Omega = \{1, 01, 001, 0001 \dots\}$ \\
  $\begin{pmatrix}
    1 & 01 & 001 & 0001 & \dots \\
    p & (1-p)p & (1-p)^2 p & (1-p)^3p & \dots
  \end{pmatrix}$.
\end{ex}


\section{Random variables}

$X: \Omega \to \Z$. \\
$E[X] = \sum_{c \in \Z} c \cdot P_r(X = c)$ expected value of $X$. \\
Properties:
\begin{itemize}[label=$\circ$]
  \item $E[f(X)] = \sum_{c \in \Z} f(c) \cdot P_r(X = c)$,
  \item $E[aX + bY] = aE[X] + bE[Y]$,
  \item $E[X \cdot Y] = E[X] \cdot E[Y]$ if $X, Y$ independent,
  \item $P_r(X \geq a) \leq \frac{E[X]}{a} \; \forall a > 0 \; X \geq 0$ Markov inequality.
\end{itemize}
\begin{ex}
  (Continuing from before). \\
  $X =$ number of trials before return. \\
  $X: \Omega \to \Z$. \\
  $X: 1 \to 1, 01 \to 2, 003 \to 3 \dots$ \\
  $\begin{pmatrix}
    1 & 2 & 3 & 4 & \dots \\
    p & (1-p)p & (1-p)^2 p & (1-p)^3p & \dots
  \end{pmatrix}$ - geometric distribution.
\end{ex}
\begin{claim}
  $E[X] = \frac{1}{p}$.
\end{claim}
\begin{pro}
  $X = \sum_{i=1}^{\infty} X_i$. \\
  $X_i = \begin{cases}
    1 \text{ if trial $i$ is executed} \\
    0 \text{ else}
  \end{cases}$ \\
  \begin{align*}
    E[X] &= E[\sum_{i=1}^{\infty} X_i] = \sum_{i=1}^{\infty} E[X_i] = \\
    &= \sum_{i=1}^{\infty} (1-p)^{i-1} = \frac{i=0}{\infty} (1-p)^i = \frac{1}{1-(1-p)} = \frac{1}{p}.
  \end{align*}
\end{pro}
$E[X] = \frac{1}{p}$. \\
$P_r(X \geq 100 \cdot \frac{1}{p}) \leq \frac{E[X]}{\frac{1}{p}} = \frac{1}{100}$.
\begin{defn}
  $H_n = 1 + \frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{n} = \sum_{i=1}^{\infty} \frac{1}{i}$.
\end{defn}
\begin{theorem}
  $H_n \leq 1 + \ln(n)$.
\end{theorem}
\begin{pro}
  \begin{equation*}
    H_n = 1 + \sum_{i=2}^{n} \frac{1}{i} \stackrel{\text{integral}}{\leq}
    1 + \int_{1}^{n} \frac{dx}{x} = 1 + \Eval{\ln(x)}{1}{n} = 1 + \ln(n).
  \end{equation*}
  % skica
\end{pro}



\chapter{Quicksort, min-cut}


\section{Quicksort}

\begin{alltt}
  Input: set (no equal element) (unordered list) S\(\in\R\)
      (or whatever you can compare linearly)
  Output: ordered list
  Code:
    def Quicksort(S):
      if |S|= 0 or 1:
        return S
      else:
        a = uniformly at random from S
        S\(\sp{-}\) = \{b \(\in\) S | b < a\}
        S\(\sp{+}\) = \{b \(\in\) S | a < b\}
        return Quicksort(S\(\sp{-}\)), a, Quicksort(S\(\sp{+}\))
\end{alltt}
% skica
$C(n)$ - random variable, the number of comparisons in evaluation of Quicksort with $|S| = n$.
\begin{theorem}
  $E[C(n)] = O\left(N \log(n)\right)$.
\end{theorem}
\begin{pro}
  $C(0) = C(1) = 0$. \\
  \begin{align*}
    E[C(n)] &= n - 1 + \sum_{i=1}^{n} \left(E[C(i-1)] + E[C(n-i)]\right) \cdot P_r(a \text{ is $i$-it element}) \leq \\
    &\leq n + \frac{2}{n} \sum_{i=1}^{n-1} E[C(i)].
  \end{align*}
  Induction: \\
  $n = 1: \checkmark$ \\
  $n-1 \to n$:
  \begin{align*}
    E[C(n)] &\leq n + \frac{2}{n} \sum_{i=1}^{n} E[C(i)] \leq \\
    &\leq n + \frac{2}{n} \sum_{i=1}^{n} 5i \log i \leq \\
    &\leq n + \frac{2}{n} \sum_{i=1}^{\lfloor\frac{n}{2}\rfloor} 5i \log i +
      \frac{2}{n} \sum_{i=1+\lfloor\frac{n}{2}\rfloor}^{n-1} 5i \log i \leq \\
    &\leq n + \frac{2}{n} \sum_{i=1}^{\lfloor\frac{n}{2}\rfloor} 5i \log \frac{n}{2} +
      \frac{2}{n} \sum_{i=1+\lfloor\frac{n}{2}\rfloor}^{n-1} 5i \log n \leq \\
    (\log \frac{n}{2} &= \log n - 1) \\
    &\leq n + \frac{2}{n} \left(\sum_{i=1}^n 5i \log n - \sum_{i=1}^{\frac{n}{2}} 5i\right) = \\
    &= n + \frac{10}{n} \left(\frac{n(n-1)}{2} \log n - \frac{\frac{n}{2} (\frac{n}{2} + 1)}{2}\right) \leq \\
    &\leq n + 5(n-1) \log n - n < \\
    &< 5n \log n.
  \end{align*}
\end{pro}
$P\left(C(n) \geq b \cdot 5n \log n\right) \stackrel{\text{Markov}}{\leq} \frac{1}{b}$.
\begin{pro} \text{} \\
  2: \\
  Let $S_1, S_2 \dots S_n$ sorted elements of $S$. \\
  Define random variable
  $X_{ij} = \begin{cases}
    1 \text{ if $S_i$ and $S_j$ are compared} \\
    0 \text{ else}
  \end{cases}$ \\
  $C(n) = \sum_{1 \leq i < j \leq n} E[X_{ij}]$. \\
  $E[X_{ij}] = P(S_i$ and $X_j$ compared$)$. \\
  % skica
  $S_{ij}$ - the last set including $S_i$ and $S_j$. \\
  $E[X_{ij}] = \frac{2}{|S_{ij}|} \leq \frac{2}{j-i+1}$. \\
  $|S_{ij}| \geq j - i + 1$. \\
  $S_{ij}$ has everything in between. \\
  \begin{align*}
    \implies E[C(n)] &\leq \sum_{1 \leq i < j \leq n} \frac{2}{j-i+1} = \\
    &\stackrel{k=j-i+1}{=} \sum_{i=1}^{n-1} \sum_{k=2}^{n-i+1} \frac{2}{k} \leq \\
    &\leq 2 \cdot n \cdot H_n \leq \\
    &\leq 2 n (1 + \log n).
  \end{align*}
\end{pro}


% 2. predavanje: 13.10.

\section{Min-cut}

$G$ multigraph. \\
Cut: $U \subset V(G), \; U \neq \emptyset, V(g)$. \\
$(U, V(G) \setminus U) = \{uv \in E(G) \mid u \in U, v \in V(G) \setminus U\}$. \\
% skica
Problem min-cut: \\
Input: $G$. \\
Output: $\min |(U, V(G) \setminus U)|$ - cut size.
\begin{alltt}
  Algorithm 1:
    x \(\in\) V(G)
    Call maxFlow(G, x, y) \(\forall y \in V(G)\)
    Take min
\end{alltt}
$maxFlow$ is Edmonds-Karp algorithm $O\left(|V| |E|^2\right)$. \\
\begin{alltt}
  Algorithm 2 (Stoer Wagner)
\end{alltt}
Is $O\left(|E| |V| + |V| log |V|\right)$.
\begin{alltt}
  Algorithm randMinCut:
    G\(\sb_{0}\) = G
    i = 0
    while |V(G\(\sb{i}\))| > 2:
      e\(\sb{i}\) = uniformly at random from G\(\sb{i}\)
      G\(\sb{i+1}\) = G\(\sb{i}\) / \(\sb{e\sb{i}}\)
      i = i + 1
    u, v = V(G\(\sb{n-2}\)) // n = |V(G)|
    U = \{w \(\in\) V(G) | w is merged into u\}
    return (U, V(G) \textbackslash U)
\end{alltt}
\begin{theorem}
  Algorithm $randMinCut$ gives you a minimal cut with probability greater or equal to $\frac{2}{n(n-1)}$.
\end{theorem}
\begin{pro} \text{} \\
  Fact 1: $minCut(G_i) \leq minCut(G_i)$;
  \begin{itemize}[label={}]
    %\item $\leq$: %example
    \item $\ngtr$: $minCut$ remains.
  \end{itemize}
  Fact 2: $minCut(G) \leq \delta(G)$. \\
  $k := minCut(G)$. \\
  Let $(A,B)$ be an optimal cut. \\
  $\epsilon_i$ not in $(A,B)$.
  \begin{align}
    &P_r(\text{Algorithm not returning } (A,B)) \nonumber \\
    &= P_r(\epsilon_0 \cap \dots \cap \epsilon_{n-3}) \nonumber  \\
    &= P_r(\epsilon_0 \cap \dots \cap \epsilon_{n-4}) \cdot
      P_r(\epsilon_{n-3} \mid \epsilon_0 \cap \dots \cap \epsilon_{n-4}) \nonumber  \\
    &= P_r(\epsilon_{n-3} \mid \cap_{i=0}^{n-4} \epsilon_i) \cdot
      P_r(\epsilon_{n-3} \mid \cap_{i=0}^{n-4} \epsilon_i) \nonumber \\
    &\dots P_r(\epsilon_1 \mid \epsilon_0) \cdot P_r(\epsilon_0). (*) %\label{randMinCut-mid}
  \end{align}
  \begin{equation*}
    P_r(\overline{\epsilon_i} \mid \epsilon_{i-1} \cap \dots \cap \epsilon_0) =
      %\frac{k}{|E(G_i)|} \stackrel{\text{\refeq{E-gi}}}{\leq} \frac{k}{\frac{(n-i)k}{2}} = \frac{2}{n-i}
      \frac{k}{|E(G_i)|} \stackrel{\text{(**)}}{\leq} \frac{k}{\frac{(n-i)k}{2}} = \frac{2}{n-i}
  \end{equation*}
  \begin{equation}
    |E(G_i)| \geq \frac{(n-i) \delta(G)}{2} \geq \frac{(n-i)k}{2}. (**) % \label(E-gi)
  \end{equation}
  \begin{equation*}
    P_r(\epsilon_i \mid \epsilon_{i-1} \cap \dots \cap \epsilon_0) \geq 1 - \frac{2}{n-i} = \frac{n-2-i}{n-i}.
  \end{equation*}
  \begin{equation*}
    %\text{\ref{randMinCut-mid}} \geq \frac{n-2}{n} \cdot \frac{n-3}{n-1} \dots \frac{1}{3} = \frac{2}{n(n-1)}.
    (*) \geq \frac{n-2}{n} \cdot \frac{n-3}{n-1} \dots \frac{1}{3} = \frac{2}{n(n-1)}.
  \end{equation*}
\end{pro}
\begin{theorem}
  Running $randMinCut \; n(n-1)$ times and taking best output gives correct solution with probability $\geq 0.86$.
\end{theorem}
\begin{pro}
  $A_i$ - event that $i$-th run gives sub-optimal solution.
  \begin{align*}
    P_r(\text{solution not correct}) &= P_r(A_1 \cap \dots \cap A_{n(n-1)}) \\
    &= \prod_{i=1}^{n(n-1)} P_r(A_i) \leq (1 - \frac{2}{n(n-1)})^{n(n-1)} \\
    &\leq e^{-\frac{2}{n(n-1)} \cdot n(n-1)} = e^{-2} \leq 0.14.
  \end{align*}
  $1 - x \leq e^x \; \forall x \in \R$.
\end{pro}
If we run $n(n-1) log(n)$ times $\to O\left(\frac{1}{n}\right)$. \\
$O\left(n^2 \log n \cdot n\right)$. \\
Improved: $O\left(n^2 \log^3 n\right)$.



\chapter{Complexity classes}


Decision problem - yes/no question on a set of inputs = asking $w \in \Pi$. \\
Randomized algorithms:
\begin{itemize}
  \item Las Vegas algorithms: always gives correct solution, example: $Quicksort$.
  \item Monte Carlo algorithms: it can give wrong answers.
    Monte Carlo algorithms subtypes:
    \begin{itemize}
      \item type(1): $\begin{cases}
          \text{if } \omega \in \Pi \implies \text{ algorithm returns \sn{$\omega \in \Pi$} with probability } \geq \frac{1}{2} \\ 
          \text{if } \omega \notin \Pi \implies \text{ algorithm returns \sn{$\omega \in \Pi$} with probability } = 0 
        \end{cases}$
      \item type(2): $\begin{cases}
          \text{if } \omega \in \Pi \implies \text{ algorithm returns \sn{$\omega \in \Pi$} with probability } = 1 \\ 
          \text{if } \omega \notin \Pi \implies \text{ algorithm returns \sn{$\omega \in \Pi$} with probability } \leq \frac{1}{2} 
        \end{cases}$
      \item type(3): $\begin{cases}
          \text{if } \omega \in \Pi \implies \text{ algorithm returns \sn{$\omega \in \Pi$} with probability } \geq \frac{3}{4} \\ 
          \text{if } \omega \notin \Pi \implies \text{ algorithm returns \sn{$\omega \in \Pi$} with probability } \leq \frac{1}{2}
        \end{cases}$
    \end{itemize}
    type(1) and type(2): one-sided error, type(3): 2-sided error. \\
    $\frac{1}{2}, \frac{3}{4}$ and $\frac{1}{4}$ arbitrary numbers, can be something different (for type(3) better than coin flip).
\end{itemize}
\begin{ex}
  Decisional problem: does a graph $G$ have $minCut \leq k$? \\
  Run $randMinCut(G) \; n(n-1)$ times.
  \begin{alltt}
    Algorithm randMinCut:
      if one of runs gives |(A,B)| \(\leq\) k:
        return true
      else:
        return false
  \end{alltt}
\end{ex}
Complexity classes:
\begin{itemize}
  \item RP (randomized polynomial time): decisional problems for which there exists Monte Carlo algorithm of type(1)
    with polynomial time complexity (worst case).
  \item co-RP: decisional problems for which there exists Monte Carlo algorithm of type(2) with polynomial time complexity
    (worst case).
  \item BRP (bounded-error probabilistic polynomial time): decisional problems for which there exists Monte Carlo algorithm of type(3)
    with polynomial time complexity (worst case).
  \item ZPP (zero-error probabilistic polynomial time): decisional problems for which there exists Las Vegas algorithm
    with expected polynomial time complexity (worst case).
\end{itemize}
% skica
ZPP = RP $\cap$ co-RP.



\chapter{Chernoff bounds}


\begin{theorem}
  Let $X_1, X_2 \dots X_n$ independent random variables with image $\{0, 1\}$. \\
  Let $p_i = P_r(X_i = x_i), X = \sum_{i=1}^{n} X_i$ and $\mu = E(X) = p_1 + \dots + p_n$. \\
  For every $\delta \in (0,1)$:
  \begin{align*}
    &P_r(X - \mu \geq \delta \mu) \leq e^{-\frac{\delta^2 \mu}{3}} \\
    &P_r(\mu - X \leq \delta \mu) \leq e^{-\frac{\delta^2 \mu}{2}} \\
    \implies &P_r(|X - \mu| \geq \delta \mu) \leq e^{-\frac{\delta^2 \mu}{3}}. \\
  \end{align*}
\end{theorem}
% skica
Probability falls extremely quickly after $E(X)$.


% 3. predavanje: 20.10.

\begin{pro}
  \begin{align*}
    P_r(X - \mu \geq \delta \mu) &= P_r(X \geq \mu(1+\delta)) \\
    &\stackrel{t>0}{=} P_r(tX \geq t\mu(1+\delta)) \\
    &\stackrel{e^y>0}{=} P_r(e^{tX} \geq e^{t\mu(1+\delta)}) \\
    &\stackrel{\text{Markov}}{\leq} \frac{E\left(e^{tX}\right)}{e^{t\mu(1+\delta)}} \\
    &\stackrel{\refeq{eq:et_x}}{\leq} \frac{e^{(e^t-1)\mu}}{e^{t\mu(1+\delta)}} \\
    &\stackrel{\refeq{eq:leq_in_exp}}{\leq} e^{-\mu \frac{\delta^2}{3}}.
  \end{align*}
  \begin{align}
    E(e^{tX}) &= E(e^{tX_1 + \dots + tX_n}) \nonumber \\
    &= E(e^{tX_1} \dots e^{tX_n}) \nonumber \\
    &\stackrel{\text{independent}}{=} \prod_{i=1}^{n} E(e^{tX_i}) \nonumber \\
    &\stackrel{\refeq{eq:et_xi}}{\leq} \prod_{i=1}^{n} e^{p_i(e^t-1)} \nonumber \\
    &= e^{(e^t-1) \sum_{i=1}^{n}p_i} \nonumber \\
    &= e^{(e^t-1)\mu} \label{eq:et_x}.
  \end{align}
  \begin{equation}
    E(e^{tX_i}) = p_i \cdot e^t + (1-p_i) \cdot e^0 = 1+p_i(e^t-1) \stackrel{1+x\leq e^x}{\leq} e^{p_i(e^t-1)}.
    \label{eq:et_xi}
  \end{equation}
  Want:
  \begin{equation}
    e^t - 1 - t(1+\delta) \leq -\frac{\delta^2}{3} \; \forall \delta \in (0,1) \label{eq:leq_in_exp}
  \end{equation}
  \begin{align*}
    &t = \ln(1+\delta) \\
    &f(\delta) = 1 + \delta - 1 - (1+\delta) \ln(1+\delta) + \frac{\delta^2}{3} \stackrel{?}{\leq} 0 \\
    &f(0) = 0 \\
    &f^{'}(\delta) = 1 - \ln(1+\delta) - 1 + \frac{2}{3} \delta = \frac{2}{3} \delta - \ln(1+\delta) \stackrel{?}{\leq} 0 \\
    &\frac{2}{3} \delta \leq \ln(1+\delta) \\
    % skica
    &\delta=1: \; \frac{2}{3} \stackrel{?}{\leq} \ln(2) \approx 0.69 \checkmark
  \end{align*}
  \begin{align*}
    P_r(\mu - X \leq \delta \mu) &= P_r(X \geq \mu(1-\delta)) \\
    &\stackrel{t>0}{=} P_r(tX \geq t\mu(1-\delta)) \\
    &\stackrel{e^y>0}{=} P_r(e^{tX} \geq e^{t\mu(1-\delta)}) \\
    &\leq \dots \leq \frac{e^{(e^t-1)\mu}}{e^{t\mu(1-\delta)}}.
  \end{align*}
  Want: $e^t - 1 - t(1-\delta) \leq -\frac{\delta^2}{2} \; \forall \delta \in (0,1)$:
  \begin{align*}
    &t = \ln(1-\delta) \\
    &f(\delta) = 1 - \delta - 1 - (1-\delta) \ln(1-\delta) + \frac{\delta^2}{2} \stackrel{?}{\leq} 0 \\
    &f(0) = 0 \\
    &f^{'}(\delta) = - 1 + 1 - \ln(1-\delta) + \delta \stackrel{?}{\leq} 0 \\
    &\frac{2}{3} \delta \leq \ln(1+\delta) \\
    &\ln(1-\delta) \stackrel{?}{\leq} -\delta \checkmark
    % skica
  \end{align*}
  \qed
\end{pro}
$X_i \sim \begin{pmatrix}0 & 1 \\ \frac{1}{2} & \frac{1}{2}\end{pmatrix}$ \\
$X = \sum_{i=1}^{n} X_i$ \\
$\mu = \frac{n}{2}$
\begin{align*}
  P_r(|X-\mu| \geq \sqrt{\frac{3}{2}n \ln(n)}) &= P_r(|X-\mu| \geq \frac{n}{2} \sqrt{\frac{6}{n} \ln(n)}) \\
  &\quad \mu = \frac{n}{2}, \delta = \sqrt{\frac{6}{n} \ln(n)}, \\
  &\quad \text{for \sn{big} }n \delta \in (0,1) \\
  &\stackrel{\text{Chernoff}}{\leq} 2 e^{-\frac{\frac{n}{2} \frac{6}{n} \ln(n)}{3}} = \frac{2}{n}.
\end{align*}
$d = \sqrt{\frac{3}{2}n \ln(n)}$
% skica
\begin{equation*}
  \implies P_r(X \in (\mu - \sqrt{\frac{3}{2}n \ln(n)}, \mu + \sqrt{\frac{3}{2}n \ln(n)})) \geq 1 - \frac{2}{n}.
\end{equation*}
% skica
\begin{claim} \text{} \\
  Let $X_1, X_2 \dots$ independent random variables with image $\{0,1\}$. \\
  $P_r(X_i = 1) = \frac{1}{2} \; \forall i$. \\
  Let $X = \sum_{i=1}^{cm} X_i$ where $c \geq 4$. \\
  Then $P_r(X \leq m) \leq e^{-\frac{cm}{16}}$.
\end{claim}
\begin{pro}
  \begin{align*}
    P_r(X \leq m) &= P_r(\frac{cm}{2} - X \geq \frac{cm}{2} - m) \\
    &= P_r(\frac{cm}{2} - X \geq \frac{cm}{2} (1 - \frac{2}{c})) \\
    &\stackrel{\text{Chernoff}}{\leq} e^{-\frac{\frac{cm}{2} (1-\frac{2}{c})^2}{2}} \\
    &\quad 1-\frac{2}{c} \geq \frac{1}{2} \text{ if } c \geq 4 \\
    &\leq e^{-\frac{\frac{cm}{2} \frac{1}{4}}{2}} = e^{-\frac{cm}{16}}.
  \end{align*}
  \qed
\end{pro}
Back to Quicksort.
\begin{theorem} \text{} \\
  With probability $\geq 1 - \frac{1}{n}$ Quicksort uses at most $48n\ln(n)$ comparisons.
\end{theorem}
\begin{pro} \text{} \\
  % skica (i)
  For $s \in S$ define $S_1^S \dots S_{t_s}^S \neq \emptyset$ sets that include $s$,
  $t_s$ - number of comparisons with $s$ where $s$ is not a pivot $+1$. \\
  Define: iteration $i$ is successful if $|S_{i+1}| \leq \frac{3}{4} |S_i|$ ($\frac{1}{2}$ is too strict).
  \begin{equation*}
    X_i = \begin{cases}
      1 \text{ if iteration } i \text{ is successful} \\
      0 \text{ else}
    \end{cases}
  \end{equation*}
  % skica (i)
  $P_r(X_i = 1) \geq \frac{1}{2}$ \\
  $S_i: n \to \frac{3}{4} n \to (\frac{3}{4})^2 n \to \dots \to 1$. \\
  Notice: max number of iteration is $\log_{\frac{4}{3}}(n) = \frac{\ln(n)}{\ln(4)-\ln(3)}$. \\
  Probability that we haven't succeeded in $\log_{\frac{4}{3}}(n)$ steps:
  \begin{align}
    P_r(\sum_{i=1}^{c \log_{\frac{4}{3}}(n)} X_i < \log_{\frac{4}{3}}(n)) &\leq
      P_r(\sum_{i=1}^{c \log_{\frac{4}{3}}(n)} Y_i < \log_{\frac{4}{3}}(n)) \label{eq:X_to_Y} \\
    &\stackrel{\text{Chernoff}}{<} e^{-\frac{c \log_{\frac{4}{3}}(n)}{24}} \\
    &= e^{-\frac{c \ln(n) \log_{\frac{4}{3}}(e)}{24}} \\
    &= \frac{1}{n} \frac{c \log_{\frac{4}{3}}(e)}{24} \\
    &\quad \log_{\frac{4}{3}}(e) \approx 3.4, \; c=14 \\
    &\leq \left(\frac{1}{n}\right)^2
  \end{align}
  \refeq{eq:X_to_Y} because $X_i$ not independent,
  $Y_i \sim \begin{pmatrix}0 & 1 \\ \frac{1}{2} & \frac{1}{2}\end{pmatrix}$ independent. \\
  $P_r(t_s \geq c \log_{\frac{4}{3}}(n)) \geq \left(\frac{1}{n}\right)^2$ for one $s$. \\
  $c=14 \implies$ at least $48 \ln(n)$ iterations with probability $\leq \left(\frac{1}{n}\right)^2$. \\
  With probability as least $1-\frac{1}{n}$ for all $s \in S$ it holds that $s$ has $\leq 48 \ln(n)$ comparisons with a pivot. \\
  $\implies$ total number of comparisons $n \cdot 48 \ln(n)$ with probability as least $1 - \frac{1}{n}$.
  \qed
\end{pro}



\chapter{Monte Carlo methods}


\section{Example 1}

% skica
Area of circle $= \frac{\pi}{4}$. \\
$X_i = \begin{cases}
  1 \text{ if you hit the area of circle} \\
  0 \text{ else}
\end{cases}$ \\
$P_r(X_i = 1) = \frac{\frac{\pi}{2}}{1} = \frac{\pi}{4}$. \\
$E(X_i) = \frac{\pi}{4}$. \\
$X = \frac{\sum_{i=1}^{n} X_i}{n}$. \\
$E(X) = \frac{n \cdot E(X_i)}{n} = E(X_i)$.


% 4. predavanje: 27.10.

\section{Example 2}

$I = \int_{\Omega} f(x) dx$ - volume. \\
% skica
$X_i = \begin{cases}
  1 \; F(x_i,y_i) \leq z_i \\
  0 \text{ otherwise}
\end{cases}$ \\
$v \cdot E\left(\frac{\sum_{i=1}^{n} X_i}{n}\right) = I$.


\section{$(\epsilon,\delta)$-approximation}

\begin{defn}[$(\epsilon,\delta)$-approximation]
  A random algorithm gives a $(\epsilon,\delta)$-approximation for value $v$ if the output $X$ satisfies:
  \begin{equation*}
    P_r(|X-v| \leq \epsilon v) \geq 1 - \delta.
  \end{equation*}
\end{defn}
\begin{theorem}
  Let $X_1 \dots X_n$ be independent and identically distributed indicator variables.
  Let $\mu = E(X_i), \; Y = \frac{\sum_{i=1}^{m} X_i}{m}$.
  If $m \geq \frac{3 \ln\left(\frac{2}{\delta}\right)}{\epsilon^2 \mu}$, then $P_r(|Y-\mu| \geq \epsilon \mu) \leq \delta$
  $\implies Y$ is $(\epsilon,\delta)$-approximation for $\mu$.
\end{theorem}
\begin{pro} \text{} \\
  $X = \sum_{i=1}^{n} X_i$ \\
  $E(X) = m E(x_i) = m \mu$ \\
  $m \geq \frac{3 \ln\left(\frac{2}{\delta}\right)}{\epsilon^2 \mu}$
  \begin{align*}
    P_r(|Y-\mu| \geq \epsilon \mu) &= P_r(\left|\frac{X}{m}-\mu\right| \geq \epsilon \mu) \\
    &= P_r(\frac{1}{m} \left|X-E(X)\right| \geq \frac{1}{m} \epsilon E(x)) \\
    &\stackrel{\text{Chernoff}}{\leq} 2 e^{-\frac{\epsilon^2 E(x)}{3}} \\
    &= 2 e^{-\frac{\epsilon^2 \mu m}{3}} \\
    &\leq 2 e^{-\frac{\epsilon^2 \mu}{3} \cdot \frac{3 \ln\left(\frac{2}{\delta}\right)}{\epsilon^2 \mu}} = \delta.
  \end{align*}
\end{pro}
Back to example 1: \\
$E(Y) = \frac{\pi}{4}, \delta = \frac{1}{1000}$ ($99.9\%$ sure), $\epsilon = \frac{1}{10000}$ \\
$\implies M = \frac{3 \ln\left(\frac{2}{\frac{1}{1000}}\right) 4}{\pi \left(\frac{1}{10000}\right)^2} \approx 29106$. \\
Problems for MC (Monte-Carlo):
\begin{itemize}
  \item rare events, e.g. $X \sim \begin{pmatrix}0 & 10^{100} \\ 1-10^{-20} & 10^{-20}\end{pmatrix}, \; E(X) = 10^{80}$
    % skica
\end{itemize}


\section{DNF counting}

CNF: $(X_{i_1} \lor \overline{X_{i_2}} \lor X_{i_4}) \land (X_{i_1} \lor \overline{X_{i_3}}) \land \dots$ \\
DNF: $(\overline{X_{i_1}} \land X_{i_2} \lor \overline{X_{i_4}}) \lor \dots$
- easy to determine if solution exists. \\
Question: number of solutions to a given DNF? \\
Observation: CNF $F$ has a solution $\iff$ DNF $\neg F$ has less than $2^n$ solutions, $n$ is number of samples.
\begin{alltt}
  ALG_1(F):
    x = 0
    for i in range(1,m+1):
      \(x_1 \dots x_n\) uniformly random from \{0,1\}\(\sp{n}\)
      if \(F(x_1 \dots x_n)\) = 1:
        x += 1
    return \(\frac{x}{m} \cdot 2\sp{n}\)
\end{alltt}
$Y = \frac{\sum_{i=1}^{m} X_i}{m}$ \\
$(\epsilon,\delta)$-approximation for $Y$ \\
$E(Y) = \frac{\text{number of solutions of }F}{2^n} = \frac{c(F)}{2^n}$ \\
$m \geq \frac{3 \ln\left(\frac{2}{\delta}\right)}{\epsilon^2 E(X)} =
\frac{3 \ln\left(\frac{2}{\delta}\right)}{\epsilon^2} \cdot \frac{2^n}{x(F)}$ \\
$c(F)$ very small $\to$ $m$ exponentially big $\to$ not good (we need a lot of samples).
\begin{defn} \text{} \\
  $SC_i = \{(a_1 \dots a_n) \in \{0,1\}^n \text{ such that } F = F_1 \lor \dots \lor F_t, \; F_i(a_1 \dots a_n) = 1\}$.
  % skica
\end{defn}
$|SC_i| = 2^{n-l_i}$, $l_i$: number of values in $F_i$ \\
$U = \{(i,a) \mid i \in \{1,2 \dots t\}, a \in SC_i\}$ \\
$U = \sum_{i=1}^{t} |SC_i|$ - $O(tn)$ (space smaller than $\{0,1\}^n$) \\
$S = \{(i,a) \in U \mid a \in SC_i, \; a \not \in SC_j \; 1 \leq j < i\}$ \\
$|S| = |SC_1| + \dots + |SC_t| = c(F)$.
\begin{alltt}
  ALG_2(F):
    x = 0
    for i in range(1,m+1):
      (i, a) uniformly random from U (**)
      if (i, a) \(\in\) S: (*)
        x += 1
    return \(\frac{x}{m} \cdot |U|\)
\end{alltt}
$(*) \; a \in SC_i \to O(n), \; a \notin SC_j \; j = 1 \dots i-1 \to O(tn) \; \implies \; O(tn), m$ times. \\
$(**)$: watch for details on how to, e.g. $x_2, x_2 \land x_3$: $x_2$ is more probable than $x_2 \land x_3 \to O(1)$.
\begin{theorem}
  For $m = \lceil\frac{3t \ln(\left(\frac{2}{\delta}\right))}{\epsilon^2}\rceil$ algorithm returns $(\epsilon,\delta)$-approximation
  in $O\left(\frac{t^n n \ln\left(\frac{2}{\delta}\right)}{\epsilon^2}\right)$ time.
\end{theorem} 
\begin{pro}
  $O(t \cdot n \cdot m)$. \\
  Insert $m = ...$
\end{pro}
Prove
\begin{equation*}
  P_r(Y|U| - c(F) > \epsilon c(F)) < \delta:
\end{equation*}
$c(F) = |S|, E(Y) = \frac{|S|}{|U|}$
\begin{align*}
  P_r(Y|U| - c(F) > \epsilon c(F)) &= P_r(|U|(Y - E(Y)) > \epsilon |U| E(Y)) \leq \delta
\end{align*}
if
\begin{equation*}
  m \geq \frac{3 \ln\left(\frac{2}{\delta}\right)}{\epsilon^2 E(Y)} \geq \frac{3 \ln\left(\frac{2}{\delta}\right)t}{\epsilon^2}
\end{equation*}
where
\begin{equation*}
  E(Y) = \frac{|S|}{|U|} \geq \frac{1}{t}
\end{equation*}
($=$ if disjoint). \\
In new space $E(Y)$ much larger $\implies$ $m$ smaller.
% skica



\chapter{Polynomials}


Let $\F$ be a field. \\
$\F$ can be $\R, \C, \Z_p, \F_{p^n}$. \\
$\F[x_1 \dots x_n]$ algebra of polynomials with values $x_1 \dots x_n$. \\
$f \in \F[x_1 \dots x_n]$ \\
$deg(f[x_1 \dots x_n]) := deg(f[x \dots x])$.
\begin{theorem}
  Let $p(x_1 \dots x_n) \in \F[x_1 \dots x_n]$ have the degree $d \geq 0$ and $p \neq 0$.
  Let $s \subset \F$ be finite.
  If $(r_1 \dots r_n)$ is uniformly at random element from $S^n$.
  Then $P_r(p(r_1 \dots r_n) = 0) \leq \frac{d}{|S|}$.
\end{theorem}
\begin{pro}
  Induction on $n$. \\
  $n=1$:
  \begin{align*}
    &p(x) = (x-z_1) (x-z_2) \dots (x-z_j) q(z) \\
    &\text{number of zeros } \leq \text{ degree - fact} \\
    &P_r(p(r_1) = 0) = \frac{\text{number of zeros}}{|S|} \leq \frac{d}{|S|}.
  \end{align*}
  $n-1 \to n$:
  \begin{align*}
    &\text{rewrite }p: \\
    &p(x_1 \dots x_n) = \sum_{i=0}^{j} x^i p_i(x_2 \dots x_n) \\
    &j \leq d \\
    P_r(p(r_1 \dots r_n) = 0) &= P_r(p(r_1 \dots r_n = 0) \mid p_j(r_2 \dots r_n) = 0) \cdot P_r(p_j(r_2 \dots r_n) = 0) \\
    &+ P_r(p(r_1 \dots r_n = 0) \mid p_j(r_2 \dots r_n) \neq 0) \cdot P_r(p_j(r_2 \dots r_n) \neq 0) \\
    &\leq 1 \cdot \frac{d-j}{|S|} + \frac{j}{|S|} \cdot 1, \\
  \end{align*}
  because
  \begin{align*}
    &P_r(p(r_1 \dots r_n = 0) \mid p_j(r_2 \dots r_n) \neq 0) \leq \frac{d-j}{|S|} \\
    &P_r(p_j(r_2 \dots r_n) \neq 0) \leq \frac{j}{|S|}.
  \end{align*}
\end{pro}
\underline{Problem}: \\
Let $A,B,C \in \F^{n \times n}$, is $A \cdot B = C$? \\
Computing $A \cdot B$:
\begin{itemize}
  \item school-book algorithm: $O\left(n^3\right)$,
  \item Strassen algorithm: $O\left(n^{2,807\dots}\right)$,
  \item galactic algorithm: $O\left(n^{2.372\dots}\right)$ - has enormous constants.
\end{itemize}
\begin{alltt}
  RAND_ACB(A,B,C):
    for i in range(1,k+1):
      x uniformly at random from \{0,1\}\(\sp{n}\)
      if \(A \cdot (B \cdot x) \neq x\):
        return false
    return true
\end{alltt}
$O\left(k n^2\right)$. \\


% 5. predavanje: 3.11.

If $A \cdot B = C$, algorithm returns true. \\
If $A \cdot B \neq C$: \\
\begin{align*}
  P_r(ABx = Cx) &= P_r((AB-C)x = 0) \\
  &= P_r(||(AB-C)x||^2 = 0) \stackrel{\text{Poly}}{\leq} \frac{2}{3}.
\end{align*}
$||(AB_C)x||^2$ - polynomial in $x_1 \dots x_n$ of degree $2$. \\
If $A \cdot B \neq C$, then algorithm return false with probability at least $1 - \left(\frac{2}{3}\right)^k$. \\
\underline{Problem}: \\
1-factor in bipartite graphs. \\
% skica
$|V(g)| = 2n$. \\
Represent $G$ with $n \times n$ matrix $Z = (Z_{ij})_{i,j=1}^n$ \\
$Z_{ij} = \begin{cases}
  &X_{ij} \text{ if } a_i b_j \in E(x) \qquad \text{(X: variable)} \\
  &0 \text{ else}
\end{cases}$ \\
\begin{align*}
  det Z(x_{11} \dots x_{nn}) &= \sum_{\pi \in S_n} sign(\pi) z_{1,\pi(1)} \dots z_{n,\pi(n)} \\
  &= \sum_{\pi \in S_n, \pi \text{ defines 1-factor}} sign(\pi) x_{1,\pi(1)} \dots x_{n,\pi(n)}.
\end{align*}
$det Z \neq 0 \iff G$ has 1-factor. \\
\begin{alltt}
  Rand_1factor(G):
    construct Z with variables x11 ... xnn
    for i in range(1,k+1):
      u <- uniformly at random from  {1,2..2n-1}\(\sp{n\sp{2}}\) (r11 ... rnn)
      compuze d = det Z(r11 ... rnn)
      if d != 0:
        return true
    return false
\end{alltt}
Complexity: $k \cdot$ computing determinant: $O\left(n^3\right)$ (Gaussian elimination). \\
or apply approximation algorithm:
\begin{itemize}
  \item if $G$ has no 1-factor it always returns false,
  \item if $G$ has 1-factor, it returns true with probability at least $1-\left(\frac{n}{2n}\right)^k = 1 - \left(\frac{1}{2}\right)^k$
    ($k$ konstant, larger set $\implies$ smaller $k$ needed).
\end{itemize}



\chapter{Random graphs}


\section{G(n,p) model}

$G$ is a random Erdös-Rény graph if it has $n$ vertices and each pair of vertices is connected with probability $p$.
\begin{ex}
  $G\left(5, \frac{1}{2}\right)$.
  % skica
\end{ex}
$E($ edges in $G$ fron $G(n,p)) = \sum_{1 \leq i < j \leq n} E(X_{ij}) = \binom{n}{2} p$. \\
$X_{ij} = \begin{cases}
  1 \text{ if } i \text{ and } j \text{ have edge} \\
  0 \text{ otherwise}
\end{cases}$ \\
$p$ can be function of $n$. \\
$Y_v:$ degree of $v$. \\
$E(Y_v) = (n-1)p$.
\begin{defn} \text{} \\
  We say that a random graph has some property almost surely (A.S.) if
  $P_r(G \in G(n,p) \text{ has property}) \stackrel{n \to \infty}{\to} 1$.
\end{defn}
\begin{claim} \text{} \\
  Let $p$ be constant.
  Then $G \in G(n,p)$ has diameter 2 A.S.
\end{claim}
\begin{pro} \text{} \\
  Let $u,v \in V(G)$ \\
  % skica
  $X_w = \begin{cases}
    1 \text{ if } uw \in E(G) \text{ in } vw \in E(G)
  \end{cases}$ \\
  $P_r(X_w = 1) = p^2$ \\
  $P_r(X_w = 0 \text{ for all } w \neq u,v) = \left(1-p^2\right)^{n-2}$. \\
  $P_r(G \text{ has diameter} > 2)$ \\
  $= P_r(X_w = 0$ for all $w \notin u,v $ for some $u,v)$ \\
  $\leq \binom{n}{2} (1-p^2)^{n-2} \stackrel{n \to \infty}{\to} 0$; \\
  $\binom{n}{2}$ - polynomial, $e^{...}$ - exponent.
\end{pro}
$p = f(n)$ \\
$\frac{1}{n}, \frac{1}{n^3}, \frac{\log n}{n}$
\begin{theorem} (without proof) \\
  Let $p$ be a function of $n$: let $G \in G(n,p)$:
  \begin{itemize}
    \item $np < 1$ - $G$ A.S. disconnected with connected components of size $O(\log n)$
    \item $np = 1$ - $G$ A.S. has $1$ large component of size $O\left(n^{\frac{2}{3}}\right)$
    \item $np = c > 1$ - $G$ A.S. has giant component of size $dn, \; d \in (0,1)$
    \item $np \leq (1-\epsilon) \ln n$ - $G$ A.S. disconnected with isolated vertices
    \item $np > (1-\epsilon) \ln n$ - $G$ A.S. connected.
  \end{itemize}
\end{theorem}
\begin{theorem} \text{} \\
  Let $np = \omega(n) \ln(n)$ for $\omega(n) \to \infty$ \sn{very slowly} think of $\omega(n) = \log (\log n)$,
  then $diam(G)$ in $\Theta\left(\frac{\ln n}{\ln (np)}\right)$ for $G$ in $G(n,p)$.
\end{theorem}
\begin{lemma} \text{} \\
  Let $S \subset V(G), |S| = cn$ for $c \in (0,1]$ and $v \notin S$. \\
  % skica
  then $cnp(1-\omega^{-\frac{1}{3}}) \leq N_S(v) \leq cnp(1+\omega^{-\frac{1}{3}})$ A.S.
  ($\omega^{-\frac{1}{3}} \to 0$ very slowly).
\end{lemma}
\begin{pro}(Lemma): \\
  $E(N_s(v)) = c \cdot n \cdot p, \delta = \omega^{-\frac{1}{3}}$ \\
  \begin{align*}
    P_r(|N_s(v) - cnp| \geq \delta cnp) &\stackrel{\text{Chernoff}}{\leq} 2 e^{-\frac{\omega^{-\frac{2}{3}} cnp}{3}} \\
    &= 2 e^{-\frac{cnp}{3 \omega(n)^{\frac{2}{3}}}} \stackrel{n \to \infty}{\to} 0.
  \end{align*}
  For all $v$: $n \cdot 2 e^{-\frac{cnp}{3 \omega(n)^{\frac{2}{3}}}} \stackrel{n \to \infty}{\to} 0$.
\end{pro}
\begin{pro}(Theorem): \\
  % skica
  $k$ be such that $\sum_{i=0}^{k-1} |N_i| \leq \frac{n}{2}, \sum_{i=0}^{k} |N_i| > \frac{n}{2}$. \\
  $|N_0| = 1$ \\
  $|N_i| \leq |N_{i-1}| \cdot n \cdot p \cdot (1+\omega^{-\frac{1}{3}})$: \\
  $|S| \leq n, \; np(1+\omega^{-\frac{1}{3}})$-each element. \\
  $k = \frac{\log \left(\frac{n}{3}\right)}{\log \left(n \cdot p \cdot \left(1+\omega^{-\frac{1}{3}}\right)\right)} \\
  = \log_{np(1+\omega^{-\frac{1}{3}})} \frac{n}{3} = \Theta\left(\frac{\ln(n)}{\ln(np)}\right)$. \\
  $|N_{\leq k}| = |N_1 \cup \dots \cup N_k|$.
  \begin{align*}
    |N_{\leq k}| &\leq \sum_{i=0}^{k} (np(1+\omega^{-\frac{1}{3}}))^i \\
    &= \frac{(np(1+\omega^{-\frac{1}{3}}))^{k+1}-1}{np(1+\omega^{-\frac{1}{3}}) - 1} \\
    &< \frac{np(1+\omega^{-\frac{1}{3}})^{k+1}}{\frac{1}{2} np(1+\omega^{-\frac{1}{3}})} \\
    &= 2 np(1+\omega^{-\frac{1}{3}})^k \\
    &\stackrel{k}{=} 2 \cdot \frac{n}{3} \text{ haven't covered all} \\
    &\implies diam(G) > k \text{ bound from below}.
  \end{align*}


% 6. predavanje: 10.11.

  % skica
  $N_i \subseteq S$ \\
  $\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right) \cdot |N_{i-1}| \leq |N_i|$ \\
  \begin{align*}
    n & \geq \sum_{i=0}^{k} |N_i| \\
    & \geq \sum_{i=0}^{k} \left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)^i \\
    &= \frac{\left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)^{k+1} - 1}
      {\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right) - 1} \\
    &\geq \left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)^k \qquad / \ln
  \end{align*}
  $\frac{\ln n}{\ln (np)} \approx \frac{\ln n}{\ln \left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)} \geq k$. \\
  $\implies w \in S^{'}$. \\
  Number of neighbors in $N_k$ A.S. $\geq 1$, \\
  $|N_k| \geq \left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)^k \approx c \cdot n$ \\
  $\implies diam (G) = k + 1 $ A.S.
\end{pro}

\subsection{Scale free property}

$G \in G(n,p)$. \\
% skica
In real world:
% skica, skica
$p(k) = $ proportion of degree $k$ vertices. \\
$\log(p(k)) = -\gamma \cdot \log k$ \\
$p(k) = k^{-\gamma}$. \\
Internet: $\gamma \approx 3.42$, \\
protein reactions: $\gamma \approx 2.89$.


\section{Barbási-Albert Model}

B.A. model. \\
Start with $m$ modes. \\
Grow:
\begin{itemize}
  \item add node $v$,
  \item add $m$ edges from $v$ (to $u$),
  \item for each new edge: $P(v \sim u) = \frac{deg u}{\sum_x deg x}$.
\end{itemize}
\begin{theorem} \text{} \\
  B.A. model has scale free property, in particular \\
  $p_k = \frac{2 m (m+1)}{k (k+1) (k+2)}$.
\end{theorem}
\begin{defn} \text{} \\
  $p_n(k)$: expected proportion of degree $k$ vertices in graph with $k$ vertices, \\
  $p_k := \lim_{n \to \infty} p_n(k)$.
\end{defn}
\begin{pro} \text{} \\
  $p_n(k) \cdot n$: expected number of degree $k$ vertices, \\
  $p_n(k) n \cdot \frac{k}{\sum_u deg u} m = p_n(k) \cdot \frac{k}{2}$:
    expected number of degree $k$ vertices changing into degree $k+1$ vertices. \\
  $\sum_u deg u = 2 |E|$ \\
  $p_{n+1}(k) \cdot (n+1) = p_n(k) \cdot n - p_n(k) \cdot \frac{k}{2} + p_n(k-1) \cdot \frac{k-1}{2}$, where \\
  $p_n(k) \cdot n$: degree $k \to k$, \\
  $p_n(k) \cdot \frac{k}{2}: k \to k+1$, \\
  $p_n(k-1) \cdot \frac{k-1}{2}: k-1 \to k$. \\
  For $n$ very big (very close to limit): \\
  $p_n \cdot (n+1) = p_k \cdot n - p_{k-1} \cdot \frac{k}{2} + p_{k-1} \cdot \frac{k-1}{2}$ \\
  $\implies p_k = \frac{k-1}{k+2} p_{k-1}$. \\
  For degree $m$: \\
  $(n+1) \cdot p_{n+1}(m) = p_n(m) \cdot n - p_n(m) \cdot \frac{m}{2} + 1$ %??
  \begin{align*}
    &p_m = \frac{2}{m+2} \\
    \implies &p_{m+1} = \frac{2}{m+2} \cdot \frac{m}{m+3} \\
    \implies &p_{m+2} = \frac{2m(m+1)}{(m+2)(m+3)} \\
    \implies &p_k = \frac{2m(m+1)}{k(k+1)(k+2)}.
  \end{align*}
\end{pro}



\chapter{Markov chains}


$\Omega$: finite set (of states).
\begin{defn}[Markov chain] \text{} \\
  (Discrete time) Markov chain is a sequence of random variables $X = X_0, X_1, X_2 \dots$ with image $\Omega$ and properties:
  \begin{itemize}
    \item $P(X_{i+1} = x \mid X_i = x_i, X_{i-1} = x_{i-1} \dots X_0 = x_0) =\\ P(X_{i+1} = x \mid X_i = x_i)$,
    \item $P{X_{i+1} = x \mid X_i = y} = P(X_1 = x \mid X_0 = y)$ - time is homogenous.
  \end{itemize}
\end{defn}
\begin{ex} \text{} \\
  $\Omega = \Z_5$ \\
  $P(X_{i+1} = x+1 \mid X_i = x) = \frac{1}{2}$ \\
  $P(X_{i+1} = x-1 \mid X_i = x) = \frac{1}{2}$. \\
  % skica
\end{ex}
\begin{defn}[Transition matrix] \text{} \\
  $\Omega = \{x_1 \dots x_n\}$ \\
  $p_{ij} = P(X_{t+1} = j \mid X_t = i)$ \\
  $\begin{bmatrix}
    p_{11} & \dots \\ p_{1n} \\
    \vdots & & \vdots \\
    p_{n1} & \dots & p_{nn}
  \end{bmatrix}$.
\end{defn}
\begin{defn}[Transition graph] \text{} \\ 
  Edge between states $i$ and $j$ exists if $p_{ij} > 0$.
  % skica
\end{defn}
$P$ is stochastic matrix: \\
$p_{ij} \in [0,1]$ \\
$\sum_j p_{ij} = 1$. \\
We choose beginning state randomly. \\
$q(0) = (q_1(0) \dots q_n(0))$ \\
$P(X_0 = i) = q_i(0)$. \\
Let $q(t) = (q_1(t) \dots q_n(t))$ \\
$P(X_t = i) = q_i(t)$. \\
It holds: $q(t) = q(t-1) \cdot P = q(0) \cdot P^t$. \\
$\begin{bmatrix}
  0 & \frac{1}{2} & 0 & 0 & \frac{1}{2} \\
  \frac{1}{2} & 0 & \frac{1}{2} & 0 & 0 \\
  0 & \frac{1}{2} & 0 & \frac{1}{2} & 0 \\
  0 & 0 & \frac{1}{2} & 0 & \frac{1}{2} \\
  \frac{1}{2} & 0 & 0 & 0 & \frac{1}{2}
\end{bmatrix}$ \\
$q(0) = (1, 0, 0, 0, 0)$ \\
$q(1) = (1, \frac{1}{2}, 0, 0, \frac{1}{2})$ \\
$q(2) = (\frac{1}{2}, 0, \frac{1}{4}, \frac{1}{4}, 0)$ \\
$\vdots$ \\
% skica
\begin{defn} \text{} \\
  \begin{itemize}
    \item Distribution $\pi$ is stationary if $\pi = \pi \cdot P$,
    \item $f_{ij}$: probability that $X_t = x_j$ for some $t$ assuming $X_0 = x_i$,
      % skica
    \item $h_{ij}$: expected number of steps needed to get to state $X_j$ strting in $X_i$ (hitting time),
    \item $N(i, t, q(0))$: expected number of times we visit $x_i$ after $t$ steps starting with distribution $q(0)$,
    \item $\forall f_{ij} > 0 \; \iff$ transition graph is strongly connected $\iff$ we say the chain is irreducible,
    \item M.C. is aperiodic if there is no $c \in \{2, 3, 4 \dots\}$ such that all lengths of cycled are divisible by $c$.
  \end{itemize}
\end{defn}
\begin{theorem} \text{} \\
  Let $X$ be finite irreducible M.C. Then:
  \begin{enumerate}[label=\alph*)]
    \item there exists unique stationary distribution $\pi = (\pi_1 \dots \pi_n)$,
    \item $f_{ii} = 1, h_{ii} = \frac{1}{\pi_i}$,
    \item $\lim_{t \to \infty} \frac{N(i, t, q(0))}{t} = \pi_i$ - approaches $\pi$ regardless of $q(0)$,
    \item if $X$ is aperiodic: $\lim_{t \to \infty} q(0) \cdot P^t = \pi$. 
  \end{enumerate}
\end{theorem}


% 7. predavanje: 17.11.

\begin{ex} \text{} \\
  % skica
  $P = \begin{bmatrix}
    0 & \frac{1}{2} & \dots & \frac{1}{2} \\
    \frac{1}{2} & 0 & \dots & 0 \\
    \vdots & & & \vdots \\
    \dots & & \frac{1}{2} & 0
  \end{bmatrix}$ \\
  $\pi = (\frac{1}{n} \dots \frac{1}{n})$ \\
  $h_{i,i} = n$ \\
  $n = h_{i,i} = 1 + \frac{1}{2} h_{i-1,i} + \frac{1}{2} h_{i+1,i}, \quad h_{i-1,i} = h_{i+1,i}$ \\
  $n-1 = h_{i-1,i}$ \\
  $E(\text{steps around}) \leq h_{0,1} + h_{1,2} + \dots + h_{n-1,n} \leq n(n-1)$.
\end{ex}


\section{2-SAT}

Recall: k-SAT:
\begin{align*}
  &F = C_1 \land \dots \land C_m \\
  &C_i = X_{i1} \lor \dots \lor X_{ik}.
\end{align*}
3-SAT: NP complete. \\
\begin{alltt}
  Algorithm:
    def rand2SAT(F):
      \(b\sp{0}\) = \((b_0\sp{0} \dots b_n\sp{0})\)
      for i in range(t):
        if F(\(b\sp{i}\)) = 1:
          return True
        Cl <- clause trat is False
        xj <- uniformly at random from xl1 and xl2
        \(b\sp{i+1}\) = (\(b_0\sp{i} \dots not b_j\sp{i} \dots b_n\sp{i}\))
      if F(\(X\sp{t}\)) = 1:
        return True
      return False
\end{alltt}
\begin{theorem} \text{} \\
  If $k = 8n^2$, then $P(\text{rand2SAT = True} \mid \text{correct answer is True}) \geq \frac{3}{4}$.
\end{theorem}
\begin{pro} \text{}
  Let $a = (a_1 \dots a_n)$ be a correct solution. \\
  Let $X_i =$ Hamming distance from $b^i$ to $a$. \\
  % skica(i)
  Goal: bount $h_{n,0}$. \\
  $P(\text{distance of } b^{i+1} \text{ to $a$ is } j-1 \mid \text{distance of $b^i$ to $a$ is } j) \geq \frac{1}{2}$. \\
  $P = \begin{bmatrix}
    0 & 1 & \dots & 0 \\
    \frac{1}{2} & 0 & \dots & 0 \\
    \vdots & & & \vdots \\
    \dots & & 1 & 0
  \end{bmatrix}$ \\
  $\pi \stackrel{?}{=} \pi P$ \\
  $\pi = (\frac{1}{2n}, \frac{1}{n} \dots \frac{1}{n}, \frac{1}{2n})$ \\
  By theorem \\
  $h_{i,i} = \frac{1}{\pi_i} = n$ for $i = 1, 2 \dots n-1$ \\
  $h_{0,0} = h_{n,n} = 2n$ \\
  $n = h_{i,i} = 1 + \frac{1}{2} h_{i+1,i} + \frac{1}{2} h_{i-1,i}$ \\
  $h_{i+1,i} \leq 2n$ \\
  $i = 0: \; 2n = h_{0,0} = 1 + h_{1,0} \implies h_{1,0} < 2n$ \\
  $h_{n,0} \leq h_{n,n-1} + \dots + h_{1,0} \leq 2n^2$ \\
  $E(\text{steps in algorithm to reach corrce solution}) = E(Z) \leq 2n^2$ \\
  $P(\text{algorithm hasn't reached correct solution after $8n^2$ steps}) \\
  = P(Z > 8n^2) \stackrel{\text{Markov}}{\leq} \frac{E(Z)}{8n^2} \leq \frac{1}{4}$.
\end{pro}


\section{Generating a uniformly random element of a set}

$\Omega$: set. \\
Let $G$ be a symmetric graph on $\Omega$. \\
We form M.C: \\
$P_{x,y} = \begin{cases}
  \frac{1}{M} \text{ if } x \neq y \land x \sim y \\
  0 \text{ if } x \neq y \land x \nsim y \\
  1 - \frac{|N(x)|}{M} \text{ if } x = y
\end{cases}$ \\
$M \geq \max_{v \in \Omega} |N(v)|$. \\
If $G$ is connected $\implies$ M.C. is irrecudible. \\
$\pi = (\frac{1}{|\Omega|} \dots \frac{1}{|\Omega|})$ \\
$\pi \stackrel{?}{=} \pi P$ \\
\begin{align*}
  (\pi P)_x &= \sum_y \pi_y P_{y,x} \\
  &= \sum_{y \in N(x)} \frac{1}{M} \cdot \frac{1}{|\Omega|} + \frac{1}{|\Omega|}
    \left(1 - \frac{|N(x)|}{M}\right) = \frac{1}{|\Omega|} = \pi_x.
\end{align*}
$\implies$ if we walk on the Markov chain long enough, we end up in state $x$ with probability $\pi_x = \frac{1}{|\Omega|}$ \\
$\implies$ we can sample uniformly.
\begin{ex} \text{} \\
  $G$ graph, finding largest independent set $(\forall u,v: u \nsim v)$ is NP-complete. \\
  Lets try sampling a uniformly random independent set \\
  $\Omega = \{\text{independent sets}\}$ \\
  $u \sim v$ if $|u \triangle v| = 1$ $((u \cup \{el\}) = v)$ \\
  M.C.: $X_0$ = arbitrary independent set \\
  $X_{i+1}$:
  \begin{itemize}
    \item pick uniformly at random $v \in V(G)$,
    \item if $v \in U$ then $X_{i+1} = U \textbackslash \{v\}$,
    \item if $U \cup \{v\}$ is independent then $X_{i+1} = U \cup \{v\}$,
    \item else $X_{i+1} = U$.
  \end{itemize}
  $M$ is number of vertices \\
  $\implies \; \forall u \in \Omega: \lim_{t \to \infty} P(X_t = u) = \frac{1}{|\Omega|}$. \\
  Note: irredudicle; $U \to \emptyset \to V$, aperiodic.
\end{ex}


\section{Metropolis algorithm}

$\Omega$: set, \\
$\pi$: chosen distribution on $\Omega$. \\
Make $G$ graph on $\Omega$ \\
$P_{x,y} = \begin{cases}
  \frac{1}{M} \cdot \min \left(1, \frac{\pi_y}{\pi_x}\right) \text{ if } x \neq y \land x \sim y \\
  0 \text{ if } x \neq y \land x \nsim y \\
  1 - \sum_{y \in N(x)} \text{ if } x = y
\end{cases}$ \\
$M \geq \max_{v \in \Omega} |N(v)|$ \\
$\pi \stackrel{?}{=} \pi P$ \\
\begin{align*}
  (\pi P)_x &= \sum_y \pi_y P_{y,x} =
    \sum_{y \in N(x)} \pi_y \frac{1}{M} \min \left(\left(1, \frac{\pi_y}{\pi_x}\right)\right)
    + \pi_x \left(1 - \sum_{y \in N(x)} \frac{1}{M} \min \left(1, \frac{\pi_y}{\pi_x}\right)\right) \\
  &= \sum_{y \in N(x), \pi_y \geq \pi_x} \pi_y \frac{1}{M} \cdot 1 +
    \sum_{y \in N(x), \pi_y < \pi_x} \pi_y \frac{1}{M} \frac{\pi_y}{\pi_x} + \pi_x \\
  &- \sum_{y \in N(x), \pi_y \geq \pi_x} \pi_x \frac{1}{M} \frac{\pi_y}{\pi_x} -
    \sum_{y \in N(x), \pi_y < \pi_x} \frac{1}{M} \cdot 1 \\
  &= \pi_x.
\end{align*}
\begin{ex} \text{} \\
  $\Omega = \Z \cap [-1000,1000]$ \\
  $\pi \sim e^{-\frac{(x-\mu)^2}{2 \delta}}$ \\
  % skica
  \begin{alltt}
    \(X\sb{0}\) arbitrary
    for i = in range(1,m):
      y <- uniformly from {\(X\sb{i}\)+1,\(X\sb{i}\)-1}
      M <- uniformly from [0,1]
      if \(M \leq \frac{\pi(y)}{\pi(x)}\):
        \(X\sb{i+1}\) = y
      else:
        \(X\sb{i+1}\) = \(X\sb{i}\)
    return \(X\sb{m}\)
  \end{alltt}
\end{ex}
\begin{ex} \text{} \\
  Find maximum of a positive function $f$. \\
  Use metropolis algorithm to sample proportional to $f$. \\
  Note: all I need to know is ratios $\frac{f(y)}{f(x)}$.
\end{ex}


% 8. predavanje: 24.11.

Back to independent sets. \\
$G = (V, E)$ \\
$\Omega$ = independent sets. \\
$\lambda \in (1, \infty)$ \\
$\pi(u) \sim \lambda^{|u|}$ \\
$\pi(u) = \frac{\lambda^{|u|}}{\sum_{v \text{ independent set}} \lambda^{|v|}}$. \\
How to calculate the sum? \\
No problem: only need proportions. \\
$X_0$: arbitrary independent set. \\
$X_i \to X_{i+1}$:
\begin{itemize}
  \item we pick $v \in V$ uniformly at random,
  \item if $v \in X_i \implies$
    \begin{itemize}
      \item $X_{i+1} = X_i \setminus \{v\}$ qith probability $\frac{1}{\lambda} = \min \{1, \frac{\pi_y}{\pi_x}\}$,
      \item $X_{i+1} = X_i$ with probability $1 - \frac{1}{\lambda}$,
    \end{itemize}
  \item if $v \in X_j$ and $X_i \cup \{v\}$ is independent $\implies \; X_{i+1} = X_i \cup \{v\}$,
  \item otherwise $X_{i+1} = X_i$.
\end{itemize}
\begin{ex} \text{}
  Bayes: $P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B \mid A) P(A)}{P(B)}$. \\
  $B \leftarrow$ machine is giving values, e.g. $y_1 = 0.05, y_2 = -0.1, y_3 = 0.07, y_4 = 3$. \\
  We believe $B \sim N(\mu, 0,05)$. \\
  $\mu = laplacian(0, 0.01)$. \\
  % skica
  $P(\mu \mid B) = \frac{e^{\frac{|\mu|}{0.01}} e^{-\sum \frac{(x_i-\mu)^2}{0.05}}}{\int \dots}$. \\
  Integral is difficult to calculate. \\
  Sample $\mu$ with Metropolis algorithm.
  % skica
\end{ex}


\section{M.C. for 1-factor in bipartite graphs}

$G$ regular graph \\
% skica
$|A| = |B|$. \\
How to find $1$-factor? \\
Augmenting paths. \\
Let $M$ be (suboptimal) matching. \\
% skica
If we find $s-t$ path, we switch edges and get bigger matching. \\
Starting point. \\
$G \; d$-regular graph. \\
Graph $G = (A \cup B, E)$, $M$ suboptimal matching.
\begin{itemize}
  \item Add $s$ and add directed edges to vertices in $A$ that are not matched with weight $d$,
  \item add $t$ and add directed edges to vertices in $B$ that are not matched with weight $d$,
  \item orient edges in $M$ from $B$ to $A$ that weight $d-1$,
  \item orient edges in $E \setminus M$ from $B$ to $A$ that weight $1$,
  \item we add edge from $t$ to $s$ that weight $(|A| - |M|) d$.
\end{itemize}
% skica
Observation:
\begin{itemize}
  \item for each vertex $x: \; deg^{-}(x) = deg^{+}(x)$ (out weights = in weights),
  \item if $|A| > |M|$, then graph is eulerian $\implies$ there is an augmenting path.
\end{itemize}
How to find $s-t$ path? \\
Do a random walk. \\
Expected time to get from $s$ to $t$ is $h_{s,t}$ \\
$\frac{1}{\pi(s)} = h_{s,s} = h_{s,t} + 1$.
\begin{lemma} \text{} \\
  Let $X$ be a M.C. defined as a random walk on directed (weighted) graph with $deg^{-}(x) = deg^{+}(x)$ for each $x$.
  Then the stationary distribution is $\pi = \left[\frac{deg^{+}(x_i)}{|E|}\right]_{i=1}^n$.
\end{lemma}
$w_{ij}$: weight from $i$ to $j$.
\begin{pro} \text{} \\
  $\pi P = \pi \left[\frac{w_{ij}}{deg^{+}(x_i)}\right]_{i,j=1}^n = \left[\frac{\sum_j w_ji}{|E|}\right]_{i=1}^n
  = \left[\frac{deg^{-}(x_i)}{|E|}\right]_{i=1}^n = \left[\frac{deg^{+}(x_i)}{|E|}\right]_{i=1}^n$.
\end{pro}
$h_{s,s} = \frac{1}{\pi_s} \leq \frac{|E|}{deg^{+}(s)} 
\leq \frac{3(|A| - |M|) d + |M| (d-1) + (|A| - |M|) d + |M| (d-1)}{(|A| - |M|) d} \leq \frac{4 |A|}{|A| - |M|}$. \\
Expected time to find augmenting path $\leq \frac{4 |A|}{|A| - |M|}$. \\
$|A| = n$ \\
Expected time to find 1-factor $\leq \frac{4n}{n-1} = 4n \sum_{i=1}^{n-1} \frac{1}{i} \leq 4n (1 + \ln n)$ - in $O(n \log n)$.

\subsection{Network centrality}

% skica
Degree as measure - natural idea. \\
Use M.C: walk randomly on the network, those that are visited more oftenly are more important. \\
Pagerank. \\
Let $A$ be the adjacency matrix of $G$. \\
$P_{ij} = \alpha \frac{A_{ij}}{deg i} + (1-\alpha) \frac{1}{n}$; \\
$\alpha$: normal random walk, \\
$1-\alpha$: jump to any. \\
$\alpha = 0.85$.



\chapter{Randomized incremented constructions (RIC)}


Observation: \\
Let $S$ be a set of $n$ distinct elements. \\
Let $X_1 \dots X_n$ be a random permutation of the elements. \\
Let $S_i = \{X_1 \dots X_i\}$. \\
$P(X_i = \min (S_i)) = \frac{1}{i}$. \\
$Y = \left| \{j \in \{1 \dots n\} \mid j = \text{ minimal of } S_j\} \right|$ \\
$Y = Y_1 + \dots + Y_n$ \\
$Y_j = \begin{cases}
  1 \text{ if } i = \min S_i \\
  0 \text{ otherwise}
\end{cases}$ \\
$E(Y) = \sum_{i=1}^{n} E(Y_i) = \sum_{i=1}^{n} \frac{1}{i}$ in $O(\log n)$.
\begin{alltt}
  Alg():
    X1 .. Xn = random permutation of S
    min = X1
    for i in range(1,n+1):
      if Xi < min:
        print("HA")
        min = Xi
\end{alltt}
We get $O(\log n)$ \sn{HA} printed. \\
Incremental construction (IC). \\
Input $S = \{s_1 \dots s_n\}$. \\
We will build structures $DS(S_i)$: \\
$DS(S_1 \to \dots \to DS(S_n))$. \\
$DS(S_n)$ will help us give answer. \\
Randomized: permute $S$ at the beginning.


\section{Quicksort as RIC}

$S$: set of elements we want to order. \\
$X_1 \dots X_n$: random permutation of $S$. \\
$S_i = \{X_1 \dots X_i\}$. \\
$S_i$ splits $\R$. \\
% skica
Define $DS(S_i)$:
\begin{itemize}
  \item save intervals: each interval will be saved by endpoints,
  \item for each interval we will be saving its points,
  \item for each $X_j$, $j > i$ we will save in which interval it is,
  \item for each left point of the interval we will save the right point.
\end{itemize}
\begin{alltt}
  QuicksortRIC(S):
    # start of DS(Si)
    I=[(-\(\infty\),\(\infty\))]
    P[(-\(\infty\),\(\infty\))] = S
    for each Xi:
      Int(Xi) = (-\(\infty\),\(\infty\))
    Next(\(\infty\)) = \(\infty\)
    # end of DS(Si)
    for i in range(1,n+1):
      Ii = Int(Xi) = (Xj,Xk)
      Ii1 = (Xj,Xi)
      Ii2 = (Xi,Xk)
      for Xl \(\neq\) Xi, Xi \(\in\) P(I): 
        add Xl to P(Ii1) or P(Ii2) depending on Xl < Xi or Xl > Xi
      Next(Xj) = Xi
      Next(Xi) = Xk
    return [Next(\(-\infty\)), Next(Next(\(-\infty\))) ..]
\end{alltt}
Similarity to quicksort: spliting intervals.


% 9. predavanje: 1.12.

% skica
Analysis: \\
for set $i$, we need $O\left(|P(I_i)|\right)$, \\
$E\left(|P(I_i)|\right) = ?$ \\
e.g. \\
if $x_4 = a_4$: \\
% skica
if $x_4 = a_2$: \\
% skica
$P(X_i = a_j) = \frac{1}{i} \; j \in \{1, 2 \dots i\}$. \\
Expected value of steps in iteration $i$ \\
$\sum_{j=1}^{i} \frac{1}{i} \left(P\left((a_{j-1},a_j)\right) + P\left((a_j,a_{j+1})\right)\right)
\leq \frac{1}{i} 2 (n-i) \leq \frac{2n}{i}$ \\
\begin{align*}
  E \left(\text{number of steps in QuicksortRIC}\right)
  &\leq \sum_{i=1}^{n} \frac{2n}{i} \\
  &\leq 2n (1 + \log n) \quad \to \text{ in } O(n \log n).
\end{align*}

\section{Linear programming}

Task: maximize $f(x_1 \dots x_n) = c_1 x_1 + \dots + c_d x_d$. \\
Constraints:
\begin{align*}
  &a_{11} x_1 + \dots + a_{1d} x_d \leq b_1 \\
  &\vdots \\
  &a_{n1} x_1 + \dots + a_{nd} x_d \leq b_n. \\
\end{align*}
Geometric interpretation. \\
% skica
Cases:
\begin{itemize}
  \item infeasible region
    % skica
  \item unbounded
    % skica
  \item .
    % skica
\end{itemize}
Alg:
\begin{itemize}
  \item symplex algorithm worst case $O\left(2^n\right),$
  \item interior point method (polynomial algorithm).
\end{itemize}
Seidel's algorithm: \\
running in expected $O(n)$ time when $d$ is constant. \\
One dimension.
\begin{align*}
  \max \; &c x \\
  &a_1 x \leq b_1 \\
  &\vdots \\
  &a_n x \leq b_n,
\end{align*}
where $n$ is number of constraints. \\
% skica
\begin{itemize}
  \item $a_i$ positive: $(-\infty, \frac{b_i}{a_i}]$,
  \item $a_i$ negative: $[\frac{b_i}{a_i}, \infty)$.
\end{itemize}
$a_i \neq 0$. \\
Alg:
\begin{itemize}[label={}]
  \item $R = \min_i \{\frac{b_i}{a_i}; a_i > 0\}$,
  \item $L = \max_i \{\frac{b_i}{a_i}; a_i < 0\}$,
  \item if $L > R$: program infeasible,
  \item else:
    \begin{itemize}[label={}]
      \item if $c > 0$: return $R$,
      \item if $c < 0$: return $L$.
    \end{itemize}
\end{itemize}
2-dim: assume general position.
\begin{align*}
  \max \; &c_1 x + c_2 y \\
  &a_{11} x + a_{12} y \leq b_1 \\
  &\vdots \\
  &a_{n1} x + a_{n2} y \leq b_n \\
  &x \leq M \text{ or } x \geq -M \\
  &y \leq M \text{ or } y \geq -M.
\end{align*}
$\leq, \geq$ depending on $c_1, c_2$. \\
% skica
Notation:
\begin{itemize}[label={}]
  \item $h_i$: halfspace defined by $a_{i1} x + a_{i2} y \leq b_i$,
  \item $m_i$: added halfspaces,
  \item $l_i$: line that bounds.
\end{itemize}
Alg:
\begin{itemize}
  \item first randomly permute $h_i$,
  \item $H_i = \{m_1, m_2, h_1 \dots h_i\}$,
  \item $v_i \in \cap H_i$ optimal solution after $i$ constraints,
  \item $v_0 = (\pm M, \pm M)$,
  \item inductively add $h_i$.
    % skica
\end{itemize}
Cases:
\begin{itemize}[label={}]
  \item if $v_{i-1} \in h_i \; \implies \; v_i = v_{i-1}$,
  \item if $v_{i-1} \notin h_i \; \implies \; v_i \in h_i$:
    \begin{itemize}[label={}]
      \item $a_{i1} x + a_{i2} y = b_i$
      \item $a_{i1}$ or $a_{i2} \neq 0$, e.g. $a_{i1}$;
      \item $x = \frac{b_i - a_{i2} y}{a_{i1}}$.
    \end{itemize}
\end{itemize}
Insert $x$ in all constraints $\implies$ linear program in 1-dim, i (i-1?) constraints
$\implies$ get $v_i$ in $O(i)$. \\
Analysis:
\begin{itemize}
  \item worst case: $\sum_{i=1}^{n} O(i) = O(n^2)$,
  \item expected: $E(X) = \sum_{i=1}^{n} E(X_i)$,
  \item $X_i =$ running time of $i$-th iteration,
  \item $X_i = \begin{cases}O(1); \text{ case 1} \\ O(i); \text{ case 2}\end{cases}$
  \item $P(\text{case 2}) \leq \frac{2}{i}$ - optimal point on at most $2$ lines,
  \item $E(X) \leq \sum_{i=1}^{n} O(1) \cdot 1 + O(i) \cdot \frac{2}{i} = O(n)$.
\end{itemize}
$d$-dim
\begin{itemize}
  \item constraints define half-spaces,
  \item boundary is hyperplane ($d-1$ dimensional),
  \item general position: intersection of $d-i$ hyperplanes is $i$ dimensional,
    intersection of $d+1$ hyperplanes is $\emptyset$.
\end{itemize}
Alg:
\begin{itemize}[label={}]
  \item first add $X_i \leq M$ or $X_i \geq -M$ depemding on $c_i$,
  \item random permutation ($h_1 \dots h_n$),
  \item $H_i = \{m_1 \dots m_d, h_1 \dots h_i\}$,
  \item $v_0 \in \cap \partial m_i$,
  \item inductively add $h_i$:
    \begin{itemize}[label={}]
      \item $v_{i-1} \in h_i \; \implies \; v_i = v_{i-1}$,
      \item $v_{i-1} \notin h_i \; \implies$
        we need to solve LP in $d-1$ dimensions with $i$ constraints ($O(i)$ expected),
      \item $P(v_{i-1} \notin h_i) \leq \frac{d}{i}$,
    \end{itemize}
  \item $E(X) \leq \sum_{i=1}^{n} O(1) + \frac{d}{i} O(i) = O(n)$.
\end{itemize}
$X$: running time. \\
Careful implementation runs in $O(d! \, n) \; \implies$ very useful for low dimensions. \\
Problem: let $P$ be convex polygon given by ordered set of vertices \\
% skica
$y = a_i x + b_i$. \\
Find largest disc embeddable in $P$. \\
Input: $P_1 \dots P_n$, \\
output: $(s_1, s_2), r$.
\begin{align*}
  \max \; &r \\
  &d = \left| \frac{s_2 - a_i s_1 - b_i}{\sqrt{a_i^2 + 1}}\right| \\
  &\frac{s_2 - a_i s_1 - b_i}{\sqrt{a_i^2 + 1}} \geq r \text{ - line above } P \\
  &-\frac{s_2 - a_i s_1 - b_i}{\sqrt{a_i^2 + 1}} \leq -r \text{ - line below } P
\end{align*}
$\implies$ LP in 3 dim. \\
Note: $\frac{s_2 - a_i s_1 - b_i}{\sqrt{a_i^2 + 1}}$ positive if $(s_1, s_2)$ above the line,
negative otherwise.



\chapter{Hashing}


A hash function is a randon function, \\
$h: U \to \{0, 1 \dots n-1\} = M$, \\
$U$ - universe, \\
$u = |U|$, \\
$m = |M|$. \\
Ideally we would like for $h$ to be as completely rondom: $P(h(x) = t) = \frac{1}{m}$. \\
Standard application. \\
Let $V \subset U, \; |V| << |U|$. \\
We would like to quickly answer if $x \in V$ for ever $x \in V$. \\
Solution:
\begin{itemize}
  \item take $h: U \to M$,
  \item make a table $T = [0, 1 \dots n-1]$,
  \item for $v \in V$:
    \begin{itemize}[label={}]
      \item $T[h(v)] = 1$,
      \item $T[y] = 0 \; \forall y \in h(V)$.
    \end{itemize}
  \item Let $x \in V$. Check
    \begin{itemize}
      \item if $T[h(x)] = 1: \; x \in V$,
      \item else: $x \notin V$.
    \end{itemize}
\end{itemize}
Note: this is not OK: $h$ not injective.


% 10. predavanje: 8.12.

For $x \in U$, tell if $x \in V$ in $O(1)$. \\
$h = \text{SHA256}: U \to \{0, 1\}^{256}$. \\
Approach:
\begin{itemize}
  \item design a family of hash functions,
  \item study collisions $P_h(h(x) = h(y))$,
  \item $H$ meeds to be \sn{simple}.
\end{itemize}
Bad example: $H$ = all functions from $U$ to $M$ storing $h \in H$ would take $|U| \log_2 |M|$ bits.
\begin{defn}
  A family of hash functions to be universal if for \\
  $\forall x, y \in U, x \neq y, h \in H: \; P(h(x) = h(y)) \leq \frac{1}{m}$ (probability of collision).
\end{defn}
k-independent if $\forall x_1 \dots x_k \in U$ pairwise different, $\forall t_1 \dots t_k \in M \;
P_r(h(x_i) = t_i \; \forall i) \leq \frac{1}{m^k}$.
\begin{ex} \text{} \\
  $U = \{0, 1, 2, 3\}$, \\
  $M = \{0, 1\}$, \\
  $H = \{h_0, h_1, h_2\}$, \\
  $h_0: \{0 \to 0, 1 \to 0, 2 \to 1, 3 \to 1\}$, \\
  $h_1: \{0 \to 0, 1 \to 1, 2 \to 0, 3 \to 1\}$, \\
  $h_2: \{0 \to 0, 1 \to 1, 2 \to 1, 3 \to 0\}$. \\
  $P(h(0) = h(2)) = \frac{2}{3} < \frac{1}{2}$ - not universal.
\end{ex}
Why universal? \\
$U, V, M$ \\
$H$ universal: $\forall x, y: \; P(h(x) = h(y)) \leq \frac{1}{m}$. \\
$X$: number of collisions of $V$. \\
$E(X) = E\left(\sum_{x,y \in V, x \neq y} X_{x,y}\right)$ \\
$X_{x,y} = \begin{cases}
  1 \text{ if } h(x) = h(y) \\
  0 \text{ else}
\end{cases}$ \\
$E(X) = \sum_{x,y \in V, x \neq y} E(X_{x,y}) \leq \binom{n}{2} \cdot \frac{1}{n}$. \\
$U, V, M, H$ \\
$T[0 \dots m-1]$ \\
$\forall v \in V$ \\
$T[h(v)] = v$. \\
For $x \in V$ we check $T[h(x)]$ if equals $x$, \\
for $y \in U \setminus V$, $T[h(y)] \neq y$. \\
For $z \in V$, $T[h(z)]$ can happen $\neq z$ if $h$ has collisions in $V$.
\begin{lemma}
  Let $m \geq n^2$ and $H$ universal.
  Then the probability that $h$ has no collisions in $V \; \geq \frac{1}{2}$.
\end{lemma}
\begin{pro} \text{} \\
  $X$: number of collisions \\
  $E(X) \leq \binom{n}{2} \cdot \frac{1}{m} < \frac{n^2}{2} \cdot \frac{1}{n^2} = \frac{1}{2}$ \\
  $P(X \geq 1) \stackrel{\text{Markov}}{\leq} \frac{E(X)}{1} = \frac{1}{2}$ \\
  $P(X = 0) \geq \frac{1}{2}$.
\end{pro}
\begin{ex}[Universal hash family] \text{} \\
  $U = \{0, 1 \dots u-1\}$ (bits $\equiv$ numbers) \\
  $M = \{0, 1 \dots m-1\}$. \\
  Define: let $p \geq u$, $p$ prime number. \\
  Define for $a, b \in \Z_p, \; a \neq 0$. \\
  $h_{a,b} = (ax + b) \mod m$ \\
  $ax + b \in \Z_p$ \\
  $H = \{h_{a,b} \mid a, b \in \Z_p, \; a \neq 0\}$.
\end{ex}
\begin{pro}
  $P(h_{a,b}(x) = h_{a,b}(y)) = ?$ \\
  $x, y$ fixed. \\
  For any $a, b$ denote \\
  $ax + b = t_x \\
  ay + b = t_y: \\
  a \sqcup + b \in \Z_p$. \\
  $\begin{bmatrix}x & 1 \\ y & 1\end{bmatrix} \begin{bmatrix}a \\b\end{bmatrix} =
  \begin{bmatrix}t_x \\ t_y\end{bmatrix}$ \\
  $\det \begin{bmatrix}x & 1 \\ y & 1\end{bmatrix} \neq 0$, because $x \neq y$ \\
  $\begin{bmatrix}a \\b\end{bmatrix} = \begin{bmatrix}x & 1 \\ y & 1\end{bmatrix}^{-1}
  \begin{bmatrix}t_x \\ t_y\end{bmatrix}$. \\
  For each $t_x, t_y$ there exists 1 $a, b$ mapping to $t_x, t_y$. \\
  $h_{a,b}(x) = h_{a,b}(y) \; \iff \; t_x = t_y \mod m$. \\
  This holds for $p\left(\lceil\frac{p}{m}\rceil + 1\right)$ \\
  $p$: choise of $t_y$ \\
  $t_x = t_y + km$ \\
  $P\left(h_{a,b}(x) = h_{a,b}(y)\right) \leq \frac{p\left(\lceil\frac{p}{m}\rceil-1\right)}{p(p-1)}
  \leq \frac{\frac{p-1}{m}}{p-1} = \frac{1}{m}$.
\end{pro}
Function random for $2$ elements, fixed for $\geq 3$. \\
Higher k-independent: better.

\section{Chaining}

$V, U, h: U \to V$. \\
Answer $x \in V$ in $O(1)$. \\
$T[0 \dots m-1]$ \\
% skica
$n = |V|$ \\
$\forall v \in V$: \\
$h(v_1) = h(v_2) \; \to \; [v_1 \; v_2 \dots]$ - linked list. \\
Now: \\
$x \in U$. \\
Check if $x$ is in list at $T[h(x)]$. \\
Check takes $O(\text{length of a list at }h(x))$ = $1 +$ number of collisions with $x$. \\
$X_x$: number of collisions with $x$. \\
$E(X_x) = \sum_{y \in V} E(X_{x,y}) \leq n \cdot \frac{1}{m}$ if hash function is universal. \\
$\alpha = \frac{n}{m}$: load factory (how many elements in $1$ place). \\
$E(X_x) = 1$ \\
$E(\max_x X_x) \neq \max_x E(X_x) = 1$.
\begin{theorem}
  Assume we throw $n$ balls into $n$ bins uniformly at random.
  Then with high probability the fullest contains
  $\theta\left(\frac{\log n}{\log (\log n)}\right)$ balls.
\end{theorem}
\begin{pro} \text{} \\
  $\stackrel{?}{\leq} \frac{3 \ln n}{\ln \ln n}$. \\
  Let $X_j$ be the number of balls in bin $j$. \\
  $P\left(X_j \geq \frac{3 \ln n}{\ln \ln n}\right) = P($ there exists subset $S$ of balls thrown to bin$j)$. \\
  $|S| = k$
  \begin{align*}
    &P\left(\cup_{S \text{ balls}, |S|=k} \text{balls from $S$ are thrown to bin }j\right) \\
    &\leq \sum_{S \text{ balls}, |S|=k} P(\text{balls from $S$ are thrown to }j) \\
    &= \binom{n}{k} \left(\frac{1}{n}\right)^k \\
    &\leq \frac{n^k}{k!} \cdot \frac{1}{n^k} = \frac{1}{k!} = (*).
  \end{align*}
  Note: $e^x = \sum_{i=1}^{\infty} \frac{k^i}{i!} \geq \frac{k^k}{k!}$.
  \begin{align*}
    (*)& \leq \frac{e^k}{k^k} \\
    &= \left(\frac{e \ln n}{3 \ln \ln n}\right)^{\frac{3 \ln n}{\ln \ln n}} \\
    &\leq e^{\frac{3 \ln n}{\ln \ln n} \cdot \left(\ln \ln \ln n - \ln \ln n\right)} \\
    &= e^{-3 \ln n + \frac{\ln \ln \ln n \cdot (\ln n \cdot 3)}{\ln \ln n}} = (**)
  \end{align*}
  $\frac{\ln \ln \ln n}{\ln \ln n} \to 0$
  \begin{equation*}
    (**) \leq e^{-3 \ln n + \ln n} = \frac{1}{n^2}.
  \end{equation*}
  $P(\text{at least for $1$ bin }j \geq k) = n \cdot \frac{1}{n^2} = \frac{1}{n}$.
\end{pro}


% 11. predavanje: 15.12.

$U, V$, $H$ hash family, $h: U \to M$ \\
$v \in V$ \\
$n = |V|$ \\
max load $O\left(\frac{\log n}{\log (\log n)}\right)$. \\
Perfect hashing: we would like
\begin{itemize}
  \item $O(1)$ lookup (worst case)
  \item $O(n)$ size of table.
\end{itemize}


\section{2 level hashing}

Input: $V$ \\
$n = |V|$. \\
% skica
Take hash function from universal family with $m = |M| = n$. \\
Count total collisions $X$. \\
$E(X) \leq \binom{n}{2} \cdot \frac{1}{m} \leq \frac{n}{2}$ \\
$P(x \geq n) \stackrel{\text{Markov}}{\leq} \frac{1}{2}$ \\
$\implies$ by repeating sample $h$ we can guarantee
\begin{itemize}
  \item for each $i \in M$ we store at $T[i]$ another hash table of size $C_i^2$,
    where $C_i = $ number of elements of $V$, hashed in $i$,
  \item we sample $h_i$ from universal hash family with $M_i = C_i^2$.
\end{itemize}
$P(h_i \text{ has no collisions}) \geq \frac{1}{2}$ (by lemma). \\
We resample if $h_i$ has collisions. \\
$E(\text{sampling } h_i) = 2$. \\
Construction time: \\
\begin{itemize}
  \item step 1: $O(n)$
  \item step 2: $O(C_1 + \cdots + C_n) = O(n)$;
\end{itemize}
together $O(n)$. \\
Lookup time: $O(1)$ (evaluating $h(x)$ and $h_{h(x)}(x)$). \\
Space: $O(C_1^2 + \dots + C_n^2)$ in $O(n)$. \\
By first step $n >$ number of collisions of $h = \sum_{i=1}^{n} \binom{C_i}{2} = \sum_{i=1}^{n} \frac{C_i^2 - C_i}{2}$ \\
$\implies \sum_{i=1}^{n} C_i^2 < 2n + \sum_{i=1}^{n} c_i = 3n$.


\section{The power of 2 choices}

Variant: placing $n$ balls in $n$ bins but for each ball we choose $d$ balls uniformly at random and
put the ball in bin with minimal load.
\begin{theorem}
  The above process with $d \geq 2$ results in at most maximum load of $O\left(\frac{\ln (\ln n)}{\ln d}\right)$.
\end{theorem}
\begin{pro}(sketch). \\
  $b_i$ = upper bound of the number of bins with load at most $i$. \\
  Height of a ball = the number of balls in the bin, where the ball is placed. \\
  $P(\text{a ball has height at least } i+1) \leq \left(\frac{b_i}{n}\right)^d$ (choose $d$ times independently). \\
  $X^{i+1}$: number of balls with height $ \geq i+1$. \\
  $X^{i+1} = \sum_{j=1}^{n} X_j^{i+1}$ \\
  $X_j^{i+1}$: indicator variable of $j$-th ball having height $i+1$. \\
  $E(X^{i+1}) \leq \sum_{j=1}^{n} \left(\frac{b_i}{n}\right)^d = d \cdot \left(\frac{b_i}{n}\right)^d$. \\
  Chernoff bound: with high probability $X^{i+1} \leq 2 n \left(\frac{b_i}{n}\right)^d$. \\
  $X^{i+1} \geq$ number of bins with load at least $i+1$. \\
  Define (set) \\
  $b_{i+1} = \frac{\sum b_i^d}{n^{d-1}}$ \\
  $b_4 = \frac{n}{4}$ \\
  $b_{i+4} = \frac{n}{2^{2 \cdot d^i - \sum_{j=0}^{i-1} d^j}}$
  \begin{itemize}[label={}]
    \item $i=0$: $b_4 = \frac{n}{2^{2^1}} = \frac{n}{4}$
    \item $i \to i+1$:
      \begin{align*}
        b_{i+4} &= \frac{2 \cdot b_{i+3}}{n^{d-1}} \\
        &\stackrel{IH}{=} \frac{2 \cdot \left(\frac{n}{2^{2 \cdot d^i - \sum_{j=0}^{i-1} d^j}}\right)^d}{n^{d-1}} \\
        &= \frac{2^1 \cdot n^d}{n^{d-1} \cdot 2^{2 \cdot d^{i+1} - \sum_{j=1}^{i} d^j}} \\
        &= \frac{n}{2^{2 \cdot d^{i+1} - \sum_{j=0}^{i} d^j}}.
      \end{align*}
  \end{itemize}
  In particular: $b_{i+4} \leq \frac{n}{2^{d^i}} < 1$ when?
  \begin{align*}
    n &< 2^{d^i} \\
    \log_2 n &< d^i \\
    \log_d \log_2 n &< i
  \end{align*}
  $\implies$ for $i = \frac{\log (\log_2 n)}{\log d}$ is $b_i < 1$ $\implies$ no bins with load
  $> \frac{\log (\log_2 n)}{\log d}$.
\end{pro}
Application: \\
We sample 2 hash functions $h_1, h_2: U \to M$. \\
For element $v \in V$ we insert in $T[h_1(v)]$ or $T[h_2(v)]$ depending on which list is shorter. \\
Max load in $O(\log (\log n))$.


\section{Cockoo hashing}

Idea: use 2 hash functions but allow moving elements later. \\
We want to have at most 1 element at each entry in the table. \\
Inserting: \\ % skica
\begin{itemize}
  \item if empty: insert,
  \item if not empty: push other element to its other choise, repeat recursively.
\end{itemize}
Questions:
\begin{itemize}
  \item how many do I need to move,
  \item how many elements can I insert before problems?
\end{itemize}
We can think of positions in the table as vertices and elements of $V$ as edges. \\
$|V|$ edges are inserted uniformly at random (if ideal hash function)
$\implies$ random graph. \\
Erdös-reny model: $G_{n,m} \approx G_{n,p}$ if $m = \binom{n}{2} p$ (A.S. properties). \\
If $np < 1 - \epsilon$: all connected components have size at most $O(\log n)$,
components are trees or at most 1 cycle per component, expected size of a component is $O(1)$. \\
% skica
% skica
Fact: if graph has at msot 1 cycle per component, then inserting can be done and takes at most
$2 \cdot $(size of component) time (each edge changes direction at most 2 times).
\begin{theorem} \text{} \\
  Let $n = |U|$, $h_1, h_2: U \to M$, $m = |M| = 2 \cdot (1 + \epsilon) \cdot n$,
  then with high probability cockoo hashing works correctly with
  \begin{itemize}
    \item inserting time:
      \begin{itemize}
        \item $O(\log n)$ time worst case,
        \item $O(1)$ expected case,
      \end{itemize}
    \item space: $O(n)$,
    \item lookup time: $O(1)$.
  \end{itemize}
\end{theorem}
Dynamically add element: \\
$m = 2 \cdot (1 + \epsilon) \cdot n$ \\
$p = \frac{m^{'}}{\binom{n^{'}}{2}} = \frac{2 m^{'}}{n^{'} (n^{'}-1)}$ \\
$pn^{'} = \frac{2m^{'}}{(n^{'}-1)} = \frac{2n^{'}}{2 (1+\epsilon) n^{'}} = \frac{1}{1+\epsilon} < 1 + \epsilon^{'}$


%\clearpage
%\phantomsection

%\addcontentsline{toc}{chapter}{Literatura}
%\bibliography{../bibtex/literatura}
%\bibliographystyle{plainnat}


%\clearpage
%\phantomsection

%\chapter*{Dodatki}
%\addcontentsline{toc}{chapter}{Dodatki}
%D.




\end{document}
