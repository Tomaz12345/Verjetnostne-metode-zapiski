\documentclass[a4paper, 12pt]{book}

\usepackage{fancyhdr}

\newcommand{\ttitle}{Verjetnostne metode v računalništvu - zapiski s predavanj prof. Marca}
\newcommand{\ttitleshort}{Verjetnostne metode v računalništvu}
\newcommand{\tauthor}{Tomaž Poljanšek}
\newcommand{\tdate}{študijsko leto 2023/24}

\usepackage{color}
\usepackage{soul}
\usepackage[numbers]{natbib}

\usepackage{physics}

\usepackage[parfill]{parskip}
\usepackage[hyphens]{url}

\usepackage[usestackEOL]{stackengine}[2013-10-15] % formatting Pascal
\usepackage[dvipsnames]{xcolor}

\usepackage{cancel}
\usepackage[export]{adjustbox}

% Related to math
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{mathtools}
\usepackage{youngtab}
\usepackage{tikz}

% encoding and language
\usepackage{lmodern}
\usepackage[slovene, english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% multiline comments
\usepackage{comment}
\usepackage{verbatim}

% random text - for texting
\usepackage{lipsum}
\usepackage{blindtext}

\usepackage{hyperref}

% images
\usepackage{graphicx}
\graphicspath{ {../images/} }

% no blank page
\usepackage{atbegshi}
\renewcommand{\cleardoublepage}{\clearpage}
%\renewcommand{\clearpage}{}

\usepackage{listings}
\usepackage{verbatim}
%\usepackage{fancyvrb}
%\usepackage{bera}

\newcommand*\Eval[3]{\left.#1\right\rvert_{#2}^{#3}}

\lstset{basicstyle=\ttfamily,
escapeinside={||},
mathescape=true}

% theorems
\theoremstyle{definition}
\newtheorem{counter}{Counter}[section]
\newtheorem{defn}[counter]{Definicija}
\newtheorem{lemma}[counter]{Lema}
\newtheorem{conseq}[counter]{Posledica}
\newtheorem{claim}[counter]{Trditev}
\newtheorem{theorem}[counter]{Izrek}
\newtheorem{pro}[counter]{Dokaz}
%%
\theoremstyle{remark}
\newtheorem*{ex}{Primer}
\newtheorem*{exmp}{Zgled}
\newtheorem*{rem}{Opomba}

% QED
\renewcommand\qedsymbol{$\blacksquare$}

\hypersetup{pdftitle={\ttitle}}

\addtolength{\marginparwidth}{-20pt}
\addtolength{\oddsidemargin}{40pt}
\addtolength{\evensidemargin}{-40pt}

\renewcommand{\baselinestretch}{1.3}
\setlength{\headheight}{15pt}
\renewcommand{\chaptermark}[1]
{\markboth{\MakeUppercase{\thechapter.\ #1}}{}} \renewcommand{\sectionmark}[1]
{\markright{\MakeUppercase{\thesection.\ #1}}} \renewcommand{\headrulewidth}{0.5pt} \renewcommand{\footrulewidth}{0pt}

% header
\fancyhf{}
\fancyhead[LE,RO]{\sl \thepage} 
\fancyhead[RE]{\sc \tauthor}
\fancyhead[LO]{\sc \ttitleshort}


\newcommand{\autfont}{\Large}
\newcommand{\titfont}{\LARGE\bf}
\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{empty}\cleardoublepage}}
\setcounter{tocdepth}{1}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\ch}{\operatorname{char}}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{float}
\usepackage{multirow}
\usepackage{icomma}
\usepackage{tabularx}
\usepackage{hhline}

\usepackage{enumitem}
\usepackage{ulem}
\newcommand{\msout}[1]{\text{\sout{\ensuremath{#1}}}} % cross text in math mode

\usepackage{alltt}

\title{\ttitle}
\author{\tauthor}
\date{\tdate}

\newcommand\mymaketitle{
  \begin{titlepage}
    \begin{center}
        \vspace*{4cm}
        \Huge
        \textbf{\ttitle}
                        
        \vspace{1.5cm}
        \huge
        \tauthor
            
        \vspace{3cm}
        \Large
        \tdate
    \end{center}
  \end{titlepage}
}




\begin{document}

\selectlanguage{slovene}
%\setcounter{page}{1}
\renewcommand{\thepage}{}
\newcommand{\sn}[1]{"`#1"'}

\mymaketitle

\clearpage
%\AtBeginShipoutNext{\AtBeginShipoutDiscard}

\frontmatter

% kazalo
\pagestyle{empty}
\def\thepage{}
\tableofcontents{}

%%
\def\x{\hspace{3ex}}    %BETWEEN TWO 1-DIGIT NUMBERS
\def\y{\hspace{2.45ex}}  %BETWEEN 1 AND 2 DIGIT NUMBERS
\def\z{\hspace{1.9ex}}    %BETWEEN TWO 2-DIGIT NUMBERS
\stackMath

%\clearpage
%\phantomsection
%\addcontentsline{toc}{chapter}{Povzetek}
%\chapter*{Povzetek}

%Predloga.

%\newpage

\pagenumbering{arabic}

\mainmatter
\setcounter{page}{1}
\pagestyle{fancy}


% 1. predavanje: 6.10.


\chapter{Introduction}


\section{Probability}

$(\Omega, F, P_r)$:
\begin{itemize}[label=$\circ$]
  \item $\emptyset \in F$,
  \item $A \in F \implies A^c \in F$,
  \item $A_1, A_2 \dots \in F \implies \cup_{i=1}^{\infty} A_i \in F$.
\end{itemize}
$P_r(A) \geq 0$, \\
$P_r\left(\cup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P_r(A_i)$ if $A_i$ disjoint, \\
$P_r\left(\cup_{i=1}^{\infty} A_i\right) \leq \sum_{i=1}^{\infty} P_r(A_i)$, \\
$\Omega = \{\omega_1, \omega_2 \dots\}$ - countable case. \\
$\begin{pmatrix}
  \omega_1 & \omega_2 & \dots \\
  p_1 & p_2 & \dots
\end{pmatrix}$
\begin{ex} \text{}
  \begin{verbatim}
    Alg():
      while True:
        B = sample as random from {0,1}  # 1 with probability p
        if B = 1:
          return
  \end{verbatim}
  $\Omega = \{1, 01, 001, 0001 \dots\}$ \\
  $\begin{pmatrix}
    1 & 01 & 001 & 0001 & \dots \\
    p & (1-p)p & (1-p)^2 p & (1-p)^3p & \dots
  \end{pmatrix}$.
\end{ex}


\section{Random variables}

$X: \Omega \to \Z$. \\
$E[X] = \sum_{c \in \Z} c \cdot P_r(X = c)$ expected value of $X$. \\
Properties:
\begin{itemize}[label=$\circ$]
  \item $E[f(X)] = \sum_{c \in \Z} f(c) \cdot P_r(X = c)$,
  \item $E[aX + bY] = aE[X] + bE[Y]$,
  \item $E[X \cdot Y] = E[X] \cdot E[Y]$ if $X, Y$ independent,
  \item $P_r(X \geq a) \leq \frac{E[X]}{a} \; \forall a > 0 \; X \geq 0$ Markov inequality.
\end{itemize}
\begin{ex}
  (Continuing from before). \\
  $X =$ number of trials before return. \\
  $X: \Omega \to \Z$. \\
  $X: 1 \to 1, 01 \to 2, 003 \to 3 \dots$ \\
  $\begin{pmatrix}
    1 & 2 & 3 & 4 & \dots \\
    p & (1-p)p & (1-p)^2 p & (1-p)^3p & \dots
  \end{pmatrix}$ - geometric distribution.
\end{ex}
\begin{claim}
  $E[X] = \frac{1}{p}$.
\end{claim}
\begin{pro}
  $X = \sum_{i=1}^{\infty} X_i$. \\
  $X_i = \begin{cases}
    1 \text{ if trial $i$ is executed} \\
    0 \text{ else}
  \end{cases}$ \\
  \begin{align*}
    E[X] &= E[\sum_{i=1}^{\infty} X_i] = \sum_{i=1}^{\infty} E[X_i] = \\
    &= \sum_{i=1}^{\infty} (1-p)^{i-1} = \frac{i=0}{\infty} (1-p)^i = \frac{1}{1-(1-p)} = \frac{1}{p}.
  \end{align*}
\end{pro}
$E[X] = \frac{1}{p}$. \\
$P_r(X \geq 100 \cdot \frac{1}{p}) \leq \frac{E[X]}{\frac{1}{p}} = \frac{1}{100}$.
\begin{defn}
  $H_n = 1 + \frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{n} = \sum_{i=1}^{\infty} \frac{1}{i}$.
\end{defn}
\begin{theorem}
  $H_n \leq 1 + \ln(n)$.
\end{theorem}
\begin{pro}
  \begin{equation*}
    H_n = 1 + \sum_{i=2}^{n} \frac{1}{i} \stackrel{\text{integral}}{\leq}
    1 + \int_{1}^{n} \frac{dx}{x} = 1 + \Eval{\ln(x)}{1}{n} = 1 + \ln(n).
  \end{equation*}
  % skica
\end{pro}



\chapter{Quicksort, min-cut}


\section{Quicksort}

\begin{alltt}
  Input: set (no equal element) (unordered list) S\(\in\R\)
      (or whatever you can compare linearly)
  Output: ordered list
  Code:
    def Quicksort(S):
      if |S|= 0 or 1:
        return S
      else:
        a = uniformly at random from S
        S\(\sp{-}\) = \{b \(\in\) S | b < a\}
        S\(\sp{+}\) = \{b \(\in\) S | a < b\}
        return Quicksort(S\(\sp{-}\)), a, Quicksort(S\(\sp{+}\))
\end{alltt}
% skica
$C(n)$ - random variable, the number of comparisons in evaluation of Quicksort with $|S| = n$.
\begin{theorem}
  $E[C(n)] = O\left(N \log(n)\right)$.
\end{theorem}
\begin{pro}
  $C(0) = C(1) = 0$. \\
  \begin{align*}
    E[C(n)] &= n - 1 + \sum_{i=1}^{n} \left(E[C(i-1)] + E[C(n-i)]\right) \cdot P_r(a \text{ is $i$-it element}) \leq \\
    &\leq n + \frac{2}{n} \sum_{i=1}^{n-1} E[C(i)].
  \end{align*}
  Induction: \\
  $n = 1: \checkmark$ \\
  $n-1 \to n$:
  \begin{align*}
    E[C(n)] &\leq n + \frac{2}{n} \sum_{i=1}^{n} E[C(i)] \leq \\
    &\leq n + \frac{2}{n} \sum_{i=1}^{n} 5i \log i \leq \\
    &\leq n + \frac{2}{n} \sum_{i=1}^{\lfloor\frac{n}{2}\rfloor} 5i \log i +
      \frac{2}{n} \sum_{i=1+\lfloor\frac{n}{2}\rfloor}^{n-1} 5i \log i \leq \\
    &\leq n + \frac{2}{n} \sum_{i=1}^{\lfloor\frac{n}{2}\rfloor} 5i \log \frac{n}{2} +
      \frac{2}{n} \sum_{i=1+\lfloor\frac{n}{2}\rfloor}^{n-1} 5i \log n \leq \\
    (\log \frac{n}{2} &= \log n - 1) \\
    &\leq n + \frac{2}{n} \left(\sum_{i=1}^n 5i \log n - \sum_{i=1}^{\frac{n}{2}} 5i\right) = \\
    &= n + \frac{10}{n} \left(\frac{n(n-1)}{2} \log n - \frac{\frac{n}{2} (\frac{n}{2} + 1)}{2}\right) \leq \\
    &\leq n + 5(n-1) \log n - n < \\
    &< 5n \log n.
  \end{align*}
\end{pro}
$P\left(C(n) \geq b \cdot 5n \log n\right) \stackrel{\text{Markov}}{\leq} \frac{1}{b}$.
\begin{pro} \text{} \\
  2: \\
  Let $S_1, S_2 \dots S_n$ sorted elements of $S$. \\
  Define random variable
  $X_{ij} = \begin{cases}
    1 \text{ if $S_i$ and $S_j$ are compared} \\
    0 \text{ else}
  \end{cases}$ \\
  $C(n) = \sum_{1 \leq i < j \leq n} E[X_{ij}]$. \\
  $E[X_{ij}] = P(S_i$ and $X_j$ compared$)$. \\
  % skica
  $S_{ij}$ - the last set including $S_i$ and $S_j$. \\
  $E[X_{ij}] = \frac{2}{|S_{ij}|} \leq \frac{2}{j-i+1}$. \\
  $|S_{ij}| \geq j - i + 1$. \\
  $S_{ij}$ has everything in between. \\
  \begin{align*}
    \implies E[C(n)] &\leq \sum_{1 \leq i < j \leq n} \frac{2}{j-i+1} = \\
    &\stackrel{k=j-i+1}{=} \sum_{i=1}^{n-1} \sum_{k=2}^{n-i+1} \frac{2}{k} \leq \\
    &\leq 2 \cdot n \cdot H_n \leq \\
    &\leq 2 n (1 + \log n).
  \end{align*}
\end{pro}


% 2. predavanje: 13.10.

\section{Min-cut}

$G$ multigraph. \\
Cut: $U \subset V(G), \; U \neq \emptyset, V(g)$. \\
$(U, V(G) \setminus U) = \{uv \in E(G) \mid u \in U, v \in V(G) \setminus U\}$. \\
% skica
Problem min-cut: \\
Input: $G$. \\
Output: $\min |(U, V(G) \setminus U)|$ - cut size.
\begin{alltt}
  Algorithm 1:
    x \(\in\) V(G)
    Call maxFlow(G, x, y) \(\forall y \in V(G)\)
    Take min
\end{alltt}
$maxFlow$ is Edmonds-Karp algorithm $O\left(|V| |E|^2\right)$. \\
\begin{alltt}
  Algorithm 2 (Stoer Wagner)
\end{alltt}
Is $O\left(|E| |V| + |V| log |V|\right)$.
\begin{alltt}
  Algorithm randMinCut:
    G\(\sb_{0}\) = G
    i = 0
    while |V(G\(\sb{i}\))| > 2:
      e\(\sb{i}\) = uniformly at random from G\(\sb{i}\)
      G\(\sb{i+1}\) = G\(\sb{i}\) / \(\sb{e\sb{i}}\)
      i = i + 1
    u, v = V(G\(\sb{n-2}\)) // n = |V(G)|
    U = \{w \(\in\) V(G) | w is merged into u\}
    return (U, V(G) \textbackslash U)
\end{alltt}
\begin{theorem}
  Algorithm $randMinCut$ gives you a minimal cut with probability greater or equal to $\frac{2}{n(n-1)}$.
\end{theorem}
\begin{pro} \text{} \\
  Fact 1: $minCut(G_i) \leq minCut(G_i)$;
  \begin{itemize}[label={}]
    %\item $\leq$: %example
    \item $\ngtr$: $minCut$ remains.
  \end{itemize}
  Fact 2: $minCut(G) \leq \delta(G)$. \\
  $k := minCut(G)$. \\
  Let $(A,B)$ be an optimal cut. \\
  $\epsilon_i$ not in $(A,B)$.
  \begin{align}
    &P_r(\text{Algorithm not returning } (A,B)) \nonumber \\
    &= P_r(\epsilon_0 \cap \dots \cap \epsilon_{n-3}) \nonumber  \\
    &= P_r(\epsilon_0 \cap \dots \cap \epsilon_{n-4}) \cdot
      P_r(\epsilon_{n-3} \mid \epsilon_0 \cap \dots \cap \epsilon_{n-4}) \nonumber  \\
    &= P_r(\epsilon_{n-3} \mid \cap_{i=0}^{n-4} \epsilon_i) \cdot
      P_r(\epsilon_{n-3} \mid \cap_{i=0}^{n-4} \epsilon_i) \nonumber \\
    &\dots P_r(\epsilon_1 \mid \epsilon_0) \cdot P_r(\epsilon_0). (*) %\label{randMinCut-mid}
  \end{align}
  \begin{equation*}
    P_r(\overline{\epsilon_i} \mid \epsilon_{i-1} \cap \dots \cap \epsilon_0) =
      %\frac{k}{|E(G_i)|} \stackrel{\text{\refeq{E-gi}}}{\leq} \frac{k}{\frac{(n-i)k}{2}} = \frac{2}{n-i}
      \frac{k}{|E(G_i)|} \stackrel{\text{(**)}}{\leq} \frac{k}{\frac{(n-i)k}{2}} = \frac{2}{n-i}
  \end{equation*}
  \begin{equation}
    |E(G_i)| \geq \frac{(n-i) \delta(G)}{2} \geq \frac{(n-i)k}{2}. (**) % \label(E-gi)
  \end{equation}
  \begin{equation*}
    P_r(\epsilon_i \mid \epsilon_{i-1} \cap \dots \cap \epsilon_0) \geq 1 - \frac{2}{n-i} = \frac{n-2-i}{n-i}.
  \end{equation*}
  \begin{equation*}
    %\text{\ref{randMinCut-mid}} \geq \frac{n-2}{n} \cdot \frac{n-3}{n-1} \dots \frac{1}{3} = \frac{2}{n(n-1)}.
    (*) \geq \frac{n-2}{n} \cdot \frac{n-3}{n-1} \dots \frac{1}{3} = \frac{2}{n(n-1)}.
  \end{equation*}
\end{pro}
\begin{theorem}
  Running $randMinCut \; n(n-1)$ times and taking best output gives correct solution with probability $\geq 0.86$.
\end{theorem}
\begin{pro}
  $A_i$ - event that $i$-th run gives sub-optimal solution.
  \begin{align*}
    P_r(\text{solution not correct}) &= P_r(A_1 \cap \dots \cap A_{n(n-1)}) \\
    &= \prod_{i=1}^{n(n-1)} P_r(A_i) \leq (1 - \frac{2}{n(n-1)})^{n(n-1)} \\
    &\leq e^{-\frac{2}{n(n-1)} \cdot n(n-1)} = e^{-2} \leq 0.14.
  \end{align*}
  $1 - x \leq e^x \; \forall x \in \R$.
\end{pro}
If we run $n(n-1) log(n)$ times $\to O\left(\frac{1}{n}\right)$. \\
$O\left(n^2 \log n \cdot n\right)$. \\
Improved: $O\left(n^2 \log^3 n\right)$.



\chapter{Complexity classes}


Decision problem - yes/no question on a set of inputs = asking $w \in \Pi$. \\
Randomized algorithms:
\begin{itemize}
  \item Las Vegas algorithms: always gives correct solution, example: $Quicksort$.
  \item Monte Carlo algorithms: it can give wrong answers.
    Monte Carlo algorithms subtypes:
    \begin{itemize}
      \item type(1): $\begin{cases}
          \text{if } \omega \in \Pi \implies \text{ algorithm returns \sn{$\omega \in \Pi$} with probability } \geq \frac{1}{2} \\ 
          \text{if } \omega \notin \Pi \implies \text{ algorithm returns \sn{$\omega \in \Pi$} with probability } = 0 
        \end{cases}$
      \item type(2): $\begin{cases}
          \text{if } \omega \in \Pi \implies \text{ algorithm returns \sn{$\omega \in \Pi$} with probability } = 1 \\ 
          \text{if } \omega \notin \Pi \implies \text{ algorithm returns \sn{$\omega \in \Pi$} with probability } \leq \frac{1}{2} 
        \end{cases}$
      \item type(3): $\begin{cases}
          \text{if } \omega \in \Pi \implies \text{ algorithm returns \sn{$\omega \in \Pi$} with probability } \geq \frac{3}{4} \\ 
          \text{if } \omega \notin \Pi \implies \text{ algorithm returns \sn{$\omega \in \Pi$} with probability } \leq \frac{1}{2}
        \end{cases}$
    \end{itemize}
    type(1) and type(2): one-sided error, type(3): 2-sided error. \\
    $\frac{1}{2}, \frac{3}{4}$ and $\frac{1}{4}$ arbitrary numbers, can be something different (for type(3) better than coin flip).
\end{itemize}
\begin{ex}
  Decisional problem: does a graph $G$ have $minCut \leq k$? \\
  Run $randMinCut(G) \; n(n-1)$ times.
  \begin{alltt}
    Algorithm randMinCut:
      if one of runs gives |(A,B)| \(\leq\) k:
        return true
      else:
        return false
  \end{alltt}
\end{ex}
Complexity classes:
\begin{itemize}
  \item RP (randomized polynomial time): decisional problems for which there exists Monte Carlo algorithm of type(1)
    with polynomial time complexity (worst case).
  \item co-RP: decisional problems for which there exists Monte Carlo algorithm of type(2) with polynomial time complexity
    (worst case).
  \item BRP (bounded-error probabilistic polynomial time): decisional problems for which there exists Monte Carlo algorithm of type(3)
    with polynomial time complexity (worst case).
  \item ZPP (zero-error probabilistic polynomial time): decisional problems for which there exists Las Vegas algorithm
    with expected polynomial time complexity (worst case).
\end{itemize}
% skica
ZPP = RP $\cap$ co-RP.



\chapter{Chernoff bounds}


\begin{theorem}
  Let $X_1, X_2 \dots X_n$ independent random variables with image $\{0, 1\}$. \\
  Let $p_i = P_r(X_i = x_i), X = \sum_{i=1}^{n} X_i$ and $\mu = E(X) = p_1 + \dots + p_n$. \\
  For every $\delta \in (0,1)$:
  \begin{align*}
    &P_r(X - \mu \geq \delta \mu) \leq e^{-\frac{\delta^2 \mu}{3}} \\
    &P_r(\mu - X \leq \delta \mu) \leq e^{-\frac{\delta^2 \mu}{2}} \\
    \implies &P_r(|X - \mu| \geq \delta \mu) \leq e^{-\frac{\delta^2 \mu}{3}}. \\
  \end{align*}
\end{theorem}
% skica
Probability falls extremely quickly after $E(X)$.


% 3. predavanje: 20.10.

\begin{pro}
  \begin{align*}
    P_r(X - \mu \geq \delta \mu) &= P_r(X \geq \mu(1+\delta)) \\
    &\stackrel{t>0}{=} P_r(tX \geq t\mu(1+\delta)) \\
    &\stackrel{e^y>0}{=} P_r(e^{tX} \geq e^{t\mu(1+\delta)}) \\
    &\stackrel{\text{Markov}}{\leq} \frac{E\left(e^{tX}\right)}{e^{t\mu(1+\delta)}} \\
    &\stackrel{\refeq{eq:et_x}}{\leq} \frac{e^{(e^t-1)\mu}}{e^{t\mu(1+\delta)}} \\
    &\stackrel{\refeq{eq:leq_in_exp}}{\leq} e^{-\mu \frac{\delta^2}{3}}.
  \end{align*}
  \begin{align}
    E(e^{tX}) &= E(e^{tX_1 + \dots + tX_n}) \nonumber \\
    &= E(e^{tX_1} \dots e^{tX_n}) \nonumber \\
    &\stackrel{\text{independent}}{=} \prod_{i=1}^{n} E(e^{tX_i}) \nonumber \\
    &\stackrel{\refeq{eq:et_xi}}{\leq} \prod_{i=1}^{n} e^{p_i(e^t-1)} \nonumber \\
    &= e^{(e^t-1) \sum_{i=1}^{n}p_i} \nonumber \\
    &= e^{(e^t-1)\mu} \label{eq:et_x}.
  \end{align}
  \begin{equation}
    E(e^{tX_i}) = p_i \cdot e^t + (1-p_i) \cdot e^0 = 1+p_i(e^t-1) \stackrel{1+x\leq e^x}{\leq} e^{p_i(e^t-1)}.
    \label{eq:et_xi}
  \end{equation}
  Want:
  \begin{equation}
    e^t - 1 - t(1+\delta) \leq -\frac{\delta^2}{3} \; \forall \delta \in (0,1) \label{eq:leq_in_exp}
  \end{equation}
  \begin{align*}
    &t = \ln(1+\delta) \\
    &f(\delta) = 1 + \delta - 1 - (1+\delta) \ln(1+\delta) + \frac{\delta^2}{3} \stackrel{?}{\leq} 0 \\
    &f(0) = 0 \\
    &f^{'}(\delta) = 1 - \ln(1+\delta) - 1 + \frac{2}{3} \delta = \frac{2}{3} \delta - \ln(1+\delta) \stackrel{?}{\leq} 0 \\
    &\frac{2}{3} \delta \leq \ln(1+\delta) \\
    % skica
    &\delta=1: \; \frac{2}{3} \stackrel{?}{\leq} \ln(2) \approx 0.69 \checkmark
  \end{align*}
  \begin{align*}
    P_r(\mu - X \leq \delta \mu) &= P_r(X \geq \mu(1-\delta)) \\
    &\stackrel{t>0}{=} P_r(tX \geq t\mu(1-\delta)) \\
    &\stackrel{e^y>0}{=} P_r(e^{tX} \geq e^{t\mu(1-\delta)}) \\
    &\leq \dots \leq \frac{e^{(e^t-1)\mu}}{e^{t\mu(1-\delta)}}.
  \end{align*}
  Want: $e^t - 1 - t(1-\delta) \leq -\frac{\delta^2}{2} \; \forall \delta \in (0,1)$:
  \begin{align*}
    &t = \ln(1-\delta) \\
    &f(\delta) = 1 - \delta - 1 - (1-\delta) \ln(1-\delta) + \frac{\delta^2}{2} \stackrel{?}{\leq} 0 \\
    &f(0) = 0 \\
    &f^{'}(\delta) = - 1 + 1 - \ln(1-\delta) + \delta \stackrel{?}{\leq} 0 \\
    &\frac{2}{3} \delta \leq \ln(1+\delta) \\
    &\ln(1-\delta) \stackrel{?}{\leq} -\delta \checkmark
    % skica
  \end{align*}
  \qed
\end{pro}
$X_i \sim \begin{pmatrix}0 & 1 \\ \frac{1}{2} & \frac{1}{2}\end{pmatrix}$ \\
$X = \sum_{i=1}^{n} X_i$ \\
$\mu = \frac{n}{2}$
\begin{align*}
  P_r(|X-\mu| \geq \sqrt{\frac{3}{2}n \ln(n)}) &= P_r(|X-\mu| \geq \frac{n}{2} \sqrt{\frac{6}{n} \ln(n)}) \\
  &\quad \mu = \frac{n}{2}, \delta = \sqrt{\frac{6}{n} \ln(n)}, \\
  &\quad \text{for \sn{big} }n \delta \in (0,1) \\
  &\stackrel{\text{Chernoff}}{\leq} 2 e^{-\frac{\frac{n}{2} \frac{6}{n} \ln(n)}{3}} = \frac{2}{n}.
\end{align*}
$d = \sqrt{\frac{3}{2}n \ln(n)}$
% skica
\begin{equation*}
  \implies P_r(X \in (\mu - \sqrt{\frac{3}{2}n \ln(n)}, \mu + \sqrt{\frac{3}{2}n \ln(n)})) \geq 1 - \frac{2}{n}.
\end{equation*}
% skica
\begin{claim} \text{} \\
  Let $X_1, X_2 \dots$ independent random variables with image $\{0,1\}$. \\
  $P_r(X_i = 1) = \frac{1}{2} \; \forall i$. \\
  Let $X = \sum_{i=1}^{cm} X_i$ where $c \geq 4$. \\
  Then $P_r(X \leq m) \leq e^{-\frac{cm}{16}}$.
\end{claim}
\begin{pro}
  \begin{align*}
    P_r(X \leq m) &= P_r(\frac{cm}{2} - X \geq \frac{cm}{2} - m) \\
    &= P_r(\frac{cm}{2} - X \geq \frac{cm}{2} (1 - \frac{2}{c})) \\
    &\stackrel{\text{Chernoff}}{\leq} e^{-\frac{\frac{cm}{2} (1-\frac{2}{c})^2}{2}} \\
    &\quad 1-\frac{2}{c} \geq \frac{1}{2} \text{ if } c \geq 4 \\
    &\leq e^{-\frac{\frac{cm}{2} \frac{1}{4}}{2}} = e^{-\frac{cm}{16}}.
  \end{align*}
  \qed
\end{pro}
Back to Quicksort.
\begin{theorem} \text{} \\
  With probability $\geq 1 - \frac{1}{n}$ Quicksort uses at most $48n\ln(n)$ comparisons.
\end{theorem}
\begin{pro} \text{} \\
  % skica (i)
  For $s \in S$ define $S_1^S \dots S_{t_s}^S \neq \emptyset$ sets that include $s$,
  $t_s$ - number of comparisons with $s$ where $s$ is not a pivot $+1$. \\
  Define: iteration $i$ is successful if $|S_{i+1}| \leq \frac{3}{4} |S_i|$ ($\frac{1}{2}$ is too strict).
  \begin{equation*}
    X_i = \begin{cases}
      1 \text{ if iteration } i \text{ is successful} \\
      0 \text{ else}
    \end{cases}
  \end{equation*}
  % skica (i)
  $P_r(X_i = 1) \geq \frac{1}{2}$ \\
  $S_i: n \to \frac{3}{4} n \to (\frac{3}{4})^2 n \to \dots \to 1$. \\
  Notice: max number of iteration is $\log_{\frac{4}{3}}(n) = \frac{\ln(n)}{\ln(4)-\ln(3)}$. \\
  Probability that we haven't succeeded in $\log_{\frac{4}{3}}(n)$ steps:
  \begin{align}
    P_r(\sum_{i=1}^{c \log_{\frac{4}{3}}(n)} X_i < \log_{\frac{4}{3}}(n)) &\leq
      P_r(\sum_{i=1}^{c \log_{\frac{4}{3}}(n)} Y_i < \log_{\frac{4}{3}}(n)) \label{eq:X_to_Y} \\
    &\stackrel{\text{Chernoff}}{<} e^{-\frac{c \log_{\frac{4}{3}}(n)}{24}} \\
    &= e^{-\frac{c \ln(n) \log_{\frac{4}{3}}(e)}{24}} \\
    &= \frac{1}{n} \frac{c \log_{\frac{4}{3}}(e)}{24} \\
    &\quad \log_{\frac{4}{3}}(e) \approx 3.4, \; c=14 \\
    &\leq \left(\frac{1}{n}\right)^2
  \end{align}
  \refeq{eq:X_to_Y} because $X_i$ not independent,
  $Y_i \sim \begin{pmatrix}0 & 1 \\ \frac{1}{2} & \frac{1}{2}\end{pmatrix}$ independent. \\
  $P_r(t_s \geq c \log_{\frac{4}{3}}(n)) \geq \left(\frac{1}{n}\right)^2$ for one $s$. \\
  $c=14 \implies$ at least $48 \ln(n)$ iterations with probability $\leq \left(\frac{1}{n}\right)^2$. \\
  With probability as least $1-\frac{1}{n}$ for all $s \in S$ it holds that $s$ has $\leq 48 \ln(n)$ comparisons with a pivot. \\
  $\implies$ total number of comparisons $n \cdot 48 \ln(n)$ with probability as least $1 - \frac{1}{n}$.
  \qed
\end{pro}



\chapter{Monte Carlo methods}


\section{Example 1}

% skica
Area of circle $= \frac{\pi}{4}$. \\
$X_i = \begin{cases}
  1 \text{ if you hit the area of circle} \\
  0 \text{ else}
\end{cases}$ \\
$P_r(X_i = 1) = \frac{\frac{\pi}{2}}{1} = \frac{\pi}{4}$. \\
$E(X_i) = \frac{\pi}{4}$. \\
$X = \frac{\sum_{i=1}^{n} X_i}{n}$. \\
$E(X) = \frac{n \cdot E(X_i)}{n} = E(X_i)$.


% 4. predavanje: 27.10.

\section{Example 2}

$I = \int_{\Omega} f(x) dx$ - volume. \\
% skica
$X_i = \begin{cases}
  1 \; F(x_i,y_i) \leq z_i \\
  0 \text{ otherwise}
\end{cases}$ \\
$v \cdot E\left(\frac{\sum_{i=1}^{n} X_i}{n}\right) = I$.


\section{$(\epsilon,\delta)$-approximation}

\begin{defn}[$(\epsilon,\delta)$-approximation]
  A random algorithm gives a $(\epsilon,\delta)$-approximation for value $v$ if the output $X$ satisfies:
  \begin{equation*}
    P_r(|X-v| \leq \epsilon v) \geq 1 - \delta.
  \end{equation*}
\end{defn}
\begin{theorem}
  Let $X_1 \dots X_n$ be independent and identically distributed indicator variables.
  Let $\mu = E(X_i), \; Y = \frac{\sum_{i=1}^{m} X_i}{m}$.
  If $m \geq \frac{3 \ln\left(\frac{2}{\delta}\right)}{\epsilon^2 \mu}$, then $P_r(|Y-\mu| \geq \epsilon \mu) \leq \delta$
  $\implies Y$ is $(\epsilon,\delta)$-approximation for $\mu$.
\end{theorem}
\begin{pro} \text{} \\
  $X = \sum_{i=1}^{n} X_i$ \\
  $E(X) = m E(x_i) = m \mu$ \\
  $m \geq \frac{3 \ln\left(\frac{2}{\delta}\right)}{\epsilon^2 \mu}$
  \begin{align*}
    P_r(|Y-\mu| \geq \epsilon \mu) &= P_r(\left|\frac{X}{m}-\mu\right| \geq \epsilon \mu) \\
    &= P_r(\frac{1}{m} \left|X-E(X)\right| \geq \frac{1}{m} \epsilon E(x)) \\
    &\stackrel{\text{Chernoff}}{\leq} 2 e^{-\frac{\epsilon^2 E(x)}{3}} \\
    &= 2 e^{-\frac{\epsilon^2 \mu m}{3}} \\
    &\leq 2 e^{-\frac{\epsilon^2 \mu}{3} \cdot \frac{3 \ln\left(\frac{2}{\delta}\right)}{\epsilon^2 \mu}} = \delta.
  \end{align*}
\end{pro}
Back to example 1: \\
$E(Y) = \frac{\pi}{4}, \delta = \frac{1}{1000}$ ($99.9\%$ sure), $\epsilon = \frac{1}{10000}$ \\
$\implies M = \frac{3 \ln\left(\frac{2}{\frac{1}{1000}}\right) 4}{\pi \left(\frac{1}{10000}\right)^2} \approx 29106$. \\
Problems for MC (Monte-Carlo):
\begin{itemize}
  \item rare events, e.g. $X \sim \begin{pmatrix}0 & 10^{100} \\ 1-10^{-20} & 10^{-20}\end{pmatrix}, \; E(X) = 10^{80}$
    % skica
\end{itemize}


\section{DNF counting}

CNF: $(X_{i_1} \lor \overline{X_{i_2}} \lor X_{i_4}) \land (X_{i_1} \lor \overline{X_{i_3}}) \land \dots$ \\
DNF: $(\overline{X_{i_1}} \land X_{i_2} \lor \overline{X_{i_4}}) \lor \dots$
- easy to determine if solution exists. \\
Question: number of solutions to a given DNF? \\
Observation: CNF $F$ has a solution $\iff$ DNF $\neg F$ has less than $2^n$ solutions, $n$ is number of samples.
\begin{alltt}
  ALG_1(F):
    x = 0
    for i in range(1,m+1):
      \(x_1 \dots x_n\) uniformly random from \{0,1\}\(\sp{n}\)
      if \(F(x_1 \dots x_n)\) = 1:
        x += 1
    return \(\frac{x}{m} \cdot 2\sp{n}\)
\end{alltt}
$Y = \frac{\sum_{i=1}^{m} X_i}{m}$ \\
$(\epsilon,\delta)$-approximation for $Y$ \\
$E(Y) = \frac{\text{number of solutions of }F}{2^n} = \frac{c(F)}{2^n}$ \\
$m \geq \frac{3 \ln\left(\frac{2}{\delta}\right)}{\epsilon^2 E(X)} =
\frac{3 \ln\left(\frac{2}{\delta}\right)}{\epsilon^2} \cdot \frac{2^n}{x(F)}$ \\
$c(F)$ very small $\to$ $m$ exponentially big $\to$ not good (we need a lot of samples).
\begin{defn} \text{} \\
  $SC_i = \{(a_1 \dots a_n) \in \{0,1\}^n \text{ such that } F = F_1 \lor \dots \lor F_t, \; F_i(a_1 \dots a_n) = 1\}$.
  % skica
\end{defn}
$|SC_i| = 2^{n-l_i}$, $l_i$: number of values in $F_i$ \\
$U = \{(i,a) \mid i \in \{1,2 \dots t\}, a \in SC_i\}$ \\
$U = \sum_{i=1}^{t} |SC_i|$ - $O(tn)$ (space smaller than $\{0,1\}^n$) \\
$S = \{(i,a) \in U \mid a \in SC_i, \; a \not \in SC_j \; 1 \leq j < i\}$ \\
$|S| = |SC_1| + \dots + |SC_t| = c(F)$.
\begin{alltt}
  ALG_2(F):
    x = 0
    for i in range(1,m+1):
      (i, a) uniformly random from U (**)
      if (i, a) \(\in\) S: (*)
        x += 1
    return \(\frac{x}{m} \cdot |U|\)
\end{alltt}
$(*) \; a \in SC_i \to O(n), \; a \notin SC_j \; j = 1 \dots i-1 \to O(tn) \; \implies \; O(tn), m$ times. \\
$(**)$: watch for details on how to, e.g. $x_2, x_2 \land x_3$: $x_2$ is more probable than $x_2 \land x_3 \to O(1)$.
\begin{theorem}
  For $m = \lceil\frac{3t \ln(\left(\frac{2}{\delta}\right))}{\epsilon^2}\rceil$ algorithm returns $(\epsilon,\delta)$-approximation
  in $O\left(\frac{t^n n \ln\left(\frac{2}{\delta}\right)}{\epsilon^2}\right)$ time.
\end{theorem} 
\begin{pro}
  $O(t \cdot n \cdot m)$. \\
  Insert $m = ...$
\end{pro}
Prove
\begin{equation*}
  P_r(Y|U| - c(F) > \epsilon c(F)) < \delta:
\end{equation*}
$c(F) = |S|, E(Y) = \frac{|S|}{|U|}$
\begin{align*}
  P_r(Y|U| - c(F) > \epsilon c(F)) &= P_r(|U|(Y - E(Y)) > \epsilon |U| E(Y)) \leq \delta
\end{align*}
if
\begin{equation*}
  m \geq \frac{3 \ln\left(\frac{2}{\delta}\right)}{\epsilon^2 E(Y)} \geq \frac{3 \ln\left(\frac{2}{\delta}\right)t}{\epsilon^2}
\end{equation*}
where
\begin{equation*}
  E(Y) = \frac{|S|}{|U|} \geq \frac{1}{t}
\end{equation*}
($=$ if disjoint). \\
In new space $E(Y)$ much larger $\implies$ $m$ smaller.
% skica



\chapter{Polynomials}


Let $\F$ be a field. \\
$\F$ can be $\R, \C, \Z_p, \F_{p^n}$. \\
$\F[x_1 \dots x_n]$ algebra of polynomials with values $x_1 \dots x_n$. \\
$f \in \F[x_1 \dots x_n]$ \\
$deg(f[x_1 \dots x_n]) := deg(f[x \dots x])$.
\begin{theorem}
  Let $p(x_1 \dots x_n) \in \F[x_1 \dots x_n]$ have the degree $d \geq 0$ and $p \neq 0$.
  Let $s \subset \F$ be finite.
  If $(r_1 \dots r_n)$ is uniformly at random element from $S^n$.
  Then $P_r(p(r_1 \dots r_n) = 0) \leq \frac{d}{|S|}$.
\end{theorem}
\begin{pro}
  Induction on $n$. \\
  $n=1$:
  \begin{align*}
    &p(x) = (x-z_1) (x-z_2) \dots (x-z_j) q(z) \\
    &\text{number of zeros } \leq \text{ degree - fact} \\
    &P_r(p(r_1) = 0) = \frac{\text{number of zeros}}{|S|} \leq \frac{d}{|S|}.
  \end{align*}
  $n-1 \to n$:
  \begin{align*}
    &\text{rewrite }p: \\
    &p(x_1 \dots x_n) = \sum_{i=0}^{j} x^i p_i(x_2 \dots x_n) \\
    &j \leq d \\
    P_r(p(r_1 \dots r_n) = 0) &= P_r(p(r_1 \dots r_n = 0) \mid p_j(r_2 \dots r_n) = 0) \cdot P_r(p_j(r_2 \dots r_n) = 0) \\
    &+ P_r(p(r_1 \dots r_n = 0) \mid p_j(r_2 \dots r_n) \neq 0) \cdot P_r(p_j(r_2 \dots r_n) \neq 0) \\
    &\leq 1 \cdot \frac{d-j}{|S|} + \frac{j}{|S|} \cdot 1, \\
  \end{align*}
  because
  \begin{align*}
    &P_r(p(r_1 \dots r_n = 0) \mid p_j(r_2 \dots r_n) \neq 0) \leq \frac{d-j}{|S|} \\
    &P_r(p_j(r_2 \dots r_n) \neq 0) \leq \frac{j}{|S|}.
  \end{align*}
\end{pro}
\underline{Problem}: \\
Let $A,B,C \in \F^{n \times n}$, is $A \cdot B = C$? \\
Computing $A \cdot B$:
\begin{itemize}
  \item school-book algorithm: $O\left(n^3\right)$,
  \item Strassen algorithm: $O\left(n^{2,807\dots}\right)$,
  \item galactic algorithm: $O\left(n^{2.372\dots}\right)$ - has enormous constants.
\end{itemize}
\begin{alltt}
  RAND_ACB(A,B,C):
    for i in range(1,k+1):
      x uniformly at random from \{0,1\}\(\sp{n}\)
      if \(A \cdot (B \cdot x) \neq x\):
        return false
    return true
\end{alltt}
$O\left(k n^2\right)$. \\


% 5. predavanje: 3.11.

If $A \cdot B = C$, algorithm returns true. \\
If $A \cdot B \neq C$: \\
\begin{align*}
  P_r(ABx = Cx) &= P_r((AB-C)x = 0) \\
  &= P_r(||(AB-C)x||^2 = 0) \stackrel{\text{Poly}}{\leq} \frac{2}{3}.
\end{align*}
$||(AB_C)x||^2$ - polynomial in $x_1 \dots x_n$ of degree $2$. \\
If $A \cdot B \neq C$, then algorithm return false with probability at least $1 - \left(\frac{2}{3}\right)^k$. \\
\underline{Problem}: \\
1-factor in bipartite graphs. \\
% skica
$|V(g)| = 2n$. \\
Represent $G$ with $n \times n$ matrix $Z = (Z_{ij})_{i,j=1}^n$ \\
$Z_{ij} = \begin{cases}
  &X_{ij} \text{ if } a_i b_j \in E(x) \qquad \text{(X: variable)} \\
  &0 \text{ else}
\end{cases}$ \\
\begin{align*}
  det Z(x_{11} \dots x_{nn}) &= \sum_{\pi \in S_n} sign(\pi) z_{1,\pi(1)} \dots z_{n,\pi(n)} \\
  &= \sum_{\pi \in S_n, \pi \text{ defines 1-factor}} sign(\pi) x_{1,\pi(1)} \dots x_{n,\pi(n)}.
\end{align*}
$det Z \neq 0 \iff G$ has 1-factor. \\
\begin{alltt}
  Rand_1factor(G):
    construct Z with variables x11 ... xnn
    for i in range(1,k+1):
      u <- uniformly at random from  {1,2..2n-1}\(\sp{n\sp{2}}\) (r11 ... rnn)
      compuze d = det Z(r11 ... rnn)
      if d != 0:
        return true
    return false
\end{alltt}
Complexity: $k \cdot$ computing determinant: $O\left(n^3\right)$ (Gaussian elimination). \\
or apply approximation algorithm:
\begin{itemize}
  \item if $G$ has no 1-factor it always returns false,
  \item if $G$ has 1-factor, it returns true with probability at least $1-\left(\frac{n}{2n}\right)^k = 1 - \left(\frac{1}{2}\right)^k$
    ($k$ konstant, larger set $\implies$ smaller $k$ needed).
\end{itemize}



\chapter{Random graphs}


\section{G(n,p) model}

$G$ is a random Erdös-Rény graph if it has $n$ vertices and each pair of vertices is connected with probability $p$.
\begin{ex}
  $G\left(5, \frac{1}{2}\right)$.
  % skica
\end{ex}
$E($ edges in $G$ fron $G(n,p)) = \sum_{1 \leq i < j \leq n} E(X_{ij}) = \binom{n}{2} p$. \\
$X_{ij} = \begin{cases}
  1 \text{ if } i \text{ and } j \text{ have edge} \\
  0 \text{ otherwise}
\end{cases}$ \\
$p$ can be function of $n$. \\
$Y_v:$ degree of $v$. \\
$E(Y_v) = (n-1)p$.
\begin{defn} \text{} \\
  We say that a random graph has some property almost surely (A.S.) if
  $P_r(G \in G(n,p) \text{ has property}) \stackrel{n \to \infty}{\to} 1$.
\end{defn}
\begin{claim} \text{} \\
  Let $p$ be constant.
  Then $G \in G(n,p)$ has diameter 2 A.S.
\end{claim}
\begin{pro} \text{} \\
  Let $u,v \in V(G)$ \\
  % skica
  $X_w = \begin{cases}
    1 \text{ if } uw \in E(G) \text{ in } vw \in E(G)
  \end{cases}$ \\
  $P_r(X_w = 1) = p^2$ \\
  $P_r(X_w = 0 \text{ for all } w \neq u,v) = \left(1-p^2\right)^{n-2}$. \\
  $P_r(G \text{ has diameter} > 2)$ \\
  $= P_r(X_w = 0$ for all $w \notin u,v $ for some $u,v)$ \\
  $\leq \binom{n}{2} (1-p^2)^{n-2} \stackrel{n \to \infty}{\to} 0$; \\
  $\binom{n}{2}$ - polynomial, $e^{...}$ - exponent.
\end{pro}
$p = f(n)$ \\
$\frac{1}{n}, \frac{1}{n^3}, \frac{\log n}{n}$
\begin{theorem} (without proof) \\
  Let $p$ be a function of $n$: let $G \in G(n,p)$:
  \begin{itemize}
    \item $np < 1$ - $G$ A.S. disconnected with connected components of size $O(\log n)$
    \item $np = 1$ - $G$ A.S. has $1$ large component of size $O\left(n^{\frac{2}{3}}\right)$
    \item $np = c > 1$ - $G$ A.S. has giant component of size $dn, \; d \in (0,1)$
    \item $np \leq (1-\epsilon) \ln n$ - $G$ A.S. disconnected with isolated vertices
    \item $np > (1-\epsilon) \ln n$ - $G$ A.S. connected.
  \end{itemize}
\end{theorem}
\begin{theorem} \text{} \\
  Let $np = \omega(n) \ln(n)$ for $\omega(n) \to \infty$ \sn{very slowly} think of $\omega(n) = \log (\log n)$,
  then $diam(G)$ in $\Theta\left(\frac{\ln n}{\ln (np)}\right)$ for $G$ in $G(n,p)$.
\end{theorem}
\begin{lemma} \text{} \\
  Let $S \subset V(G), |S| = cn$ for $c \in (0,1]$ and $v \notin S$. \\
  % skica
  then $cnp(1-\omega^{-\frac{1}{3}}) \leq N_S(v) \leq cnp(1+\omega^{-\frac{1}{3}})$ A.S.
  ($\omega^{-\frac{1}{3}} \to 0$ very slowly).
\end{lemma}
\begin{pro}(Lemma): \\
  $E(N_s(v)) = c \cdot n \cdot p, \delta = \omega^{-\frac{1}{3}}$ \\
  \begin{align*}
    P_r(|N_s(v) - cnp| \geq \delta cnp) &\stackrel{\text{Chernoff}}{\leq} 2 e^{-\frac{\omega^{-\frac{2}{3}} cnp}{3}} \\
    &= 2 e^{-\frac{cnp}{3 \omega(n)^{\frac{2}{3}}}} \stackrel{n \to \infty}{\to} 0.
  \end{align*}
  For all $v$: $n \cdot 2 e^{-\frac{cnp}{3 \omega(n)^{\frac{2}{3}}}} \stackrel{n \to \infty}{\to} 0$.
\end{pro}
\begin{pro}(Theorem): \\
  % skica
  $k$ be such that $\sum_{i=0}^{k-1} |N_i| \leq \frac{n}{2}, \sum_{i=0}^{k} |N_i| > \frac{n}{2}$. \\
  $|N_0| = 1$ \\
  $|N_i| \leq |N_{i-1}| \cdot n \cdot p \cdot (1+\omega^{-\frac{1}{3}})$: \\
  $|S| \leq n, \; np(1+\omega^{-\frac{1}{3}})$-each element. \\
  $k = \frac{\log \left(\frac{n}{3}\right)}{\log \left(n \cdot p \cdot \left(1+\omega^{-\frac{1}{3}}\right)\right)} \\
  = \log_{np(1+\omega^{-\frac{1}{3}})} \frac{n}{3} = \Theta\left(\frac{\ln(n)}{\ln(np)}\right)$. \\
  $|N_{\leq k}| = |N_1 \cup \dots \cup N_k|$.
  \begin{align*}
    |N_{\leq k}| &\leq \sum_{i=0}^{k} (np(1+\omega^{-\frac{1}{3}}))^i \\
    &= \frac{(np(1+\omega^{-\frac{1}{3}}))^{k+1}-1}{np(1+\omega^{-\frac{1}{3}}) - 1} \\
    &< \frac{np(1+\omega^{-\frac{1}{3}})^{k+1}}{\frac{1}{2} np(1+\omega^{-\frac{1}{3}})} \\
    &= 2 np(1+\omega^{-\frac{1}{3}})^k \\
    &\stackrel{k}{=} 2 \cdot \frac{n}{3} \text{ haven't covered all} \\
    &\implies diam(G) > k \text{ bound from below}.
  \end{align*}


% 6. predavanje: 10.11.

  % skica
  $N_i \subseteq S$ \\
  $\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right) \cdot |N_{i-1}| \leq |N_i|$ \\
  \begin{align*}
    n & \geq \sum_{i=0}^{k} |N_i| \\
    & \geq \sum_{i=0}^{k} \left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)^i \\
    &= \frac{\left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)^{k+1} - 1}
      {\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right) - 1} \\
    &\geq \left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)^k \qquad / \ln
  \end{align*}
  $\frac{\ln n}{\ln (np)} \approx \frac{\ln n}{\ln \left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)} \geq k$. \\
  $\implies w \in S^{'}$. \\
  Number of neighbors in $N_k$ A.S. $\geq 1$, \\
  $|N_k| \geq \left(\frac{1}{2} n p \left(1-\omega^{-\frac{1}{3}}\right)\right)^k \approx c \cdot n$ \\
  $\implies diam (G) = k + 1 $ A.S.
\end{pro}

\subsection{Scale free property}

$G \in G(n,p)$. \\
% skica
In real world:
% skica, skica
$p(k) = $ proportion of degree $k$ vertices. \\
$\log(p(k)) = -\gamma \cdot \log k$ \\
$p(k) = k^{-\gamma}$. \\
Internet: $\gamma \approx 3.42$, \\
protein reactions: $\gamma \approx 2.89$.


\section{Barbási-Albert Model}

B.A. model. \\
Start with $m$ modes. \\
Grow:
\begin{itemize}
  \item add node $v$,
  \item add $m$ edges from $v$ (to $u$),
  \item for each new edge: $P(v \sim u) = \frac{deg u}{\sum_x deg x}$.
\end{itemize}
\begin{theorem} \text{} \\
  B.A. model has scale free property, in particular \\
  $p_k = \frac{2 m (m+1)}{k (k+1) (k+2)}$.
\end{theorem}
\begin{defn} \text{} \\
  $p_n(k)$: expected proportion of degree $k$ vertices in graph with $k$ vertices, \\
  $p_k := \lim_{n \to \infty} p_n(k)$.
\end{defn}
\begin{pro} \text{} \\
  $p_n(k) \cdot n$: expected number of degree $k$ vertices, \\
  $p_n(k) n \cdot \frac{k}{\sum_u deg u} m = p_n(k) \cdot \frac{k}{2}$:
    expected number of degree $k$ vertices changing into degree $k+1$ vertices. \\
  $\sum_u deg u = 2 |E|$ \\
  $p_{n+1}(k) \cdot (n+1) = p_n(k) \cdot n - p_n(k) \cdot \frac{k}{2} + p_n(k-1) \cdot \frac{k-1}{2}$, where \\
  $p_n(k) \cdot n$: degree $k \to k$, \\
  $p_n(k) \cdot \frac{k}{2}: k \to k+1$, \\
  $p_n(k-1) \cdot \frac{k-1}{2}: k-1 \to k$. \\
  For $n$ very big (very close to limit): \\
  $p_n \cdot (n+1) = p_k \cdot n - p_{k-1} \cdot \frac{k}{2} + p_{k-1} \cdot \frac{k-1}{2}$ \\
  $\implies p_k = \frac{k-1}{k+2} p_{k-1}$. \\
  For degree $m$: \\
  $(n+1) \cdot p_{n+1}(m) = p_n(m) \cdot n - p_n(m) \cdot \frac{m}{2} + 1$ %??
  \begin{align*}
    &p_m = \frac{2}{m+2} \\
    \implies &p_{m+1} = \frac{2}{m+2} \cdot \frac{m}{m+3} \\
    \implies &p_{m+2} = \frac{2m(m+1)}{(m+2)(m+3)} \\
    \implies &p_k = \frac{2m(m+1)}{k(k+1)(k+2)}.
  \end{align*}
\end{pro}



\chapter{Markov chains}


$\Omega$: finite set (of states).
\begin{defn}[Markov chain] \text{} \\
  (Discrete time) Markov chain is a sequence of random variables $X = X_0, X_1, X_2 \dots$ with image $\Omega$ and properties:
  \begin{itemize}
    \item $P(X_{i+1} = x \mid X_i = x_i, X_{i-1} = x_{i-1} \dots X_0 = x_0) =\\ P(X_{i+1} = x \mid X_i = x_i)$,
    \item $P{X_{i+1} = x \mid X_i = y} = P(X_1 = x \mid X_0 = y)$ - time is homogenous.
  \end{itemize}
\end{defn}
\begin{ex} \text{} \\
  $\Omega = \Z_5$ \\
  $P(X_{i+1} = x+1 \mid X_i = x) = \frac{1}{2}$ \\
  $P(X_{i+1} = x-1 \mid X_i = x) = \frac{1}{2}$. \\
  % skica
\end{ex}
\begin{defn}[Transition matrix] \text{} \\
  $\Omega = \{x_1 \dots x_n\}$ \\
  $p_{ij} = P(X_{t+1} = j \mid X_t = i)$ \\
  $\begin{bmatrix}
    p_{11} & \dots \\ p_{1n} \\
    \vdots & & \vdots \\
    p_{n1} & \dots & p_{nn}
  \end{bmatrix}$.
\end{defn}
\begin{defn}[Transition graph] \text{} \\ 
  Edge between states $i$ and $j$ exists if $p_{ij} > 0$.
  % skica
\end{defn}
$P$ is stochastic matrix: \\
$p_{ij} \in [0,1]$ \\
$\sum_j p_{ij} = 1$. \\
We choose beginning state randomly. \\
$q(0) = (q_1(0) \dots q_n(0))$ \\
$P(X_0 = i) = q_i(0)$. \\
Let $q(t) = (q_1(t) \dots q_n(t))$ \\
$P(X_t = i) = q_i(t)$. \\
It holds: $q(t) = q(t-1) \cdot P = q(0) \cdot P^t$. \\
$\begin{bmatrix}
  0 & \frac{1}{2} & 0 & 0 & \frac{1}{2} \\
  \frac{1}{2} & 0 & \frac{1}{2} & 0 & 0 \\
  0 & \frac{1}{2} & 0 & \frac{1}{2} & 0 \\
  0 & 0 & \frac{1}{2} & 0 & \frac{1}{2} \\
  \frac{1}{2} & 0 & 0 & 0 & \frac{1}{2}
\end{bmatrix}$ \\
$q(0) = (1, 0, 0, 0, 0)$ \\
$q(1) = (1, \frac{1}{2}, 0, 0, \frac{1}{2})$ \\
$q(2) = (\frac{1}{2}, 0, \frac{1}{4}, \frac{1}{4}, 0)$ \\
$\vdots$ \\
% skica
\begin{defn} \text{} \\
  \begin{itemize}
    \item Distribution $\pi$ is stationary if $\pi = \pi \cdot P$,
    \item $f_{ij}$: probability that $X_t = x_j$ for some $t$ assuming $X_0 = x_i$,
      % skica
    \item $h_{ij}$: expected number of steps needed to get to state $X_j$ strting in $X_i$ (hitting time),
    \item $N(i, t, q(0))$: expected number of times we visit $x_i$ after $t$ steps starting with distribution $q(0)$,
    \item $\forall f_{ij} > 0 \; \iff$ transition graph is strongly connected $\iff$ we say the chain is irreducible,
    \item M.C. is aperiodic if there is no $c \in \{2, 3, 4 \dots\}$ such that all lengths of cycled are divisible by $c$.
  \end{itemize}
\end{defn}
\begin{theorem} \text{} \\
  Let $X$ be finite irreducible M.C. Then:
  \begin{enumerate}[label=\alph*)]
    \item there exists unique stationary distribution $\pi = (\pi_1 \dots \pi_n)$,
    \item $f_{ii} = 1, h_{ii} = \frac{1}{\pi_i}$,
    \item $\lim_{t \to \infty} \frac{N(i, t, q(0))}{t} = \pi_i$ - approaches $\pi$ regardless of $q(0)$,
    \item if $X$ is aperiodic: $\lim_{t \to \infty} q(0) \cdot P^t = \pi$. 
  \end{enumerate}
\end{theorem}


% 7. predavanje: 17.11.

\begin{ex} \text{} \\
  % skica
  $P = \begin{bmatrix}
    0 & \frac{1}{2} & \dots & \frac{1}{2} \\
    \frac{1}{2} & 0 & \dots & 0 \\
    \vdots & & & \vdots \\
    \dots & & \frac{1}{2} & 0
  \end{bmatrix}$ \\
  $\pi = (\frac{1}{n} \dots \frac{1}{n})$ \\
  $h_{i,i} = n$ \\
  $n = h_{i,i} = 1 + \frac{1}{2} h_{i-1,i} + \frac{1}{2} h_{i+1,i}, \quad h_{i-1,i} = h_{i+1,i}$ \\
  $n-1 = h_{i-1,i}$ \\
  $E(\text{steps around}) \leq h_{0,1} + h_{1,2} + \dots + h_{n-1,n} \leq n(n-1)$.
\end{ex}


\section{2-SAT}

Recall: k-SAT:
\begin{align*}
  &F = C_1 \land \dots \land C_m \\
  &C_i = X_{i1} \lor \dots \lor X_{ik}.
\end{align*}
3-SAT: NP complete. \\
\begin{alltt}
  Algorithm:
    def rand2SAT(F):
      \(b\sp{0}\) = \((b_0\sp{0} \dots b_n\sp{0})\)
      for i in range(t):
        if F(\(b\sp{i}\)) = 1:
          return True
        Cl <- clause trat is False
        xj <- uniformly at random from xl1 and xl2
        \(b\sp{i+1}\) = (\(b_0\sp{i} \dots not b_j\sp{i} \dots b_n\sp{i}\))
      if F(\(X\sp{t}\)) = 1:
        return True
      return False
\end{alltt}
\begin{theorem} \text{} \\
  If $k = 8n^2$, then $P(\text{rand2SAT = True} \mid \text{correct answer is True}) \geq \frac{3}{4}$.
\end{theorem}
\begin{pro} \text{}
  Let $a = (a_1 \dots a_n)$ be a correct solution. \\
  Let $X_i =$ Hamming distance from $b^i$ to $a$. \\
  % skica(i)
  Goal: bount $h_{n,0}$. \\
  $P(\text{distance of } b^{i+1} \text{ to $a$ is } j-1 \mid \text{distance of $b^i$ to $a$ is } j) \geq \frac{1}{2}$. \\
  $P = \begin{bmatrix}
    0 & 1 & \dots & 0 \\
    \frac{1}{2} & 0 & \dots & 0 \\
    \vdots & & & \vdots \\
    \dots & & 1 & 0
  \end{bmatrix}$ \\
  $\pi \stackrel{?}{=} \pi P$ \\
  $\pi = (\frac{1}{2n}, \frac{1}{n} \dots \frac{1}{n}, \frac{1}{2n})$ \\
  By theorem \\
  $h_{i,i} = \frac{1}{\pi_i} = n$ for $i = 1, 2 \dots n-1$ \\
  $h_{0,0} = h_{n,n} = 2n$ \\
  $n = h_{i,i} = 1 + \frac{1}{2} h_{i+1,i} + \frac{1}{2} h_{i-1,i}$ \\
  $h_{i+1,i} \leq 2n$ \\
  $i = 0: \; 2n = h_{0,0} = 1 + h_{1,0} \implies h_{1,0} < 2n$ \\
  $h_{n,0} \leq h_{n,n-1} + \dots + h_{1,0} \leq 2n^2$ \\
  $E(\text{steps in algorithm to reach corrce solution}) = E(Z) \leq 2n^2$ \\
  $P(\text{algorithm hasn't reached correct solution after $8n^2$ steps}) \\
  = P(Z > 8n^2) \stackrel{\text{Markov}}{\leq} \frac{E(Z)}{8n^2} \leq \frac{1}{4}$.
\end{pro}


\section{Generating a uniformly random element of a set}

$\Omega$: set. \\
Let $G$ be a symmetric graph on $\Omega$. \\
We form M.C: \\
$P_{x,y} = \begin{cases}
  \frac{1}{M} \text{ if } x \neq y \land x \sim y \\
  0 \text{ if } x \neq y \land x \nsim y \\
  1 - \frac{|N(x)|}{M} \text{ if } x = y
\end{cases}$ \\
$M \geq \max_{v \in \Omega} |N(v)|$. \\
If $G$ is connected $\implies$ M.C. is irrecudible. \\
$\pi = (\frac{1}{|\Omega|} \dots \frac{1}{|\Omega|})$ \\
$\pi \stackrel{?}{=} \pi P$ \\
\begin{align*}
  (\pi P)_x &= \sum_y \pi_y P_{y,x} \\
  &= \sum_{y \in N(x)} \frac{1}{M} \cdot \frac{1}{|\Omega|} + \frac{1}{|\Omega|}
    \left(1 - \frac{|N(x)|}{M}\right) = \frac{1}{|\Omega|} = \pi_x.
\end{align*}
$\implies$ if we walk on the Markov chain long enough, we end up in state $x$ with probability $\pi_x = \frac{1}{|\Omega|}$ \\
$\implies$ we can sample uniformly.
\begin{ex} \text{} \\
  $G$ graph, finding largest independent set $(\forall u,v: u \nsim v)$ is NP-complete. \\
  Lets try sampling a uniformly random independent set \\
  $\Omega = \{\text{independent sets}\}$ \\
  $u \sim v$ if $|u \triangle v| = 1$ $((u \cup \{el\}) = v)$ \\
  M.C.: $X_0$ = arbitrary independent set \\
  $X_{i+1}$:
  \begin{itemize}
    \item pick uniformly at random $v \in V(G)$,
    \item if $v \in U$ then $X_{i+1} = U \textbackslash \{v\}$,
    \item if $U \cup \{v\}$ is independent then $X_{i+1} = U \cup \{v\}$,
    \item else $X_{i+1} = U$.
  \end{itemize}
  $M$ is number of vertices \\
  $\implies \; \forall u \in \Omega: \lim_{t \to \infty} P(X_t = u) = \frac{1}{|\Omega|}$. \\
  Note: irredudicle; $U \to \emptyset \to V$, aperiodic.
\end{ex}


\section{Metropolis algorithm}

$\Omega$: set, \\
$\pi$: chosen distribution on $\Omega$. \\
Make $G$ graph on $\Omega$ \\
$P_{x,y} = \begin{cases}
  \frac{1}{M} \cdot \min \left(1, \frac{\pi_y}{\pi_x}\right) \text{ if } x \neq y \land x \sim y \\
  0 \text{ if } x \neq y \land x \nsim y \\
  1 - \sum_{y \in N(x)} \text{ if } x = y
\end{cases}$ \\
$M \geq \max_{v \in \Omega} |N(v)|$ \\
$\pi \stackrel{?}{=} \pi P$ \\
\begin{align*}
  (\pi P)_x &= \sum_y \pi_y P_{y,x} =
    \sum_{y \in N(x)} \pi_y \frac{1}{M} \min \left(\left(1, \frac{\pi_y}{\pi_x}\right)\right)
    + \pi_x \left(1 - \sum_{y \in N(x)} \frac{1}{M} \min \left(1, \frac{\pi_y}{\pi_x}\right)\right) \\
  &= \sum_{y \in N(x), \pi_y \geq \pi_x} \pi_y \frac{1}{M} \cdot 1 +
    \sum_{y \in N(x), \pi_y < \pi_x} \pi_y \frac{1}{M} \frac{\pi_y}{\pi_x} + \pi_x \\
  &- \sum_{y \in N(x), \pi_y \geq \pi_x} \pi_x \frac{1}{M} \frac{\pi_y}{\pi_x} -
    \sum_{y \in N(x), \pi_y < \pi_x} \frac{1}{M} \cdot 1 \\
  &= \pi_x.
\end{align*}
\begin{ex} \text{} \\
  $\Omega = \Z \cap [-1000,1000]$ \\
  $\pi \sim e^{-\frac{(x-\mu)^2}{2 \delta}}$ \\
  % skica
  \begin{alltt}
    \(X\sb{0}\) arbitrary
    for i = in range(1,m):
      y <- uniformly from {\(X\sb{i}\)+1,\(X\sb{i}\)-1}
      M <- uniformly from [0,1]
      if \(M \leq \frac{\pi(y)}{\pi(x)}\):
        \(X\sb{i+1}\) = y
      else:
        \(X\sb{i+1}\) = \(X\sb{i}\)
    return \(X\sb{m}\)
  \end{alltt}
\end{ex}
\begin{ex} \text{} \\
  Find maximum of a positive function $f$. \\
  Use metropolis algorithm to sample proportional to $f$. \\
  Note: all I need to know is ratios $\frac{f(y)}{f(x)}$.
\end{ex}


%\clearpage
%\phantomsection

%\addcontentsline{toc}{chapter}{Literatura}
%\bibliography{../bibtex/literatura}
%\bibliographystyle{plainnat}


%\clearpage
%\phantomsection

%\chapter*{Dodatki}
%\addcontentsline{toc}{chapter}{Dodatki}
%D.




\end{document}
